#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Normalize Guard Script v2.0
å†»ç»“å ä½ç¬¦/æ ‡ç­¾ä¸º tokenï¼Œç”Ÿæˆ draft.csv å’Œ placeholder_map.json

èåˆç‰ˆæœ¬ï¼šç»“åˆ v1.0 çš„ä¸¥æ ¼éªŒè¯å’Œ v2.0 çš„æ–°ç‰¹æ€§

Usage:
    python normalize_guard.py <input_csv> <output_draft_csv> <output_map_json> <schema_yaml>

Features:
    - ä½¿ç”¨æ–°çš„ schema v2.0 æ ¼å¼ (patterns, token_format)
    - Token é‡ç”¨æœºåˆ¶ï¼ˆç›¸åŒå ä½ç¬¦ä½¿ç”¨ç›¸åŒ tokenï¼‰
    - åŸºæœ¬å¹³è¡¡æ£€æŸ¥ï¼ˆæ‹¬å·ã€æ ‡ç­¾å¹³è¡¡ï¼‰
    - æ—©æœŸ QA æŠ¥å‘Šç”Ÿæˆ
    - è¯¦ç»†çš„é”™è¯¯å¤„ç†å’ŒéªŒè¯
    - é‡å¤ ID æ£€æµ‹
"""

import csv
import json
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Set
from datetime import datetime

try:
    import yaml
except ImportError:
    print("âŒ Error: PyYAML is required. Install with: pip install pyyaml")
    sys.exit(1)


class PlaceholderFreezer:
    """å ä½ç¬¦å†»ç»“å™¨ - ä½¿ç”¨ schema v2.0"""
    
    def __init__(self, schema_path: str):
        self.schema_path = Path(schema_path)
        self.patterns: List[Dict] = []
        self.token_format: Dict[str, str] = {}
        
        # è®¡æ•°å™¨
        self.ph_counter = 0
        self.tag_counter = 0
        
        # æ˜ å°„ï¼štoken_name -> original_text
        self.placeholder_map: Dict[str, str] = {}
        
        # åå‘æ˜ å°„ï¼šoriginal_text -> token_nameï¼ˆç”¨äºé‡ç”¨ï¼‰
        self.reverse_map: Dict[str, str] = {}
        
        self.load_schema()
    
    def load_schema(self) -> None:
        """åŠ è½½ schema v2.0"""
        try:
            with open(self.schema_path, 'r', encoding='utf-8') as f:
                schema = yaml.safe_load(f)
                
                # v2.0 ä½¿ç”¨ 'patterns' è€Œä¸æ˜¯ 'placeholder_patterns'
                self.patterns = schema.get('patterns', [])
                self.token_format = schema.get('token_format', {
                    'placeholder': 'âŸ¦PH_{n}âŸ§',
                    'tag': 'âŸ¦TAG_{n}âŸ§'
                })
                
                if not self.patterns:
                    print("âš ï¸  Warning: No patterns found in schema")
                    print(f"   Schema keys: {list(schema.keys())}")
                else:
                    print(f"âœ… Loaded {len(self.patterns)} patterns from schema v{schema.get('version', 'unknown')}")
                
        except FileNotFoundError:
            print(f"âŒ Error: Schema file not found: {self.schema_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error loading schema: {str(e)}")
            sys.exit(1)
    
    def freeze_text(self, text: str) -> Tuple[str, Dict[str, str]]:
        """
        å†»ç»“æ–‡æœ¬ä¸­çš„å ä½ç¬¦å’Œæ ‡ç­¾
        
        é‡è¦ï¼šä½¿ç”¨ token é‡ç”¨æœºåˆ¶ï¼Œç›¸åŒçš„å ä½ç¬¦é‡ç”¨ç›¸åŒçš„ token
        
        Returns:
            (tokenized_text, local_map) - token åŒ–çš„æ–‡æœ¬å’Œæœ¬æ¬¡å†»ç»“çš„æ˜ å°„
        """
        if not text:
            return text, {}
        
        local_map = {}
        result = text
        
        # ç¼–è¯‘æ‰€æœ‰æ¨¡å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        compiled_patterns = []
        for p in self.patterns:
            try:
                compiled_patterns.append({
                    'name': p['name'],
                    'type': p['type'],
                    'regex': re.compile(p['regex'])
                })
            except re.error as e:
                print(f"âš ï¸  Warning: Invalid regex in pattern '{p['name']}': {e}")
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å¼
        for pattern_def in compiled_patterns:
            regex = pattern_def['regex']
            ptype = pattern_def['type']
            
            def repl(match):
                original = match.group(0)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»å†»ç»“è¿‡è¿™ä¸ªå­—ç¬¦ä¸²ï¼ˆé‡ç”¨ tokenï¼‰
                if original in self.reverse_map:
                    token_name = self.reverse_map[original]
                    return f"âŸ¦{token_name}âŸ§"
                
                # ç”Ÿæˆæ–° token
                if ptype == 'placeholder':
                    self.ph_counter += 1
                    token_name = f"PH_{self.ph_counter}"
                else:  # tag
                    self.tag_counter += 1
                    token_name = f"TAG_{self.tag_counter}"
                
                # è®°å½•æ˜ å°„
                self.placeholder_map[token_name] = original
                self.reverse_map[original] = token_name
                local_map[token_name] = original
                
                return f"âŸ¦{token_name}âŸ§"
            
            result = regex.sub(repl, result)
        
        return result, local_map
    
    def reset_counters(self) -> None:
        """é‡ç½®è®¡æ•°å™¨ï¼ˆç”¨äºå¤„ç†æ–°æ–‡ä»¶ï¼‰"""
        self.ph_counter = 0
        self.tag_counter = 0
        self.placeholder_map = {}
        self.reverse_map = {}


def detect_unbalanced_basic(text: str) -> List[str]:
    """
    åŸºæœ¬çš„å¹³è¡¡æ£€æŸ¥ - æ£€æµ‹æ˜æ˜¾çš„ä¸å¹³è¡¡
    
    è¿™æ˜¯ä¿å®ˆçš„å¥å…¨æ€§æ£€æŸ¥ï¼Œç”¨äºæ—©æœŸå‘ç°é—®é¢˜
    """
    issues = []
    
    # èŠ±æ‹¬å·å¹³è¡¡
    if text.count('{') != text.count('}'):
        issues.append('brace_unbalanced')
    
    # å°–æ‹¬å·å¹³è¡¡ï¼ˆç²—ç•¥æ£€æŸ¥ï¼Œæ ‡ç­¾ä¼šåœ¨ QA ä¸­è¯¦ç»†æ£€æŸ¥ï¼‰
    if text.count('<') != text.count('>'):
        issues.append('angle_unbalanced')
    
    # æ–¹æ‹¬å·å¹³è¡¡
    if text.count('[') != text.count(']'):
        issues.append('square_unbalanced')
    
    return issues


class NormalizeGuard:
    """ä¸»å¤„ç†ç±»ï¼šè§„èŒƒåŒ–è¾“å…¥å¹¶ç”Ÿæˆ draft.csv å’Œ placeholder_map.json"""
    
    def __init__(self, input_path: str, output_draft_path: str,
                 output_map_path: str, schema_path: str):
        self.input_path = Path(input_path)
        self.output_draft_path = Path(output_draft_path)
        self.output_map_path = Path(output_map_path)
        self.schema_path = Path(schema_path)
        
        self.freezer = PlaceholderFreezer(schema_path)
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.sanity_errors: List[Dict] = []  # æºæ–‡æœ¬å¹³è¡¡é—®é¢˜
    
    def validate_input_headers(self, headers: List[str]) -> bool:
        """éªŒè¯è¾“å…¥æ–‡ä»¶å¿…éœ€åˆ—"""
        required = ['string_id', 'source_zh']
        missing = set(required) - set(headers)
        
        if missing:
            self.errors.append(f"Missing required columns: {missing}")
            return False
        return True
    
    def process_csv(self) -> Tuple[bool, List[Dict]]:
        """å¤„ç† CSV æ–‡ä»¶"""
        try:
            with open(self.input_path, 'r', encoding='utf-8-sig', newline='') as f:
                reader = csv.DictReader(f)
                headers = reader.fieldnames
                
                if not self.validate_input_headers(headers):
                    return False, []
                
                processed_rows = []
                seen_ids: Set[str] = set()
                
                for idx, row in enumerate(reader, start=2):
                    string_id = (row.get('string_id') or '').strip()
                    source_zh = row.get('source_zh') or ''
                    
                    # éªŒè¯ string_id
                    if not string_id:
                        self.errors.append(f"Row {idx}: Empty string_id")
                        continue
                    
                    if string_id in seen_ids:
                        self.errors.append(f"Row {idx}: Duplicate string_id '{string_id}'")
                        continue
                    
                    seen_ids.add(string_id)
                    
                    # åŸºæœ¬å¹³è¡¡æ£€æŸ¥
                    issues = detect_unbalanced_basic(source_zh)
                    if issues:
                        self.sanity_errors.append({
                            'string_id': string_id,
                            'issues': issues,
                            'source_zh': source_zh,
                            'row': idx
                        })
                    
                    # å†»ç»“å ä½ç¬¦
                    tokenized_zh, local_map = self.freezer.freeze_text(source_zh)
                    
                    # æ„å»ºè¾“å‡ºè¡Œ
                    output_row = {
                        'string_id': string_id,
                        'source_zh': source_zh,
                        'tokenized_zh': tokenized_zh,
                    }
                    
                    # ä¿ç•™å…¶ä»–åˆ—
                    for key in headers:
                        if key not in ['string_id', 'source_zh']:
                            output_row[key] = row.get(key, '')
                    
                    processed_rows.append(output_row)
                    
                    # æ‰“å°å†»ç»“ä¿¡æ¯
                    if local_map:
                        print(f"  Row {idx} ({string_id}): Froze {len(local_map)} placeholders")
                
                return len(self.errors) == 0, processed_rows
                
        except FileNotFoundError:
            self.errors.append(f"Input file not found: {self.input_path}")
            return False, []
        except Exception as e:
            self.errors.append(f"Error processing CSV: {str(e)}")
            import traceback
            traceback.print_exc()
            return False, []
    
    def write_draft_csv(self, rows: List[Dict]) -> bool:
        """å†™å…¥ draft.csv"""
        try:
            if not rows:
                self.warnings.append("No rows to write")
                return True
            
            # ç¡®ä¿åˆ—é¡ºåºï¼šstring_id, source_zh, tokenized_zh, å…¶ä»–åˆ—
            fieldnames = ['string_id', 'source_zh', 'tokenized_zh']
            for key in rows[0].keys():
                if key not in fieldnames:
                    fieldnames.append(key)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_draft_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.output_draft_path, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(rows)
            
            print(f"âœ… Wrote {len(rows)} rows to {self.output_draft_path}")
            return True
            
        except Exception as e:
            self.errors.append(f"Error writing draft CSV: {str(e)}")
            return False
    
    def write_placeholder_map(self) -> bool:
        """å†™å…¥ placeholder_map.json"""
        try:
            output = {
                'metadata': {
                    'version': '2.0',
                    'generated_at': datetime.now().isoformat(),
                    'input_file': str(self.input_path),
                    'total_placeholders': len(self.freezer.placeholder_map),
                    'ph_count': self.freezer.ph_counter,
                    'tag_count': self.freezer.tag_counter
                },
                'mappings': self.freezer.placeholder_map
            }
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_map_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.output_map_path, 'w', encoding='utf-8') as f:
                json.dump(output, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Wrote {len(self.freezer.placeholder_map)} placeholder mappings to {self.output_map_path}")
            return True
            
        except Exception as e:
            self.errors.append(f"Error writing placeholder map: {str(e)}")
            return False
    
    def write_early_qa_report(self, total_rows: int) -> None:
        """
        å†™å…¥æ—©æœŸ QA æŠ¥å‘Šï¼ˆå¦‚æœå‘ç°æºæ–‡æœ¬å¹³è¡¡é—®é¢˜ï¼‰
        
        è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„æ—©æœŸæ£€æŸ¥ï¼Œå¸®åŠ©åœ¨ç¿»è¯‘å‰å‘ç°é—®é¢˜
        """
        if not self.sanity_errors:
            return
        
        early_report = {
            'has_errors': True,
            'total_rows': total_rows,
            'error_counts': {
                'source_unbalanced_basic': len(self.sanity_errors)
            },
            'errors': [
                {
                    'row': e['row'],
                    'string_id': e['string_id'],
                    'type': 'source_unbalanced_basic',
                    'detail': ', '.join(e['issues']),
                    'source': e['source_zh']
                }
                for e in self.sanity_errors[:200]  # é™åˆ¶é”™è¯¯æ•°é‡
            ],
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'note': 'Early sanity check - source text balance issues detected'
            }
        }
        
        # å†™å…¥æ—©æœŸæŠ¥å‘Š
        early_path = self.output_map_path.parent / 'qa_hard_report.json'
        with open(early_path, 'w', encoding='utf-8') as f:
            json.dump(early_report, f, ensure_ascii=False, indent=2)
        
        print(f"âš ï¸  Found {len(self.sanity_errors)} source sanity issues")
        print(f"   Early QA report written: {early_path}")
    
    def run(self) -> bool:
        """æ‰§è¡Œè§„èŒƒåŒ–æµç¨‹"""
        print("ğŸš€ Starting normalize guard v2.0...")
        print(f"   Input: {self.input_path}")
        print(f"   Output draft: {self.output_draft_path}")
        print(f"   Output map: {self.output_map_path}")
        print(f"   Schema: {self.schema_path}")
        print()
        
        # å¤„ç† CSV
        success, rows = self.process_csv()
        
        if not success:
            self._print_errors()
            return False
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        success = self.write_draft_csv(rows)
        if not success:
            self._print_errors()
            return False
        
        success = self.write_placeholder_map()
        if not success:
            self._print_errors()
            return False
        
        # å†™å…¥æ—©æœŸ QA æŠ¥å‘Šï¼ˆå¦‚æœæœ‰å¹³è¡¡é—®é¢˜ï¼‰
        self.write_early_qa_report(len(rows))
        
        # æ‰“å°æ€»ç»“
        self._print_summary(rows)
        
        return True
    
    def _print_errors(self) -> None:
        """æ‰“å°é”™è¯¯ä¿¡æ¯"""
        if self.warnings:
            print("\nâš ï¸  Warnings:")
            for warning in self.warnings:
                print(f"   {warning}")
        
        if self.errors:
            print("\nâŒ Errors:")
            for error in self.errors:
                print(f"   {error}")
    
    def _print_summary(self, rows: List[Dict]) -> None:
        """æ‰“å°å¤„ç†æ€»ç»“"""
        print(f"\nğŸ“Š Summary:")
        print(f"   Total strings processed: {len(rows)}")
        print(f"   Total placeholders frozen: {len(self.freezer.placeholder_map)}")
        print(f"   PH tokens: {self.freezer.ph_counter}")
        print(f"   TAG tokens: {self.freezer.tag_counter}")
        
        if self.sanity_errors:
            print(f"   âš ï¸  Source balance issues: {len(self.sanity_errors)}")
        
        if self.warnings:
            print(f"   Warnings: {len(self.warnings)}")
        
        print(f"\nâœ… Normalization complete!")


def main():
    """ä¸»å…¥å£"""
    if len(sys.argv) != 5:
        print("Usage: python normalize_guard.py <input_csv> <output_draft_csv> <output_map_json> <schema_yaml>")
        print()
        print("Example:")
        print("  python normalize_guard.py data/input.csv data/draft.csv data/placeholder_map.json workflow/placeholder_schema.yaml")
        sys.exit(1)
    
    guard = NormalizeGuard(
        input_path=sys.argv[1],
        output_draft_path=sys.argv[2],
        output_map_path=sys.argv[3],
        schema_path=sys.argv[4]
    )
    
    success = guard.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
