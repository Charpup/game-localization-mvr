#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
soft_qa_llm.py (v2.0 - Batch Mode)
LLM-based soft quality review for translations.

Purpose:
  Soft QA çš„ä»·å€¼ä¸æ˜¯"æ‰“åˆ†"ï¼Œè€Œæ˜¯è¾“å‡ºå¯æ‰§è¡Œçš„ repair tasksï¼Œè®© repair loop èƒ½è‡ªåŠ¨ä¿®ã€‚
  è¯„å®¡ç»´åº¦ï¼šstyle_officialness, anime_tone, terminology_consistency, ui_brevity, ambiguity

  BATCH processing: multiple items per LLM call to reduce prompt token waste.

Usage:
  python scripts/soft_qa_llm.py \\
    data/translated.csv workflow/style_guide.md data/glossary.yaml workflow/soft_qa_rubric.yaml \\
    --batch_size 15 --out_report data/qa_soft_report.json --out_tasks data/repair_tasks.jsonl

Environment:
  LLM_BASE_URL, LLM_API_KEY, LLM_MODEL (via runtime_adapter)
"""

import argparse
import csv
import json
import os
import re
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# Ensure UTF-8 output on Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

try:
    import yaml
except Exception:
    yaml = None

from runtime_adapter import LLMClient, LLMError

# Import batch utilities
try:
    from batch_utils import (
        BatchConfig, split_into_batches, parse_json_array, format_progress
    )
except ImportError:
    print("ERROR: batch_utils.py not found. Please ensure it exists in scripts/")
    sys.exit(1)

TOKEN_RE = re.compile(r"âŸ¦(PH_\d+|TAG_\d+)âŸ§")


def load_text(p: str) -> str:
    """Load text file content."""
    with open(p, "r", encoding="utf-8") as f:
        return f.read().strip()


def load_yaml(p: str) -> dict:
    """Load YAML file."""
    if yaml is None:
        raise RuntimeError("PyYAML required: pip install pyyaml")
    with open(p, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def read_csv(p: str) -> List[Dict[str, str]]:
    """Read CSV file as list of dicts."""
    with open(p, "r", encoding="utf-8-sig", newline="") as f:
        return list(csv.DictReader(f))


def write_json(p: str, obj: Any) -> None:
    """Write JSON file."""
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def append_jsonl(p: str, items: List[dict]) -> None:
    """Append items to JSONL file."""
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    with open(p, "a", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")


def token_counts(s: str) -> Dict[str, int]:
    """Count tokens in string."""
    d = {}
    for m in TOKEN_RE.finditer(s or ""):
        k = m.group(1)
        d[k] = d.get(k, 0) + 1
    return d


# Import glossary logic from translate_llm
from translate_llm import load_glossary, build_glossary_constraints, GlossaryEntry


def build_system_batch(style: str, glossary_summary: str) -> str:
    """Build system prompt for batch soft QA."""
    return (
        "ä½ æ˜¯æ‰‹æ¸¸æœ¬åœ°åŒ–è½¯è´¨æ£€ï¼ˆzh-CN â†’ ru-RUï¼‰ã€‚\n\n"
        "è¾“å…¥ï¼šJSON æ•°ç»„ï¼Œæ¯é¡¹åŒ…å« string_idã€source_zhã€target_ruã€‚\n"
        "è¾“å‡ºï¼šJSON å¯¹è±¡ï¼ŒåŒ…å« tasks æ•°ç»„ï¼Œä»…åˆ—å‡ºæœ‰é—®é¢˜çš„é¡¹ã€‚\n\n"
        "æ£€æŸ¥ç»´åº¦ï¼ˆåªæŠ¥é—®é¢˜ï¼Œä¸è¦å¤¸ï¼‰ï¼š\n"
        "- æœ¯è¯­ä¸€è‡´æ€§ï¼ˆglossaryï¼‰\n"
        "- è¯­æ°”ï¼šå®˜æ–¹ä¸ºä¸»ï¼ŒäºŒæ¬¡å…ƒå£è¯­ä¸ºè¾…ï¼ˆé¿å…è¿‡åº¦å£è¯­æˆ–è¿‡åº¦ä¹¦é¢ï¼‰\n"
        "- UI ç®€æ´æ€§ï¼ˆå†—é•¿/é‡å¤/ä¸è‡ªç„¶ï¼‰\n"
        "- æ­§ä¹‰/è¯¯è¯‘/ä¿¡æ¯ç¼ºå¤±\n"
        "- æ ‡ç‚¹ä¸ç¬¦å·ï¼šç¦æ­¢ã€ã€‘ï¼›å ä½ç¬¦å¿…é¡»å®Œæ•´\n\n"
        "è¾“å‡ºæ ¼å¼ï¼ˆç¡¬æ€§ï¼Œä»…è¾“å‡º JSONï¼‰ï¼š\n"
        "{\n"
        '  "tasks": [\n'
        "    {\n"
        '      "string_id": "<id>",\n'
        '      "severity": "minor|major",\n'
        '      "issue_type": "terminology|tone|brevity|ambiguity|mistranslation|format|punctuation",\n'
        '      "problem": "<ä¸€å¥è¯æè¿°é—®é¢˜>",\n'
        '      "suggestion": "<ä¸€å¥è¯ç»™å‡ºä¿®å¤æ–¹å‘>",\n'
        '      "preferred_fix_ru": "<å¯é€‰ï¼šå»ºè®®çš„ä¿®å¤åä¿„æ–‡>"\n'
        "    }\n"
        "  ]\n"
        "}\n"
        "è§„åˆ™ï¼š\n"
        '- æ²¡é—®é¢˜åˆ™è¾“å‡º { "tasks": [] }ã€‚\n'
        "- problem/suggestion å¿…é¡»çŸ­å¥ã€‚\n"
        "- æ¯ä¸ªæœ‰é—®é¢˜çš„ string_id åªè¾“å‡ºä¸€ä¸ªæœ€ä¸¥é‡çš„ taskã€‚\n\n"
        f"æœ¯è¯­è¡¨æ‘˜è¦ï¼ˆå‰ 50 æ¡ï¼‰ï¼š\n{glossary_summary[:1500]}\n\n"
        f"style_guideï¼ˆèŠ‚é€‰ï¼‰ï¼š\n{style[:1000]}\n"
    )


def build_batch_input(rows: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Build batch input for soft QA."""
    batch_items = []
    for row in rows:
        batch_items.append({
            "string_id": row.get("string_id", ""),
            "source_zh": row.get("source_zh", "") or row.get("tokenized_zh", ""),
            "target_ru": row.get("target_text", "")
        })
    return batch_items


def build_glossary_summary(entries: List[GlossaryEntry], max_entries: int = 50) -> str:
    """Build compact glossary summary."""
    approved = [e for e in entries if e.status.lower() == "approved"][:max_entries]
    if not approved:
        return "(æ— )"
    lines = [f"- {e.term_zh} â†’ {e.term_ru}" for e in approved]
    return "\n".join(lines)


def extract_tasks_from_response(text: str) -> List[dict]:
    """Extract tasks array from LLM response."""
    text = (text or "").strip()
    
    # Try direct parse
    try:
        obj = json.loads(text)
        if isinstance(obj, dict) and "tasks" in obj:
            return obj["tasks"]
    except json.JSONDecodeError:
        pass
    
    # Find { ... } block
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        try:
            obj = json.loads(text[start:end + 1])
            if isinstance(obj, dict) and "tasks" in obj:
                return obj["tasks"]
        except json.JSONDecodeError:
            pass
    
    return []


def process_batch(
    batch: List[Dict[str, str]],
    llm: LLMClient,
    style: str,
    glossary_summary: str,
    batch_idx: int,
    max_retries: int = 2
) -> List[dict]:
    """
    Process a batch of rows through soft QA.
    
    Returns:
        List of task dicts (only problem items)
    """
    if not batch:
        return []
    
    system = build_system_batch(style, glossary_summary)
    batch_input = build_batch_input(batch)
    user_prompt = json.dumps(batch_input, ensure_ascii=False, indent=None)
    
    for attempt in range(max_retries + 1):
        try:
            result = llm.chat(
                system=system,
                user=user_prompt,
                temperature=0.1,
                metadata={
                    "step": "soft_qa",
                    "batch_idx": batch_idx,
                    "batch_size": len(batch),
                    "attempt": attempt
                },
                response_format={"type": "json_object"}
            )
            
            tasks = extract_tasks_from_response(result.text)
            
            # Normalize tasks
            valid_tasks = []
            for t in tasks:
                valid_tasks.append({
                    "string_id": t.get("string_id", ""),
                    "type": t.get("issue_type", "issue"),
                    "severity": t.get("severity", "minor"),
                    "note": f"{t.get('problem', '')} | Suggestion: {t.get('suggestion', '')}",
                    "suggested_fix": t.get("preferred_fix_ru", ""),
                })
            
            return valid_tasks
            
        except Exception as e:
            if attempt >= max_retries:
                print(f"    âš ï¸ Batch {batch_idx} error: {e}")
                return []
            time.sleep(1)
    
    return []


def main():
    ap = argparse.ArgumentParser(description="LLM-based soft QA (Batch Mode v2.0)")
    ap.add_argument("translated_csv", help="Input translated.csv")
    ap.add_argument("style_guide_md", help="Style guide file")
    ap.add_argument("glossary_yaml", help="Glossary file")
    ap.add_argument("rubric_yaml", help="Soft QA rubric config (legacy, ignored)")
    ap.add_argument("--batch_size", type=int, default=15, help="Items per batch")
    ap.add_argument("--max_batch_tokens", type=int, default=4000, help="Max tokens per batch")
    ap.add_argument("--out_report", default="data/qa_soft_report.json", help="Output report JSON")
    ap.add_argument("--out_tasks", default="data/repair_tasks.jsonl", help="Output repair tasks JSONL")
    ap.add_argument("--dry-run", action="store_true", 
                    help="Validate configuration without making LLM calls")
    args = ap.parse_args()

    print(f"ğŸ” Soft QA v2.0 (Batch Mode)")
    print(f"   Input: {args.translated_csv}")
    print(f"   Batch size: {args.batch_size}")
    print()

    # Load resources
    rows = read_csv(args.translated_csv)
    style = load_text(args.style_guide_md)

    glossary_path = args.glossary_yaml
    glossary_entries = []
    if glossary_path and Path(glossary_path).exists():
        glossary_entries, _ = load_glossary(glossary_path)
    
    glossary_summary = build_glossary_summary(glossary_entries)
    
    print(f"âœ… Loaded {len(rows)} rows")
    print(f"   Glossary: {len(glossary_entries)} entries")
    
    # Filter rows with target_text
    rows_with_target = [r for r in rows if r.get("target_text")]
    print(f"   Rows with translations: {len(rows_with_target)}")
    
    # Dry-run mode
    if getattr(args, 'dry_run', False):
        print()
        print("=" * 60)
        print("DRY-RUN MODE - Validation Summary")
        print("=" * 60)
        
        config = BatchConfig(max_items=args.batch_size, max_tokens=args.max_batch_tokens)
        config.text_fields = ["source_zh", "tokenized_zh", "target_text"]
        batches = split_into_batches(rows_with_target, config)
        
        print(f"[OK] Would create {len(batches)} batches")
        print(f"[OK] Average batch size: {len(rows_with_target) / max(1, len(batches)):.1f}")
        print()
        print("[OK] Dry-run validation PASSED")
        print("=" * 60)
        return 0

    # Initialize LLM
    try:
        llm = LLMClient()
        print(f"âœ… LLM: {llm.default_model}")
    except LLMError as e:
        print(f"âŒ LLM Error: {e}")
        return 1

    print()

    # Clean output file (fresh start)
    if Path(args.out_tasks).exists():
        Path(args.out_tasks).unlink()

    # Split into batches
    config = BatchConfig(max_items=args.batch_size, max_tokens=args.max_batch_tokens)
    config.text_fields = ["source_zh", "tokenized_zh", "target_text"]
    batches = split_into_batches(rows_with_target, config)
    print(f"   Batches: {len(batches)}")
    print()

    # Process batches
    start_time = time.time()
    major = 0
    minor = 0
    all_tasks = 0
    batch_errors = 0

    for batch_idx, batch in enumerate(batches):
        batch_start = time.time()
        
        tasks = process_batch(
            batch, llm, style, glossary_summary, batch_idx
        )
        
        if tasks:
            append_jsonl(args.out_tasks, tasks)
            all_tasks += len(tasks)
            for t in tasks:
                if t.get("severity") == "major":
                    major += 1
                else:
                    minor += 1
        
        # Progress
        batch_time = time.time() - batch_start
        elapsed = time.time() - start_time
        
        if (batch_idx + 1) % 5 == 0 or batch_idx == len(batches) - 1:
            print(format_progress(
                batch_idx + 1, len(batches), batch_idx + 1, len(batches),
                elapsed, batch_time
            ))

    # Write report
    report = {
        "version": "2.0",
        "mode": "batch",
        "has_findings": (major + minor) > 0,
        "summary": {
            "major": major,
            "minor": minor,
            "total_tasks": all_tasks,
            "batch_errors": batch_errors,
            "rows_processed": len(rows_with_target),
            "batches_processed": len(batches),
        },
        "outputs": {
            "repair_tasks_jsonl": args.out_tasks,
        },
    }
    write_json(args.out_report, report)

    # Print summary
    total_elapsed = time.time() - start_time
    print()
    print(f"ğŸ“Š Soft QA Summary:")
    print(f"   Rows processed: {len(rows_with_target)}")
    print(f"   Major issues: {major}")
    print(f"   Minor issues: {minor}")
    print(f"   Total tasks: {all_tasks}")
    print(f"   Total time: {int(total_elapsed)}s")
    print()
    print(f"âœ… Report: {args.out_report}")
    if all_tasks > 0:
        print(f"âœ… Repair tasks: {args.out_tasks}")

    return 0


if __name__ == "__main__":
    exit(main())
