#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
soft_qa_llm.py (v2.0 - Batch Mode)
LLM-based soft quality review for translations.

Purpose:
  Soft QA çš„ä»·å€¼ä¸æ˜¯"æ‰“åˆ†"ï¼Œè€Œæ˜¯è¾“å‡ºå¯æ‰§è¡Œçš„ repair tasksï¼Œè®© repair loop èƒ½è‡ªåŠ¨ä¿®ã€‚
  è¯„å®¡ç»´åº¦ï¼šstyle_officialness, anime_tone, terminology_consistency, ui_brevity, ambiguity

  BATCH processing: multiple items per LLM call to reduce prompt token waste.

Usage:
  python scripts/soft_qa_llm.py \\
    data/translated.csv workflow/style_guide.md data/glossary.yaml workflow/soft_qa_rubric.yaml \\
    --batch_size 15 --out_report data/qa_soft_report.json --out_tasks data/repair_tasks.jsonl

Environment:
  LLM_BASE_URL, LLM_API_KEY, LLM_MODEL (via runtime_adapter)
"""

import argparse
import csv
import json
import os
import re
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# sys.stdout wrapping moved to __main__ block

try:
    import yaml
except Exception:
    yaml = None

from runtime_adapter import LLMClient, LLMError, BatchConfig, get_batch_config, batch_llm_call, log_llm_progress

TOKEN_RE = re.compile(r"âŸ¦(PH_\d+|TAG_\d+)âŸ§")


def load_text(p: str) -> str:
    """Load text file content."""
    with open(p, "r", encoding="utf-8") as f:
        return f.read().strip()


def load_yaml(p: str) -> dict:
    """Load YAML file."""
    if yaml is None:
        raise RuntimeError("PyYAML required: pip install pyyaml")
    with open(p, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def read_csv(p: str) -> List[Dict[str, str]]:
    """Read CSV file as list of dicts."""
    with open(p, "r", encoding="utf-8-sig", newline="") as f:
        return list(csv.DictReader(f))


def write_json(p: str, obj: Any) -> None:
    """Write JSON file."""
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def append_jsonl(p: str, items: List[dict]) -> None:
    """Append items to JSONL file."""
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    with open(p, "a", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")


def token_counts(s: str) -> Dict[str, int]:
    """Count tokens in string."""
    d = {}
    for m in TOKEN_RE.finditer(s or ""):
        k = m.group(1)
        d[k] = d.get(k, 0) + 1
    return d


# Import glossary logic from translate_llm
from translate_llm import load_glossary, build_glossary_constraints, GlossaryEntry


def build_system_batch(style: str, glossary_summary: str) -> str:
    """Build system prompt for batch soft QA."""
    return (
        "ä½ æ˜¯æ‰‹æ¸¸æœ¬åœ°åŒ–è½¯è´¨æ£€ï¼ˆzh-CN â†’ ru-RUï¼‰ã€‚\n\n"
        "ä»»åŠ¡ï¼šåˆ†æç¿»è¯‘è´¨é‡ï¼Œä»…åˆ—å‡ºæœ‰é—®é¢˜çš„é¡¹ã€‚\n\n"
        "æ£€æŸ¥ç»´åº¦ï¼ˆåªæŠ¥é—®é¢˜ï¼Œä¸è¦å¤¸ï¼‰ï¼š\n"
        "- æœ¯è¯­ä¸€è‡´æ€§ï¼ˆglossaryï¼‰\n"
        "- è¯­æ°”ï¼šå®˜æ–¹ä¸ºä¸»ï¼ŒäºŒæ¬¡å…ƒå£è¯­ä¸ºè¾…ï¼ˆé¿å…è¿‡åº¦å£è¯­æˆ–è¿‡åº¦ä¹¦é¢ï¼‰\n"
        "- UI ç®€æ´æ€§ï¼ˆå†—é•¿/é‡å¤/ä¸è‡ªç„¶ï¼‰\n"
        "- æ­§ä¹‰/è¯¯è¯‘/ä¿¡æ¯ç¼ºå¤±\n"
        "- æ ‡ç‚¹ä¸ç¬¦å·ï¼šç¦æ­¢ã€ã€‘ï¼›å ä½ç¬¦å¿…é¡»å®Œæ•´\n\n"
        "è¾“å‡ºæ ¼å¼ï¼ˆç¡¬æ€§ï¼Œä»…è¾“å‡º JSONï¼‰ï¼š\n"
        "{\n"
        '  "items": [\n'
        "    {\n"
        '      "id": "<id>",\n'
        '      "severity": "minor|major",\n'
        '      "issue_type": "terminology|tone|brevity|ambiguity|mistranslation|format|punctuation",\n'
        '      "problem": "<ä¸€å¥è¯æè¿°é—®é¢˜>",\n'
        '      "suggestion": "<ä¸€å¥è¯ç»™å‡ºä¿®å¤æ–¹å‘>",\n'
        '      "preferred_fix_ru": "<å¯é€‰ï¼šå»ºè®®çš„ä¿®å¤åä¿„æ–‡>"\n'
        "    }\n"
        "  ]\n"
        "}\n"
        "è§„åˆ™ï¼š\n"
        '- æ²¡é—®é¢˜åˆ™é¡¹ç›®ä¸å‡ºç°åœ¨ items ä¸­ã€‚\n'
        "- problem/suggestion å¿…é¡»çŸ­å¥ã€‚\n"
        "- æ¯ä¸ªæœ‰é—®é¢˜çš„ id åªè¾“å‡ºä¸€ä¸ªæœ€ä¸¥é‡çš„ itemã€‚\n\n"
        f"æœ¯è¯­è¡¨æ‘˜è¦ï¼ˆå‰ 50 æ¡ï¼‰ï¼š\n{glossary_summary[:1500]}\n\n"
        f"style_guideï¼ˆèŠ‚é€‰ï¼‰ï¼š\n{style[:1000]}\n"
    )


def build_user_prompt(items: List[Dict]) -> str:
    """Build user prompt for soft QA from batch items."""
    # items comes from batch_llm_call where:
    # id = string_id
    # source_text = "SRC: {source_zh} | TGT: {target_ru}"
    # But for soft QA, we might want a cleaner format
    candidates = []
    for it in items:
        # Re-split source_text to get zh and ru
        parts = it.get("source_text", "").split(" | TGT: ")
        src_zh = parts[0].replace("SRC: ", "") if len(parts) > 0 else ""
        tgt_ru = parts[1] if len(parts) > 1 else ""
        
        candidates.append({
            "string_id": it["id"],
            "source_zh": src_zh,
            "target_ru": tgt_ru
        })
    
    return json.dumps(candidates, ensure_ascii=False, indent=2)


def build_glossary_summary(entries: List[GlossaryEntry], max_entries: int = 50) -> str:
    """Build compact glossary summary."""
    approved = [e for e in entries if e.status.lower() == "approved"][:max_entries]
    if not approved:
        return "(æ— )"
    lines = [f"- {e.term_zh} â†’ {e.term_ru}" for e in approved]
    return "\n".join(lines)


def process_batch_results(batch_items: List[Dict]) -> List[dict]:
    """Normalize batch output items into task dicts."""
    valid_tasks = []
    for t in batch_items:
        # Note: items will only contain items with issues due to system prompt
        valid_tasks.append({
            "string_id": t.get("id", ""),
            "type": t.get("issue_type", "issue"),
            "severity": t.get("severity", "minor"),
            "note": f"{t.get('problem', '')} | Suggestion: {t.get('suggestion', '')}",
            "suggested_fix": t.get("preferred_fix_ru", ""),
        })
    return valid_tasks


def main():
    ap = argparse.ArgumentParser(description="LLM-based soft QA (Batch Mode v2.0)")
    ap.add_argument("translated_csv", nargs="?", help="Input translated.csv")
    ap.add_argument("--input", help="Alias for translated_csv")
    ap.add_argument("style_guide_md", nargs="?", default="workflow/style_guide.md", help="Style guide file")
    ap.add_argument("glossary_yaml", nargs="?", default="data/glossary.yaml", help="Glossary file")
    ap.add_argument("rubric_yaml", nargs="?", default="workflow/soft_qa_rubric.yaml", help="Soft QA rubric config (legacy, ignored)")
    ap.add_argument("--batch_size", type=int, default=15, help="Items per batch")
    ap.add_argument("--model", default="claude-haiku-4-5-20251001", help="Model override")
    ap.add_argument("--max_batch_tokens", type=int, default=4000, help="Max tokens per batch")
    ap.add_argument("--out_report", default="data/qa_soft_report.json", help="Output report JSON")
    ap.add_argument("--out_tasks", default="data/repair_tasks.jsonl", help="Output repair tasks JSONL")
    ap.add_argument("--dry-run", action="store_true", 
                    help="Validate configuration without making LLM calls")
    args = ap.parse_args()

    # Resolve input path
    input_path = args.input or args.translated_csv
    if not input_path:
        ap.print_help()
        return 1

    print(f"ğŸ” Soft QA v2.0 (Batch Mode)")
    # Load resources
    rows = read_csv(input_path)
    style = load_text(args.style_guide_md)

    glossary_path = args.glossary_yaml
    glossary_entries = []
    if glossary_path and Path(glossary_path).exists():
        glossary_entries, _ = load_glossary(glossary_path)
    
    glossary_summary = build_glossary_summary(glossary_entries)
    
    print(f"âœ… Loaded {len(rows)} rows from {input_path}")
    print(f"   Glossary: {len(glossary_entries)} entries")
    
    # Filter rows with target_text
    rows_with_target = [r for r in rows if r.get("target_text")]
    print(f"   Rows with translations: {len(rows_with_target)}")
    
    # Dry-run mode
    if getattr(args, 'dry_run', False):
        print()
        print("=" * 60)
        print("DRY-RUN MODE - Validation Summary")
        print("=" * 60)
        
        config = BatchConfig(max_items=args.batch_size, max_tokens=args.max_batch_tokens)
        config.text_fields = ["source_zh", "tokenized_zh", "target_text"]
        batches = split_into_batches(rows_with_target, config)
        
        print(f"[OK] Would create {len(batches)} batches")
        print(f"[OK] Average batch size: {len(rows_with_target) / max(1, len(batches)):.1f}")
        print()
        print("[OK] Dry-run validation PASSED")
        print("=" * 60)
        return 0

    # Initialize LLM
    try:
        llm = LLMClient()
        print(f"âœ… LLM: {llm.default_model}")
    except LLMError as e:
        print(f"âŒ LLM Error: {e}")
        return 1

    print()

    # Split into batches (logic handled by batch_llm_call internally)
    start_time = time.time()
    major = 0
    minor = 0
    all_tasks = 0
    batch_errors = 0

    # Prepare rows for batch_llm_call
    # Source text for soft QA needs both ZH and RU
    batch_rows = []
    for r in rows_with_target:
        src = r.get("source_zh") or r.get("tokenized_zh") or ""
        tgt = r.get("target_text") or ""
        batch_rows.append({
            "id": r.get("string_id"),
            "source_text": f"SRC: {src} | TGT: {tgt}"
        })

    # Execute batch call
    try:
        batch_results = batch_llm_call(
            step="soft_qa",
            rows=batch_rows,
            model=args.model,
            system_prompt=build_system_batch(style, glossary_summary),
            user_prompt_template=build_user_prompt,
            content_type="normal",
            retry=1,
            allow_fallback=True,
            partial_match=True
        )
        
        print("   Batch results received, processing tasks...")
        tasks = process_batch_results(batch_results)
        
        if tasks:
            append_jsonl(args.out_tasks, tasks)
            all_tasks = len(tasks)
            for t in tasks:
                if t.get("severity") == "major":
                    major += 1
                else:
                    minor += 1
                    
    except Exception as e:
        print(f"âŒ Soft QA failed: {e}")
        return 1

    # Calculate batches for report
    config_inst = get_batch_config()
    b_size = config_inst.get_batch_size(args.model, "normal")
    total_batches = (len(batch_rows) + b_size - 1) // b_size if b_size > 0 else 1

    # Write report
    report = {
        "version": "2.0",
        "mode": "batch",
        "has_findings": (major + minor) > 0,
        "summary": {
            "major": major,
            "minor": minor,
            "total_tasks": all_tasks,
            "batch_errors": batch_errors,
            "rows_processed": len(rows_with_target),
            "batches_processed": total_batches,
        },
        "outputs": {
            "repair_tasks_jsonl": args.out_tasks,
        },
    }
    write_json(args.out_report, report)

    # Print summary
    total_elapsed = time.time() - start_time
    print()
    print(f"ğŸ“Š Soft QA Summary:")
    print(f"   Rows processed: {len(rows_with_target)}")
    print(f"   Major issues: {major}")
    print(f"   Minor issues: {minor}")
    print(f"   Total tasks: {all_tasks}")
    print(f"   Total time: {int(total_elapsed)}s")
    print()
    print(f"âœ… Report: {args.out_report}")
    if all_tasks > 0:
        print(f"âœ… Repair tasks: {args.out_tasks}")

    return 0


if __name__ == "__main__":
    # Ensure UTF-8 output on Windows
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    exit(main())
