#!/usr/bin/env python3
"""
QA Hard Script
å¯¹ tokenized ç¿»è¯‘æ–‡æœ¬è¿›è¡Œç¡¬æ€§è§„åˆ™æ ¡éªŒ

Usage:
    python qa_hard.py <translated_csv> <placeholder_map_json> <schema_yaml> <forbidden_txt> <report_json>
"""

import csv
import json
import re
import sys
import yaml
from pathlib import Path
from typing import List, Dict, Set, Tuple
from datetime import datetime
from collections import Counter


class QAHardValidator:
    """ç¡¬æ€§è§„åˆ™æ ¡éªŒå™¨"""
    
    def __init__(self, translated_csv: str, placeholder_map: str, 
                 schema_yaml: str, forbidden_txt: str, report_json: str):
        self.translated_csv = Path(translated_csv)
        self.placeholder_map_path = Path(placeholder_map)
        self.schema_yaml = Path(schema_yaml)
        self.forbidden_txt = Path(forbidden_txt)
        self.report_json = Path(report_json)
        
        # æ•°æ®
        self.placeholder_map: Dict[str, str] = {}
        self.forbidden_patterns: List[str] = []
        self.tag_patterns: List[str] = []
        
        # é”™è¯¯æ”¶é›†
        self.errors: List[Dict] = []
        self.error_counts: Dict[str, int] = {
            'token_mismatch': 0,
            'tag_unbalanced': 0,
            'forbidden_hit': 0,
            'new_placeholder_found': 0
        }
        self.total_rows = 0
        
        # Token æ­£åˆ™
        self.token_pattern = re.compile(r'âŸ¦(PH_\d+|TAG_\d+)âŸ§')
    
    def load_placeholder_map(self) -> bool:
        """åŠ è½½å ä½ç¬¦æ˜ å°„"""
        try:
            with open(self.placeholder_map_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.placeholder_map = data.get('mappings', {})
            print(f"âœ… Loaded {len(self.placeholder_map)} placeholder mappings")
            return True
        except FileNotFoundError:
            print(f"âŒ Error: Placeholder map not found: {self.placeholder_map_path}")
            return False
        except Exception as e:
            print(f"âŒ Error loading placeholder map: {str(e)}")
            return False
    
    def load_schema(self) -> bool:
        """åŠ è½½ schemaï¼Œæå–æ ‡ç­¾æ¨¡å¼"""
        try:
            with open(self.schema_yaml, 'r', encoding='utf-8') as f:
                schema = yaml.safe_load(f)
                patterns = schema.get('placeholder_patterns', [])
                
                # æå–æ‰€æœ‰ TAG ç±»å‹çš„æ¨¡å¼
                for pattern_def in patterns:
                    if pattern_def.get('type') == 'TAG':
                        self.tag_patterns.append(pattern_def['pattern'])
                
            print(f"âœ… Loaded schema with {len(self.tag_patterns)} tag patterns")
            return True
        except FileNotFoundError:
            print(f"âš ï¸  Warning: Schema not found, skipping tag validation")
            return True
        except Exception as e:
            print(f"âš ï¸  Warning: Error loading schema: {str(e)}")
            return True
    
    def load_forbidden_patterns(self) -> bool:
        """åŠ è½½ç¦ç”¨æ¨¡å¼"""
        try:
            with open(self.forbidden_txt, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.forbidden_patterns.append(line)
            
            print(f"âœ… Loaded {len(self.forbidden_patterns)} forbidden patterns")
            return True
        except FileNotFoundError:
            print(f"âš ï¸  Warning: Forbidden patterns file not found")
            return True
        except Exception as e:
            print(f"âš ï¸  Warning: Error loading forbidden patterns: {str(e)}")
            return True
    
    def extract_tokens(self, text: str) -> Set[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ‰€æœ‰ token"""
        if not text:
            return set()
        return set(self.token_pattern.findall(text))
    
    def check_token_mismatch(self, string_id: str, source_text: str, 
                            target_text: str, row_num: int) -> None:
        """æ£€æŸ¥ token æ˜¯å¦åŒ¹é…"""
        source_tokens = self.extract_tokens(source_text)
        target_tokens = self.extract_tokens(target_text)
        
        missing = source_tokens - target_tokens
        extra = target_tokens - source_tokens
        
        if missing:
            for token in missing:
                self.errors.append({
                    'row': row_num,
                    'string_id': string_id,
                    'type': 'token_mismatch',
                    'detail': f"missing âŸ¦{token}âŸ§ in target_text",
                    'source': source_text,
                    'target': target_text
                })
                self.error_counts['token_mismatch'] += 1
        
        if extra:
            for token in extra:
                self.errors.append({
                    'row': row_num,
                    'string_id': string_id,
                    'type': 'token_mismatch',
                    'detail': f"extra âŸ¦{token}âŸ§ in target_text",
                    'source': source_text,
                    'target': target_text
                })
                self.error_counts['token_mismatch'] += 1
    
    def check_tag_balance(self, string_id: str, target_text: str, 
                         row_num: int) -> None:
        """æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å¹³è¡¡ï¼ˆæˆå¯¹å‡ºç°ï¼‰"""
        if not target_text:
            return
        
        # æå–æ‰€æœ‰ TAG token
        tokens = self.extract_tokens(target_text)
        tag_tokens = [t for t in tokens if t.startswith('TAG_')]
        
        if not tag_tokens:
            return
        
        # æ£€æŸ¥æ¯ä¸ª TAG å¯¹åº”çš„åŸå§‹æ ‡ç­¾
        opening_tags = []
        closing_tags = []
        
        for tag_token in tag_tokens:
            original = self.placeholder_map.get(tag_token, '')
            
            # ç®€å•åˆ¤æ–­ï¼šä»¥ </ å¼€å¤´çš„æ˜¯é—­åˆæ ‡ç­¾
            if original.startswith('</'):
                closing_tags.append(tag_token)
            elif original.startswith('<') and not original.startswith('</'):
                opening_tags.append(tag_token)
        
        # æ£€æŸ¥æ•°é‡æ˜¯å¦å¹³è¡¡
        if len(opening_tags) != len(closing_tags):
            self.errors.append({
                'row': row_num,
                'string_id': string_id,
                'type': 'tag_unbalanced',
                'detail': f"unbalanced tags: {len(opening_tags)} opening, {len(closing_tags)} closing",
                'target': target_text,
                'opening_tags': opening_tags,
                'closing_tags': closing_tags
            })
            self.error_counts['tag_unbalanced'] += 1
    
    def check_forbidden_patterns(self, string_id: str, target_text: str, 
                                 row_num: int) -> None:
        """æ£€æŸ¥ç¦ç”¨æ¨¡å¼"""
        if not target_text:
            return
        
        for pattern in self.forbidden_patterns:
            try:
                if re.search(pattern, target_text, re.IGNORECASE):
                    self.errors.append({
                        'row': row_num,
                        'string_id': string_id,
                        'type': 'forbidden_hit',
                        'detail': f"matched forbidden pattern: {pattern}",
                        'target': target_text
                    })
                    self.error_counts['forbidden_hit'] += 1
            except re.error as e:
                # è·³è¿‡æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼
                pass
    
    def check_new_placeholders(self, string_id: str, target_text: str, 
                              row_num: int) -> None:
        """æ£€æŸ¥æ˜¯å¦å‡ºç°äº†æœªç»å†»ç»“çš„æ–°å ä½ç¬¦"""
        if not target_text:
            return
        
        # æ£€æŸ¥å¸¸è§å ä½ç¬¦æ¨¡å¼ï¼ˆåº”è¯¥å·²ç»è¢«å†»ç»“ï¼‰
        suspicious_patterns = [
            (r'\{\d+\}', 'C# numbered placeholder'),
            (r'\{[a-zA-Z_][a-zA-Z0-9_]*\}', 'C# named placeholder'),
            (r'%[sdf]', 'printf-style placeholder'),
            (r'<color=#?[0-9A-Fa-f]{6,8}>', 'Unity color tag'),
            (r'</color>', 'Unity closing tag'),
        ]
        
        for pattern, desc in suspicious_patterns:
            matches = re.findall(pattern, target_text)
            if matches:
                for match in matches:
                    self.errors.append({
                        'row': row_num,
                        'string_id': string_id,
                        'type': 'new_placeholder_found',
                        'detail': f"found unfrozen {desc}: {match}",
                        'target': target_text
                    })
                    self.error_counts['new_placeholder_found'] += 1
    
    def validate_csv(self) -> bool:
        """éªŒè¯ CSV æ–‡ä»¶"""
        try:
            with open(self.translated_csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['string_id', 'tokenized_zh']
                if not all(field in reader.fieldnames for field in required_fields):
                    print(f"âŒ Error: Missing required fields. Need: {required_fields}")
                    return False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¿»è¯‘åˆ—
                target_field = None
                for possible_field in ['target_text', 'translated_text', 'target_zh', 'tokenized_target']:
                    if possible_field in reader.fieldnames:
                        target_field = possible_field
                        break
                
                if not target_field:
                    print(f"âŒ Error: No target translation field found")
                    print(f"   Available fields: {reader.fieldnames}")
                    return False
                
                print(f"âœ… Using '{target_field}' as target translation field")
                print()
                
                # é€è¡ŒéªŒè¯
                for idx, row in enumerate(reader, start=2):
                    self.total_rows += 1
                    
                    string_id = row.get('string_id', '')
                    source_text = row.get('tokenized_zh', '')
                    target_text = row.get(target_field, '')
                    
                    # è·³è¿‡ç©ºç¿»è¯‘
                    if not target_text or not target_text.strip():
                        continue
                    
                    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
                    self.check_token_mismatch(string_id, source_text, target_text, idx)
                    self.check_tag_balance(string_id, target_text, idx)
                    self.check_forbidden_patterns(string_id, target_text, idx)
                    self.check_new_placeholders(string_id, target_text, idx)
                
                return True
                
        except FileNotFoundError:
            print(f"âŒ Error: Translated CSV not found: {self.translated_csv}")
            return False
        except Exception as e:
            print(f"âŒ Error validating CSV: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_report(self) -> None:
        """ç”Ÿæˆ JSON æŠ¥å‘Š"""
        report = {
            'has_errors': len(self.errors) > 0,
            'total_rows': self.total_rows,
            'error_counts': self.error_counts,
            'errors': self.errors,
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'input_file': str(self.translated_csv),
                'total_errors': len(self.errors)
            }
        }
        
        with open(self.report_json, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
    def print_summary(self) -> None:
        """æ‰“å°éªŒè¯æ€»ç»“"""
        print(f"\nğŸ“Š QA Validation Summary:")
        print(f"   Total rows checked: {self.total_rows}")
        print(f"   Total errors: {len(self.errors)}")
        print()
        
        if self.error_counts['token_mismatch'] > 0:
            print(f"   âŒ Token mismatch: {self.error_counts['token_mismatch']}")
        
        if self.error_counts['tag_unbalanced'] > 0:
            print(f"   âŒ Tag unbalanced: {self.error_counts['tag_unbalanced']}")
        
        if self.error_counts['forbidden_hit'] > 0:
            print(f"   âŒ Forbidden patterns: {self.error_counts['forbidden_hit']}")
        
        if self.error_counts['new_placeholder_found'] > 0:
            print(f"   âŒ New placeholders found: {self.error_counts['new_placeholder_found']}")
        
        print()
        
        if len(self.errors) > 0:
            print(f"âŒ Validation FAILED with {len(self.errors)} errors")
            print(f"   See detailed report: {self.report_json}")
            print()
            print("   Sample errors:")
            for error in self.errors[:5]:
                print(f"   - [{error['type']}] {error['string_id']}: {error['detail']}")
        else:
            print(f"âœ… All checks passed!")
            print(f"   Report saved to: {self.report_json}")
    
    def run(self) -> bool:
        """è¿è¡Œ QA éªŒè¯"""
        print(f"ğŸš€ Starting QA Hard validation...")
        print(f"   Input CSV: {self.translated_csv}")
        print(f"   Placeholder map: {self.placeholder_map_path}")
        print(f"   Schema: {self.schema_yaml}")
        print(f"   Forbidden patterns: {self.forbidden_txt}")
        print(f"   Output report: {self.report_json}")
        print()
        
        # åŠ è½½èµ„æº
        if not self.load_placeholder_map():
            return False
        
        self.load_schema()
        self.load_forbidden_patterns()
        
        print()
        
        # éªŒè¯ CSV
        if not self.validate_csv():
            return False
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        return len(self.errors) == 0


def main():
    """ä¸»å…¥å£"""
    if len(sys.argv) != 6:
        print("Usage: python qa_hard.py <translated_csv> <placeholder_map_json> <schema_yaml> <forbidden_txt> <report_json>")
        print()
        print("Example:")
        print("  python qa_hard.py data/translated.csv data/placeholder_map.json workflow/placeholder_schema.yaml workflow/forbidden_patterns.txt data/qa_report.json")
        sys.exit(1)
    
    validator = QAHardValidator(
        translated_csv=sys.argv[1],
        placeholder_map=sys.argv[2],
        schema_yaml=sys.argv[3],
        forbidden_txt=sys.argv[4],
        report_json=sys.argv[5]
    )
    
    success = validator.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
