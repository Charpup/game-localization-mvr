#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QA Hard Script v2.0
å¯¹ tokenized ç¿»è¯‘æ–‡æœ¬è¿›è¡Œç¡¬æ€§è§„åˆ™æ ¡éªŒ

èåˆç‰ˆæœ¬ï¼šç»“åˆ v1.0 çš„å®Œæ•´æ€§å’Œ v2.0 çš„ç°ä»£åŒ–ç‰¹æ€§

Usage:
    python qa_hard.py <translated_csv> <placeholder_map_json> <schema_yaml> <forbidden_txt> <report_json>

Features:
    - ä½¿ç”¨ schema v2.0 æ ¼å¼ (patterns, paired_tags)
    - 4 ç±»é”™è¯¯æ£€æŸ¥ï¼štoken_mismatch, tag_unbalanced, forbidden_hit, new_placeholder_found
    - ä½¿ç”¨ paired_tags è¿›è¡Œç²¾ç¡®çš„æ ‡ç­¾é…å¯¹æ£€æŸ¥
    - åŠ¨æ€åŠ è½½æ–°å ä½ç¬¦æ£€æµ‹æ¨¡å¼
    - æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼–è¯‘æ­£åˆ™ï¼‰
    - é™åˆ¶é”™è¯¯è¾“å‡ºï¼ˆ2000æ¡ï¼‰
    - å‘åå…¼å®¹ schema v1.0
"""

import csv
import json
import re
import sys
from pathlib import Path
from typing import List, Dict, Set, Tuple
from datetime import datetime
from collections import Counter

try:
    import yaml
except ImportError:
    print("âŒ Error: PyYAML is required. Install with: pip install pyyaml")
    sys.exit(1)


class QAHardValidator:
    """ç¡¬æ€§è§„åˆ™æ ¡éªŒå™¨ v2.0"""
    
    def __init__(self, translated_csv: str, placeholder_map: str,
                 schema_yaml: str, forbidden_txt: str, report_json: str):
        self.translated_csv = Path(translated_csv)
        self.placeholder_map_path = Path(placeholder_map)
        self.schema_yaml = Path(schema_yaml)
        self.forbidden_txt = Path(forbidden_txt)
        self.report_json = Path(report_json)
        
        # æ•°æ®
        self.placeholder_map: Dict[str, str] = {}
        self.paired_tags: List[Dict] = []
        self.compiled_patterns: List[re.Pattern] = []
        self.compiled_forbidden: List[re.Pattern] = []
        
        # é”™è¯¯æ”¶é›†
        self.errors: List[Dict] = []
        self.error_counts: Dict[str, int] = {
            'token_mismatch': 0,
            'tag_unbalanced': 0,
            'forbidden_hit': 0,
            'new_placeholder_found': 0
        }
        self.total_rows = 0
        
        # Token æ­£åˆ™
        self.token_pattern = re.compile(r'âŸ¦(PH_\d+|TAG_\d+)âŸ§')
    
    def load_placeholder_map(self) -> bool:
        """åŠ è½½å ä½ç¬¦æ˜ å°„"""
        try:
            with open(self.placeholder_map_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.placeholder_map = data.get('mappings', {})
            print(f"âœ… Loaded {len(self.placeholder_map)} placeholder mappings")
            return True
        except FileNotFoundError:
            print(f"âŒ Error: Placeholder map not found: {self.placeholder_map_path}")
            return False
        except Exception as e:
            print(f"âŒ Error loading placeholder map: {str(e)}")
            return False
    
    def load_schema(self) -> bool:
        """åŠ è½½ schema v2.0ï¼ˆæ”¯æŒ v1.0 fallbackï¼‰"""
        try:
            with open(self.schema_yaml, 'r', encoding='utf-8') as f:
                schema = yaml.safe_load(f)
                schema_version = schema.get('version', 1)
                
                # å°è¯• v2.0 æ ¼å¼
                patterns = schema.get('patterns', None)
                if patterns is None:
                    # Fallback åˆ° v1.0 æ ¼å¼
                    patterns = schema.get('placeholder_patterns', [])
                    print(f"âš ï¸  Using schema v1.0 format (placeholder_patterns)")
                else:
                    print(f"âœ… Using schema v2.0 format (patterns)")
                
                # ç¼–è¯‘æ‰€æœ‰æ¨¡å¼ç”¨äºæ–°å ä½ç¬¦æ£€æµ‹
                for pattern_def in patterns:
                    try:
                        regex = pattern_def.get('regex') or pattern_def.get('pattern')
                        if regex:
                            self.compiled_patterns.append(re.compile(regex))
                    except re.error as e:
                        print(f"âš ï¸  Warning: Invalid regex in pattern '{pattern_def.get('name')}': {e}")
                
                # åŠ è½½ paired_tagsï¼ˆv2.0 æ–°ç‰¹æ€§ï¼‰
                self.paired_tags = schema.get('paired_tags', [])
                
                print(f"âœ… Loaded schema with {len(self.compiled_patterns)} patterns")
                if self.paired_tags:
                    print(f"âœ… Loaded {len(self.paired_tags)} paired tag rules")
                
                return True
                
        except FileNotFoundError:
            print(f"âš ï¸  Warning: Schema not found, skipping advanced validation")
            return True
        except Exception as e:
            print(f"âš ï¸  Warning: Error loading schema: {str(e)}")
            return True
    
    def load_forbidden_patterns(self) -> bool:
        """åŠ è½½ç¦ç”¨æ¨¡å¼ï¼ˆç¼–è¯‘æ­£åˆ™ï¼‰"""
        try:
            with open(self.forbidden_txt, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            self.compiled_forbidden.append(re.compile(line))
                        except re.error as e:
                            print(f"âš ï¸  Warning: Invalid forbidden pattern '{line}': {e}")
            
            print(f"âœ… Loaded {len(self.compiled_forbidden)} forbidden patterns")
            return True
        except FileNotFoundError:
            print(f"âš ï¸  Warning: Forbidden patterns file not found")
            return True
        except Exception as e:
            print(f"âš ï¸  Warning: Error loading forbidden patterns: {str(e)}")
            return True
    
    def extract_tokens(self, text: str) -> Set[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ‰€æœ‰ token"""
        if not text:
            return set()
        return set(self.token_pattern.findall(text))
    
    def check_token_mismatch(self, string_id: str, source_text: str,
                            target_text: str, row_num: int) -> None:
        """æ£€æŸ¥ token æ˜¯å¦åŒ¹é…"""
        source_tokens = self.extract_tokens(source_text)
        target_tokens = self.extract_tokens(target_text)
        
        missing = source_tokens - target_tokens
        extra = target_tokens - source_tokens
        
        if missing:
            for token in missing:
                self.errors.append({
                    'row': row_num,
                    'string_id': string_id,
                    'type': 'token_mismatch',
                    'detail': f"missing âŸ¦{token}âŸ§ in target_text",
                    'source': source_text,
                    'target': target_text
                })
                self.error_counts['token_mismatch'] += 1
        
        if extra:
            for token in extra:
                self.errors.append({
                    'row': row_num,
                    'string_id': string_id,
                    'type': 'token_mismatch',
                    'detail': f"extra âŸ¦{token}âŸ§ in target_text",
                    'source': source_text,
                    'target': target_text
                })
                self.error_counts['token_mismatch'] += 1
    
    def check_tag_balance(self, string_id: str, target_text: str,
                         row_num: int) -> None:
        """
        æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å¹³è¡¡ï¼ˆä½¿ç”¨ paired_tags é…ç½®ï¼‰
        
        v2.0 æ–°ç‰¹æ€§ï¼šä½¿ç”¨ schema ä¸­çš„ paired_tags è¿›è¡Œç²¾ç¡®é…å¯¹æ£€æŸ¥
        Fallbackï¼šå¦‚æœæ²¡æœ‰ paired_tagsï¼Œä½¿ç”¨ç®€å•çš„å¼€æ”¾/é—­åˆæ ‡ç­¾è®¡æ•°
        """
        if not target_text:
            return
        
        # æå–æ‰€æœ‰ TAG token
        tokens = self.extract_tokens(target_text)
        tag_tokens = [t for t in tokens if t.startswith('TAG_')]
        
        if not tag_tokens:
            return
        
        # å¦‚æœæœ‰ paired_tags é…ç½®ï¼Œä½¿ç”¨ç²¾ç¡®é…å¯¹æ£€æŸ¥
        if self.paired_tags:
            self._check_paired_tags(string_id, target_text, tag_tokens, row_num)
        else:
            # Fallbackï¼šç®€å•çš„å¼€æ”¾/é—­åˆæ ‡ç­¾è®¡æ•°
            self._check_tag_count(string_id, target_text, tag_tokens, row_num)
    
    def _check_paired_tags(self, string_id: str, target_text: str,
                          tag_tokens: List[str], row_num: int) -> None:
        """ä½¿ç”¨ paired_tags é…ç½®è¿›è¡Œç²¾ç¡®é…å¯¹æ£€æŸ¥"""
        # ç»Ÿè®¡æ¯ç§æ ‡ç­¾å¯¹çš„æ•°é‡
        for pair_config in self.paired_tags:
            open_pattern = pair_config['open']
            close_pattern = pair_config['close']
            
            open_count = 0
            close_count = 0
            
            for tag_token in tag_tokens:
                original = self.placeholder_map.get(tag_token, '')
                if open_pattern in original and not original.startswith('</'):
                    open_count += 1
                elif close_pattern in original:
                    close_count += 1
            
            # æ£€æŸ¥é…å¯¹æ˜¯å¦å¹³è¡¡
            if open_count != close_count:
                self.errors.append({
                    'row': row_num,
                    'string_id': string_id,
                    'type': 'tag_unbalanced',
                    'detail': f"unbalanced {pair_config.get('description', 'tags')}: {open_count} opening, {close_count} closing",
                    'target': target_text,
                    'open_pattern': open_pattern,
                    'close_pattern': close_pattern
                })
                self.error_counts['tag_unbalanced'] += 1
    
    def _check_tag_count(self, string_id: str, target_text: str,
                        tag_tokens: List[str], row_num: int) -> None:
        """Fallbackï¼šç®€å•çš„å¼€æ”¾/é—­åˆæ ‡ç­¾è®¡æ•°"""
        opening_tags = []
        closing_tags = []
        
        for tag_token in tag_tokens:
            original = self.placeholder_map.get(tag_token, '')
            
            if original.startswith('</'):
                closing_tags.append(tag_token)
            elif original.startswith('<') and not original.startswith('</'):
                opening_tags.append(tag_token)
        
        if len(opening_tags) != len(closing_tags):
            self.errors.append({
                'row': row_num,
                'string_id': string_id,
                'type': 'tag_unbalanced',
                'detail': f"unbalanced tags: {len(opening_tags)} opening, {len(closing_tags)} closing",
                'target': target_text,
                'opening_tags': opening_tags,
                'closing_tags': closing_tags
            })
            self.error_counts['tag_unbalanced'] += 1
    
    def check_forbidden_patterns(self, string_id: str, target_text: str,
                                 row_num: int) -> None:
        """æ£€æŸ¥ç¦ç”¨æ¨¡å¼ï¼ˆä½¿ç”¨ç¼–è¯‘çš„æ­£åˆ™ï¼‰"""
        if not target_text:
            return
        
        for pattern in self.compiled_forbidden:
            try:
                if pattern.search(target_text):
                    self.errors.append({
                        'row': row_num,
                        'string_id': string_id,
                        'type': 'forbidden_hit',
                        'detail': f"matched forbidden pattern: {pattern.pattern}",
                        'target': target_text
                    })
                    self.error_counts['forbidden_hit'] += 1
                    break  # åªæŠ¥å‘Šç¬¬ä¸€ä¸ªåŒ¹é…çš„ç¦ç”¨æ¨¡å¼
            except Exception:
                pass
    
    def check_new_placeholders(self, string_id: str, target_text: str,
                              row_num: int) -> None:
        """
        æ£€æŸ¥æ˜¯å¦å‡ºç°äº†æœªç»å†»ç»“çš„æ–°å ä½ç¬¦
        
        v2.0 æ”¹è¿›ï¼šåŠ¨æ€ä» schema åŠ è½½æ£€æµ‹æ¨¡å¼
        """
        if not target_text:
            return
        
        # ä½¿ç”¨ä» schema åŠ è½½çš„æ¨¡å¼
        for pattern in self.compiled_patterns:
            try:
                matches = pattern.findall(target_text)
                if matches:
                    # ç¡®ä¿åŒ¹é…çš„ä¸æ˜¯ token æ ¼å¼ï¼ˆâŸ¦...âŸ§ï¼‰
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0] if match else ''
                        
                        # è·³è¿‡å·²ç»æ˜¯ token çš„
                        if match.startswith('âŸ¦') or 'âŸ¦' in match:
                            continue
                        
                        self.errors.append({
                            'row': row_num,
                            'string_id': string_id,
                            'type': 'new_placeholder_found',
                            'detail': f"found unfrozen placeholder: {match}",
                            'target': target_text,
                            'pattern': pattern.pattern
                        })
                        self.error_counts['new_placeholder_found'] += 1
                        break  # æ¯ä¸ªæ¨¡å¼åªæŠ¥å‘Šä¸€æ¬¡
            except Exception:
                pass
    
    def validate_csv(self) -> bool:
        """éªŒè¯ CSV æ–‡ä»¶"""
        try:
            with open(self.translated_csv, 'r', encoding='utf-8-sig', newline='') as f:
                reader = csv.DictReader(f)
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['string_id', 'tokenized_zh']
                if not all(field in reader.fieldnames for field in required_fields):
                    print(f"âŒ Error: Missing required fields. Need: {required_fields}")
                    return False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¿»è¯‘åˆ—
                target_field = None
                for possible_field in ['target_text', 'translated_text', 'target_zh', 'tokenized_target']:
                    if possible_field in reader.fieldnames:
                        target_field = possible_field
                        break
                
                if not target_field:
                    print(f"âŒ Error: No target translation field found")
                    print(f"   Available fields: {reader.fieldnames}")
                    return False
                
                print(f"âœ… Using '{target_field}' as target translation field")
                print()
                
                # é€è¡ŒéªŒè¯
                for idx, row in enumerate(reader, start=2):
                    self.total_rows += 1
                    
                    string_id = row.get('string_id', '')
                    source_text = row.get('tokenized_zh', '')
                    target_text = row.get(target_field, '')
                    
                    # è·³è¿‡ç©ºç¿»è¯‘
                    if not target_text or not target_text.strip():
                        continue
                    
                    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
                    self.check_token_mismatch(string_id, source_text, target_text, idx)
                    self.check_tag_balance(string_id, target_text, idx)
                    self.check_forbidden_patterns(string_id, target_text, idx)
                    self.check_new_placeholders(string_id, target_text, idx)
                
                return True
                
        except FileNotFoundError:
            print(f"âŒ Error: Translated CSV not found: {self.translated_csv}")
            return False
        except Exception as e:
            print(f"âŒ Error validating CSV: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_report(self) -> None:
        """ç”Ÿæˆ JSON æŠ¥å‘Šï¼ˆé™åˆ¶é”™è¯¯æ•°é‡ï¼‰"""
        report = {
            'has_errors': len(self.errors) > 0,
            'total_rows': self.total_rows,
            'error_counts': self.error_counts,
            'errors': self.errors[:2000],  # é™åˆ¶åˆ° 2000 æ¡
            'metadata': {
                'version': '2.0',
                'generated_at': datetime.now().isoformat(),
                'input_file': str(self.translated_csv),
                'total_errors': len(self.errors),
                'errors_truncated': len(self.errors) > 2000
            }
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.report_json.parent.mkdir(parents=True, exist_ok=True)
        
        with open(self.report_json, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
    def print_summary(self) -> None:
        """æ‰“å°éªŒè¯æ€»ç»“"""
        print(f"\nğŸ“Š QA Validation Summary:")
        print(f"   Total rows checked: {self.total_rows}")
        print(f"   Total errors: {len(self.errors)}")
        print()
        
        if self.error_counts['token_mismatch'] > 0:
            print(f"   âŒ Token mismatch: {self.error_counts['token_mismatch']}")
        
        if self.error_counts['tag_unbalanced'] > 0:
            print(f"   âŒ Tag unbalanced: {self.error_counts['tag_unbalanced']}")
        
        if self.error_counts['forbidden_hit'] > 0:
            print(f"   âŒ Forbidden patterns: {self.error_counts['forbidden_hit']}")
        
        if self.error_counts['new_placeholder_found'] > 0:
            print(f"   âŒ New placeholders found: {self.error_counts['new_placeholder_found']}")
        
        print()
        
        if len(self.errors) > 0:
            print(f"âŒ Validation FAILED with {len(self.errors)} errors")
            if len(self.errors) > 2000:
                print(f"   âš ï¸  Report truncated to 2000 errors (total: {len(self.errors)})")
            print(f"   See detailed report: {self.report_json}")
            print()
            print("   Sample errors:")
            for error in self.errors[:5]:
                print(f"   - [{error['type']}] {error['string_id']}: {error['detail']}")
        else:
            print(f"âœ… All checks passed!")
            print(f"   Report saved to: {self.report_json}")
    
    def run(self) -> bool:
        """è¿è¡Œ QA éªŒè¯"""
        print(f"ğŸš€ Starting QA Hard validation v2.0...")
        print(f"   Input CSV: {self.translated_csv}")
        print(f"   Placeholder map: {self.placeholder_map_path}")
        print(f"   Schema: {self.schema_yaml}")
        print(f"   Forbidden patterns: {self.forbidden_txt}")
        print(f"   Output report: {self.report_json}")
        print()
        
        # åŠ è½½èµ„æº
        if not self.load_placeholder_map():
            return False
        
        self.load_schema()
        self.load_forbidden_patterns()
        
        print()
        
        # éªŒè¯ CSV
        if not self.validate_csv():
            return False
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        return len(self.errors) == 0


def main():
    """ä¸»å…¥å£"""
    if len(sys.argv) != 6:
        print("Usage: python qa_hard.py <translated_csv> <placeholder_map_json> <schema_yaml> <forbidden_txt> <report_json>")
        print()
        print("Example:")
        print("  python qa_hard.py data/translated.csv data/placeholder_map.json workflow/placeholder_schema.yaml workflow/forbidden_patterns.txt data/qa_report.json")
        sys.exit(1)
    
    validator = QAHardValidator(
        translated_csv=sys.argv[1],
        placeholder_map=sys.argv[2],
        schema_yaml=sys.argv[3],
        forbidden_txt=sys.argv[4],
        report_json=sys.argv[5]
    )
    
    success = validator.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
