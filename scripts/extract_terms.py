#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Extract Terms Script v2.0
ä»æºæ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­å€™é€‰

å¤šæ¨¡å¼æ”¯æŒï¼š
  - jieba: ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯ï¼ˆé»˜è®¤ï¼‰
  - heuristic: å¯å‘å¼æ­£åˆ™æå–ï¼ˆæ— ä¾èµ–ï¼‰
  - llm: LLM API æå–ï¼ˆæ˜¾å¼è°ƒç”¨ï¼‰

Usage:
    python extract_terms.py <input_csv> <output_yaml> [options]
    
Options:
    --mode MODE       æå–æ¨¡å¼: jieba/heuristic/llm (é»˜è®¤: jieba)
    --glossary FILE   ç°æœ‰æœ¯è¯­è¡¨æ–‡ä»¶
    --min-freq N      æœ€å°è¯é¢‘ (é»˜è®¤: 2)
    --model MODEL     LLM æ¨¡å‹ (ä»… llm æ¨¡å¼)
    --provider PROV   LLM æä¾›å•† (ä»… llm æ¨¡å¼)
"""

import csv
import re
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime
from collections import Counter, defaultdict
from abc import ABC, abstractmethod

# Unified batch infrastructure
try:
    from runtime_adapter import batch_llm_call, log_llm_progress, BatchConfig
except ImportError:
    batch_llm_call = None

try:
    import yaml
except ImportError:
    print("âŒ Error: PyYAML is required. Install with: pip install pyyaml")
    sys.exit(1)

# æ£€æŸ¥ jieba æ˜¯å¦å¯ç”¨
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False


# ============================================================================
# åŸºç±»
# ============================================================================

class BaseExtractor(ABC):
    """æœ¯è¯­æå–å™¨åŸºç±»"""
    
    def __init__(self, glossary_terms: Set[str] = None):
        self.glossary_terms = glossary_terms or set()
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯"""
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'äº›',
            'ä¸ª', 'ä¸º', 'ä¸', 'æˆ–', 'åŠ', 'ä¹‹', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å¦‚æœ',
            'å¯ä»¥', 'å·²ç»', 'è¿˜', 'ä»', 'å¯¹', 'æŠŠ', 'è¢«', 'è®©', 'ç»™', 'ç”¨'
        }
        
        stopwords_file = Path(__file__).parent.parent / 'workflow' / 'stopwords.txt'
        if stopwords_file.exists():
            with open(stopwords_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        stopwords.add(word)
        
        return stopwords
    
    @abstractmethod
    def extract(self, texts: List[Dict]) -> List[Dict]:
        """æå–æœ¯è¯­å€™é€‰"""
        pass
    
    @property
    @abstractmethod
    def mode_name(self) -> str:
        """æ¨¡å¼åç§°"""
        pass


# ============================================================================
# Jieba æ¨¡å¼
# ============================================================================

class JiebaExtractor(BaseExtractor):
    """ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯çš„æœ¯è¯­æå–å™¨"""
    
    @property
    def mode_name(self) -> str:
        return "jieba"
    
    def extract(self, texts: List[Dict], min_freq: int = 2, 
                min_len: int = 2, max_len: int = 8) -> List[Dict]:
        """ä½¿ç”¨ jieba åˆ†è¯æå–æœ¯è¯­"""
        if not JIEBA_AVAILABLE:
            raise RuntimeError("jieba æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install jieba")
        
        freq = Counter()
        positions = defaultdict(list)
        
        for item in texts:
            string_id = item['string_id']
            text = item['text']
            
            # ç§»é™¤ token
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            
            # jieba åˆ†è¯
            words = jieba.cut(text_clean)
            
            for word in words:
                word = word.strip()
                if not word:
                    continue
                if word in self.stopwords:
                    continue
                if len(word) < min_len or len(word) > max_len:
                    continue
                if re.match(r'^[0-9]+$', word) or re.match(r'^[a-zA-Z]+$', word):
                    continue
                if re.match(r'^[^\w]+$', word):
                    continue
                
                freq[word] += 1
                if len(positions[word]) < 5:
                    positions[word].append({'string_id': string_id, 'source_zh': text})
        
        # ç”Ÿæˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, count in freq.most_common():
            if count < min_freq:
                break
            if term in self.glossary_terms:
                continue
            
            candidates.append({
                'term_zh': term,
                'score': count,
                'status': 'proposed',
                'examples': positions[term]
            })
        
        return candidates


# ============================================================================
# Heuristic æ¨¡å¼
# ============================================================================

# Module weights for weighted extraction
MODULE_WEIGHTS = {
    "ui_button": 2.0,
    "ui_label": 1.8,
    "system_notice": 1.5,
    "skill_desc": 2.2,
    "item_desc": 1.8,
    "dialogue": 0.3,
    "misc": 1.0
}

# IP/world term bonus patterns
IP_TERM_PATTERNS = ['ä¹‹', 'æ‘', 'å½±', 'é', 'æœ¯', 'å¼', 'å°', 'ä¸¸', 'å¿', 'çœ¼', 'é“', 'æ—', 'å›½', 'éš']

class HeuristicExtractor(BaseExtractor):
    """å¯å‘å¼æœ¯è¯­æå–å™¨ï¼ˆæ— ä¾èµ–ï¼‰"""
    
    # CJK è¿ç»­å­—ç¬¦ (2-8å­—)
    RE_CJK = re.compile(r"[\u4e00-\u9fff]{2,8}")
    # æ‹¬å·å†…è¯
    RE_BRACKETED = re.compile(r"[ã€Šã€ã€Œã€](.+?)[ã€‹ã€‘ã€ã€]")
    
    # é¢å¤–åœç”¨è¯ï¼ˆå¸¸è§ä½†éæœ¯è¯­çš„è¯ï¼‰
    EXTRA_STOP = {
        "ç³»ç»Ÿ", "æç¤º", "ç‚¹å‡»", "ç¡®å®š", "å–æ¶ˆ", "å¼€å§‹", "ç»“æŸ", "ä»Šæ—¥", "æ˜æ—¥",
        "è·å¾—", "ä½¿ç”¨", "è¿›è¡Œ", "å®Œæˆ", "ä»»åŠ¡", "æ´»åŠ¨", "å¥–åŠ±", "é“å…·", "è§’è‰²",
    }
    
    @property
    def mode_name(self) -> str:
        return "heuristic"
    
    def extract(self, texts: List[Dict], max_terms: int = 300) -> List[Dict]:
        """ä½¿ç”¨å¯å‘å¼è§„åˆ™æå–æœ¯è¯­"""
        freq = Counter()
        examples = defaultdict(list)
        
        all_stop = self.stopwords | self.EXTRA_STOP
        
        for item in texts:
            string_id = item['string_id']
            text = item['text']
            
            # ç§»é™¤ token
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            
            # æ‹¬å·å†…è¯ï¼ˆæƒé‡ +2ï¼‰
            for m in self.RE_BRACKETED.finditer(text_clean):
                term = m.group(1).strip()
                if 2 <= len(term) <= 12 and term not in all_stop:
                    freq[term] += 2
                    if len(examples[term]) < 5:
                        examples[term].append({'string_id': string_id, 'source_zh': text})
            
            # CJK è¿ç»­ä¸²ï¼ˆæƒé‡ +1ï¼‰
            for m in self.RE_CJK.finditer(text_clean):
                term = m.group(0)
                if term not in all_stop:
                    freq[term] += 1
                    if len(examples[term]) < 3:
                        examples[term].append({'string_id': string_id, 'source_zh': text})
        
        # ç”Ÿæˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, count in freq.most_common(max_terms):
            if term in self.glossary_terms:
                continue
            
            candidates.append({
                'term_zh': term,
                'score': count,
                'status': 'proposed',
                'examples': examples[term]
            })
        
        return candidates


# ============================================================================
# Weighted æ¨¡å¼ (uses normalized.csv with module_tag)
# ============================================================================

class WeightedExtractor(BaseExtractor):
    """ä½¿ç”¨ normalized.csv çš„åŠ æƒæœ¯è¯­æå–å™¨"""
    
    def __init__(self, glossary_terms: Set[str] = None, blacklist_path: str = None):
        super().__init__(glossary_terms)
        self.blacklist = self._load_blacklist(blacklist_path)
    
    def _load_blacklist(self, path: str = None) -> Set[str]:
        """Load generic terms blacklist."""
        blacklist = set()
        
        # Default path
        if not path:
            path = Path(__file__).parent.parent / 'glossary' / 'generic_terms_zh.txt'
        
        if Path(path).exists():
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        blacklist.add(line)
        
        return blacklist
    
    @property
    def mode_name(self) -> str:
        return "weighted"
    
    def _compute_termness(self, term: str, module_mix: Dict[str, float]) -> float:
        """Compute term-ness score (0.0-1.0)."""
        score = 0.5  # Base score
        
        # Length bonus
        if len(term) >= 3:
            score += 0.1
        if len(term) >= 4:
            score += 0.1
        
        # IP/world term pattern bonus
        for pattern in IP_TERM_PATTERNS:
            if pattern in term:
                score += 0.15
                break
        
        # Skill-heavy terms get bonus
        if module_mix.get('skill_desc', 0) > 0.3:
            score += 0.1
        if module_mix.get('item_desc', 0) > 0.3:
            score += 0.05
        
        # Dialogue-heavy terms get penalty
        if module_mix.get('dialogue', 0) > 0.5:
            score -= 0.2
        
        return max(0.0, min(1.0, score))
    
    def extract(self, texts: List[Dict], min_freq: int = 2, 
                min_len: int = 2, max_len: int = 8,
                min_termness: float = 0.3) -> List[Dict]:
        """Weighted extraction from normalized texts with module_tag."""
        if not JIEBA_AVAILABLE:
            raise RuntimeError("jieba æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install jieba")
        
        # Per-term stats
        freq = Counter()
        weighted_freq = Counter()
        module_counts = defaultdict(lambda: Counter())
        positions = defaultdict(list)
        
        for item in texts:
            string_id = item['string_id']
            text = item['text']
            module_tag = item.get('module_tag', 'misc')
            weight = MODULE_WEIGHTS.get(module_tag, 1.0)
            
            # Clean text
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            text_clean = re.sub(r'\{[\d\w]+\}', '', text_clean)
            text_clean = re.sub(r'<[^>]+>', '', text_clean)
            
            # Jieba segment
            words = jieba.cut(text_clean)
            
            for word in words:
                word = word.strip()
                if not word:
                    continue
                if word in self.stopwords:
                    continue
                if word in self.blacklist:
                    continue
                if len(word) < min_len or len(word) > max_len:
                    continue
                if re.match(r'^[0-9]+$', word) or re.match(r'^[a-zA-Z]+$', word):
                    continue
                if re.match(r'^[^\w]+$', word):
                    continue
                
                freq[word] += 1
                weighted_freq[word] += weight
                module_counts[word][module_tag] += 1
                if len(positions[word]) < 3:
                    positions[word].append({'string_id': string_id, 'source_zh': text[:100]})
        
        # Generate candidates
        candidates = []
        filtered_counts = {'generic_blacklist': 0, 'low_freq': 0, 'in_glossary': 0, 'low_termness': 0}
        
        for term, wfreq in weighted_freq.most_common():
            raw_freq = freq[term]
            
            if raw_freq < min_freq:
                filtered_counts['low_freq'] += 1
                continue
            if term in self.glossary_terms:
                filtered_counts['in_glossary'] += 1
                continue
            
            # Compute module mix
            total_module = sum(module_counts[term].values())
            module_mix = {k: v/total_module for k, v in module_counts[term].items()}
            
            # Compute termness
            termness = self._compute_termness(term, module_mix)
            if termness < min_termness:
                filtered_counts['low_termness'] += 1
                continue
            
            candidates.append({
                'term_zh': term,
                'score': round(wfreq, 2),
                'raw_freq': raw_freq,
                'weighted_freq': round(wfreq, 2),
                'module_mix': {k: round(v, 2) for k, v in module_mix.items()},
                'termness_score': round(termness, 2),
                'status': 'proposed',
                'examples': positions[term]
            })
        
        print(f"  Filtered: {filtered_counts}")
        return candidates


# ============================================================================
# LLM æ¨¡å¼
# ============================================================================

def build_system_prompt_extract() -> str:
    """Build system prompt for term extraction."""
    return (
        "ä½ æ˜¯æ‰‹æ¸¸æœ¬åœ°åŒ–æœ¯è¯­æå–ä¸“å®¶ã€‚\n\n"
        "ä»»åŠ¡ï¼šä»æä¾›çš„æ–‡æœ¬ä¸­æå–å€™é€‰æœ¯è¯­ï¼ˆzh-CNï¼‰ã€‚\n"
        "ç›®æ ‡ï¼šè¯†åˆ«å…·æœ‰ä¸“ä¸šæ€§ã€ä»£è¡¨æ€§æˆ–ç¿»è¯‘éš¾åº¦çš„è¯æ±‡ï¼ŒåŒ…æ‹¬ï¼š\n"
        "- æ¸¸æˆæœºåˆ¶/æ•°å€¼åç§°\n"
        "- ä¸“å±åè¯ï¼ˆäººåã€åœ°åã€ç»„ç»‡ã€æŠ€èƒ½åã€é“å…·åï¼‰\n"
        "- UI ç•Œé¢å›ºå®šç”¨è¯­\n\n"
        "è¾“å‡ºæ ¼å¼ï¼ˆç¡¬æ€§ JSONï¼‰ï¼š\n"
        "{\n"
        '  "items": [\n'
        "    {\n"
        '      "id": "<string_id>",\n'
        '      "terms": ["æœ¯è¯­1", "æœ¯è¯­2", ...]\n'
        "    }\n"
        "  ]\n"
        "}\n"
        "è§„åˆ™ï¼š\n"
        "- å¦‚æœè¡Œå†…æ²¡æœ‰æœ¯è¯­ï¼Œä¸è¦å‡ºç°åœ¨ items ä¸­ã€‚\n"
        "- æœ¯è¯­åº”ä¸º 2-8 å­—ï¼Œé¿å…æå–é•¿éš¾å¥ã€‚\n"
        "- æ’é™¤é€šç”¨ä»£è¯å’Œæç®€å¸¸ç”¨è¯ã€‚\n"
    )

def build_user_prompt_extract(items: List[Dict]) -> str:
    """Build user prompt for extraction."""
    # items from batch_llm_call: list of {'id', 'source_text'}
    clean_items = []
    for it in items:
        clean_items.append({
            "string_id": it["id"],
            "text": it["source_text"]
        })
    return json.dumps(clean_items, ensure_ascii=False, indent=2)

class LLMExtractor(BaseExtractor):
    """ä½¿ç”¨ LLM API çš„æœ¯è¯­æå–å™¨"""
    
    def __init__(self, glossary_terms: Set[str] = None, 
                 provider: str = None, model: str = None):
        super().__init__(glossary_terms)
        self.provider = provider
        self.model = model or "claude-haiku-4-5-20251001"
    
    @property
    def mode_name(self) -> str:
        return "llm"
    
    def extract(self, texts: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ LLM æå–æœ¯è¯­"""
        if not batch_llm_call:
            raise RuntimeError("batch_llm_call is not available")
        
        print(f"âœ… ä½¿ç”¨ LLM æ¨¡å¼: {self.model}")
        
        # å‡†å¤‡ batch_rows
        batch_rows = []
        id_to_original_text = {}
        for item in texts:
            sid = str(item['string_id'])
            text = item['text']
            batch_rows.append({
                "id": sid,
                "source_text": text
            })
            id_to_original_text[sid] = text

        # æ‰§è¡Œæ‰¹æ¬¡è°ƒç”¨
        try:
            batch_results = batch_llm_call(
                step="glossary_extract",
                rows=batch_rows,
                model=self.model,
                system_prompt=build_system_prompt_extract(),
                user_prompt_template=build_user_prompt_extract,
                content_type="normal",
                retry=1,
                allow_fallback=True,
                partial_match=True
            )
        except Exception as e:
            print(f"âŒ LLM æå–å¤±è´¥: {e}")
            return []

        # èšåˆç»“æœ
        term_freq = Counter()
        term_examples = defaultdict(list)
        
        for item in batch_results:
            sid = str(item.get("id", ""))
            terms = item.get("terms", [])
            if not isinstance(terms, list):
                continue
                
            orig_text = id_to_original_text.get(sid, "")
            
            for term in terms:
                term = term.strip()
                if not term or term in self.glossary_terms:
                    continue
                if term in self.stopwords:
                    continue
                
                term_freq[term] += 1
                if len(term_examples[term]) < 5:
                    term_examples[term].append({
                        "string_id": sid,
                        "source_zh": orig_text
                    })

        # æ„å»ºæœ€ç»ˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, count in term_freq.most_common():
            candidates.append({
                'term_zh': term,
                'score': count,
                'status': 'proposed',
                'examples': term_examples[term]
            })
            
        return candidates


# ============================================================================
# ä¸»é€»è¾‘
# ============================================================================

def load_glossary(glossary_path: str) -> Set[str]:
    """åŠ è½½æœ¯è¯­è¡¨"""
    if not glossary_path or not Path(glossary_path).exists():
        return set()
    
    with open(glossary_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
        terms = data.get('terms', {})
        return set(terms.keys())


def load_source_texts(input_csv: str) -> List[Dict]:
    """åŠ è½½æºæ–‡æœ¬ - æ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼Œæ”¯æŒ normalized.csv"""
    texts = []
    with open(input_csv, 'r', encoding='utf-8-sig', newline='') as f:
        reader = csv.DictReader(f)
        fields = reader.fieldnames or []
        
        # Flexible column name mapping
        id_col = None
        zh_col = None
        tag_col = None  # For weighted mode
        
        for name in ['string_id', 'id', 'ID', 'StringId']:
            if name in fields:
                id_col = name
                break
        
        for name in ['source_zh', 'zh', 'ZH', 'text', 'Text', 'text_zh', 'SourceText']:
            if name in fields:
                zh_col = name
                break
        
        # Optional: module_tag for weighted mode
        if 'module_tag' in fields:
            tag_col = 'module_tag'
        
        if not id_col or not zh_col:
            raise ValueError(f"CSV å¿…é¡»åŒ…å« ID åˆ— (string_id/id) å’Œæºæ–‡æœ¬åˆ— (source_zh/zh/text). Found: {fields}")
        
        for row in reader:
            if row.get(zh_col):
                item = {
                    'string_id': row[id_col],
                    'text': row[zh_col]
                }
                if tag_col:
                    item['module_tag'] = row.get(tag_col, 'misc')
                texts.append(item)
    
    return texts


def save_candidates(candidates: List[Dict], output_path: str, 
                   mode: str, texts_count: int, config: Dict = None) -> None:
    """ä¿å­˜å€™é€‰åˆ—è¡¨"""
    language_pair = {'source': 'zh-CN', 'target': 'ru-RU'}
    if config:
        language_pair = config.get('language_pair', language_pair)
    
    output = {
        'version': '2.0',
        'extraction_mode': mode,
        'generated_at': datetime.now().isoformat(),
        'language_pair': language_pair,
        'statistics': {
            'total_strings': texts_count,
            'unique_terms': len(candidates),
            'total_occurrences': sum(c['score'] for c in candidates) if candidates else 0
        },
        'candidates': candidates
    }
    
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.safe_dump(output, f, allow_unicode=True, sort_keys=False)


def main():
    parser = argparse.ArgumentParser(
        description='Extract Terms v2.0 - å¤šæ¨¡å¼æœ¯è¯­æå–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ¨¡å¼è¯´æ˜:
  jieba      ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯ï¼ˆé»˜è®¤ï¼Œéœ€å®‰è£… jiebaï¼‰
  heuristic  å¯å‘å¼æ­£åˆ™æå–ï¼ˆæ— ä¾èµ–ï¼Œå¿«é€Ÿï¼‰
  llm        ä½¿ç”¨ LLM API æå–ï¼ˆéœ€é…ç½® API å¯†é’¥ï¼‰

ç¤ºä¾‹:
  python extract_terms.py data/input.csv data/terms.yaml
  python extract_terms.py data/input.csv data/terms.yaml --mode heuristic
  python extract_terms.py data/input.csv data/terms.yaml --mode llm --model gpt-4o
        """
    )
    
    parser.add_argument('input_csv', help='è¾“å…¥ CSV æ–‡ä»¶ (æˆ– normalized.csv)')
    parser.add_argument('output_yaml', help='è¾“å‡ºæœ¯è¯­å€™é€‰ YAML')
    parser.add_argument('--mode', choices=['jieba', 'heuristic', 'llm', 'weighted'], 
                       default='jieba', help='æå–æ¨¡å¼ (é»˜è®¤: jieba, weightedéœ€è¦normalized.csv)')
    parser.add_argument('--glossary', help='ç°æœ‰æœ¯è¯­è¡¨æ–‡ä»¶')
    parser.add_argument('--blacklist', help='é€šç”¨è¯é»‘åå• (weightedæ¨¡å¼)')
    parser.add_argument('--min-freq', type=int, default=2, help='æœ€å°è¯é¢‘ (é»˜è®¤: 2)')
    parser.add_argument('--min-termness', type=float, default=0.3, help='æœ€å°æœ¯è¯­åº¦ (weightedæ¨¡å¼, é»˜è®¤: 0.3)')
    parser.add_argument('--model', help='LLM æ¨¡å‹ (ä»… llm æ¨¡å¼)')
    parser.add_argument('--provider', help='LLM æä¾›å•† (ä»… llm æ¨¡å¼)')
    
    args = parser.parse_args()
    
    print("ğŸš€ Extract Terms v2.0")
    print(f"   è¾“å…¥: {args.input_csv}")
    print(f"   è¾“å‡º: {args.output_yaml}")
    print(f"   æ¨¡å¼: {args.mode}")
    print()
    
    # åŠ è½½æœ¯è¯­è¡¨
    glossary_terms = load_glossary(args.glossary)
    if glossary_terms:
        print(f"âœ… åŠ è½½äº† {len(glossary_terms)} ä¸ªå·²çŸ¥æœ¯è¯­")
    
    # åŠ è½½æºæ–‡æœ¬
    texts = load_source_texts(args.input_csv)
    print(f"âœ… åŠ è½½äº† {len(texts)} æ¡æºæ–‡æœ¬")
    
    # é€‰æ‹©æå–å™¨
    mode = args.mode
    
    # jieba æ¨¡å¼è‡ªåŠ¨ fallback
    if mode == 'jieba' and not JIEBA_AVAILABLE:
        print("âš ï¸  jieba æœªå®‰è£…ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° heuristic æ¨¡å¼")
        mode = 'heuristic'
    
    # åˆ›å»ºæå–å™¨
    if mode == 'jieba':
        extractor = JiebaExtractor(glossary_terms)
        candidates = extractor.extract(texts, min_freq=args.min_freq)
    elif mode == 'heuristic':
        extractor = HeuristicExtractor(glossary_terms)
        candidates = extractor.extract(texts)
    elif mode == 'weighted':
        extractor = WeightedExtractor(glossary_terms, args.blacklist)
        candidates = extractor.extract(texts, min_freq=args.min_freq, min_termness=args.min_termness)
    elif mode == 'llm':
        extractor = LLMExtractor(glossary_terms, args.provider, args.model)
        candidates = extractor.extract(texts)
    
    print(f"\nâœ… æå–äº† {len(candidates)} ä¸ªæœ¯è¯­å€™é€‰")
    
    # åŠ è½½ LLM é…ç½®ï¼ˆç”¨äºè¯­è¨€å¯¹ï¼‰
    config = {}
    config_path = Path(__file__).parent.parent / 'workflow' / 'llm_config.yaml'
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    
    # ä¿å­˜ç»“æœ
    save_candidates(candidates, args.output_yaml, extractor.mode_name, len(texts), config)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output_yaml}")
    
    # æ‰“å° Top 10
    if candidates:
        print(f"\nğŸ“Š é«˜é¢‘æœ¯è¯­ TOP 10:")
        for i, c in enumerate(candidates[:10], 1):
            print(f"   {i}. {c['term_zh']} (score: {c['score']})")
    
    print("\nâœ… æœ¯è¯­æå–å®Œæˆ!")


if __name__ == "__main__":
    main()
