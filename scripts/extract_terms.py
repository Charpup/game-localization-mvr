#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Extract Terms Script v2.0
ä»æºæ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­å€™é€‰

å¤šæ¨¡å¼æ”¯æŒï¼š
  - jieba: ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯ï¼ˆé»˜è®¤ï¼‰
  - heuristic: å¯å‘å¼æ­£åˆ™æå–ï¼ˆæ— ä¾èµ–ï¼‰
  - llm: LLM API æå–ï¼ˆæ˜¾å¼è°ƒç”¨ï¼‰

Usage:
    python extract_terms.py <input_csv> <output_yaml> [options]
    
Options:
    --mode MODE       æå–æ¨¡å¼: jieba/heuristic/llm (é»˜è®¤: jieba)
    --glossary FILE   ç°æœ‰æœ¯è¯­è¡¨æ–‡ä»¶
    --min-freq N      æœ€å°è¯é¢‘ (é»˜è®¤: 2)
    --model MODEL     LLM æ¨¡å‹ (ä»… llm æ¨¡å¼)
    --provider PROV   LLM æä¾›å•† (ä»… llm æ¨¡å¼)
"""

import csv
import re
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime
from collections import Counter, defaultdict
from abc import ABC, abstractmethod

try:
    import yaml
except ImportError:
    print("âŒ Error: PyYAML is required. Install with: pip install pyyaml")
    sys.exit(1)

# æ£€æŸ¥ jieba æ˜¯å¦å¯ç”¨
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False


# ============================================================================
# åŸºç±»
# ============================================================================

class BaseExtractor(ABC):
    """æœ¯è¯­æå–å™¨åŸºç±»"""
    
    def __init__(self, glossary_terms: Set[str] = None):
        self.glossary_terms = glossary_terms or set()
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯"""
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'äº›',
            'ä¸ª', 'ä¸º', 'ä¸', 'æˆ–', 'åŠ', 'ä¹‹', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å¦‚æœ',
            'å¯ä»¥', 'å·²ç»', 'è¿˜', 'ä»', 'å¯¹', 'æŠŠ', 'è¢«', 'è®©', 'ç»™', 'ç”¨'
        }
        
        stopwords_file = Path(__file__).parent.parent / 'workflow' / 'stopwords.txt'
        if stopwords_file.exists():
            with open(stopwords_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        stopwords.add(word)
        
        return stopwords
    
    @abstractmethod
    def extract(self, texts: List[Dict]) -> List[Dict]:
        """æå–æœ¯è¯­å€™é€‰"""
        pass
    
    @property
    @abstractmethod
    def mode_name(self) -> str:
        """æ¨¡å¼åç§°"""
        pass


# ============================================================================
# Jieba æ¨¡å¼
# ============================================================================

class JiebaExtractor(BaseExtractor):
    """ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯çš„æœ¯è¯­æå–å™¨"""
    
    @property
    def mode_name(self) -> str:
        return "jieba"
    
    def extract(self, texts: List[Dict], min_freq: int = 2, 
                min_len: int = 2, max_len: int = 8) -> List[Dict]:
        """ä½¿ç”¨ jieba åˆ†è¯æå–æœ¯è¯­"""
        if not JIEBA_AVAILABLE:
            raise RuntimeError("jieba æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install jieba")
        
        freq = Counter()
        positions = defaultdict(list)
        
        for item in texts:
            string_id = item['string_id']
            text = item['text']
            
            # ç§»é™¤ token
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            
            # jieba åˆ†è¯
            words = jieba.cut(text_clean)
            
            for word in words:
                word = word.strip()
                if not word:
                    continue
                if word in self.stopwords:
                    continue
                if len(word) < min_len or len(word) > max_len:
                    continue
                if re.match(r'^[0-9]+$', word) or re.match(r'^[a-zA-Z]+$', word):
                    continue
                if re.match(r'^[^\w]+$', word):
                    continue
                
                freq[word] += 1
                if len(positions[word]) < 5:
                    positions[word].append({'string_id': string_id, 'source_zh': text})
        
        # ç”Ÿæˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, count in freq.most_common():
            if count < min_freq:
                break
            if term in self.glossary_terms:
                continue
            
            candidates.append({
                'term_zh': term,
                'score': count,
                'status': 'proposed',
                'examples': positions[term]
            })
        
        return candidates


# ============================================================================
# Heuristic æ¨¡å¼
# ============================================================================

class HeuristicExtractor(BaseExtractor):
    """å¯å‘å¼æœ¯è¯­æå–å™¨ï¼ˆæ— ä¾èµ–ï¼‰"""
    
    # CJK è¿ç»­å­—ç¬¦ (2-8å­—)
    RE_CJK = re.compile(r"[\u4e00-\u9fff]{2,8}")
    # æ‹¬å·å†…è¯
    RE_BRACKETED = re.compile(r"[ã€Šã€ã€Œã€](.+?)[ã€‹ã€‘ã€ã€]")
    
    # é¢å¤–åœç”¨è¯ï¼ˆå¸¸è§ä½†éæœ¯è¯­çš„è¯ï¼‰
    EXTRA_STOP = {
        "ç³»ç»Ÿ", "æç¤º", "ç‚¹å‡»", "ç¡®å®š", "å–æ¶ˆ", "å¼€å§‹", "ç»“æŸ", "ä»Šæ—¥", "æ˜æ—¥",
        "è·å¾—", "ä½¿ç”¨", "è¿›è¡Œ", "å®Œæˆ", "ä»»åŠ¡", "æ´»åŠ¨", "å¥–åŠ±", "é“å…·", "è§’è‰²",
    }
    
    @property
    def mode_name(self) -> str:
        return "heuristic"
    
    def extract(self, texts: List[Dict], max_terms: int = 300) -> List[Dict]:
        """ä½¿ç”¨å¯å‘å¼è§„åˆ™æå–æœ¯è¯­"""
        freq = Counter()
        examples = defaultdict(list)
        
        all_stop = self.stopwords | self.EXTRA_STOP
        
        for item in texts:
            string_id = item['string_id']
            text = item['text']
            
            # ç§»é™¤ token
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            
            # æ‹¬å·å†…è¯ï¼ˆæƒé‡ +2ï¼‰
            for m in self.RE_BRACKETED.finditer(text_clean):
                term = m.group(1).strip()
                if 2 <= len(term) <= 12 and term not in all_stop:
                    freq[term] += 2
                    if len(examples[term]) < 5:
                        examples[term].append({'string_id': string_id, 'source_zh': text})
            
            # CJK è¿ç»­ä¸²ï¼ˆæƒé‡ +1ï¼‰
            for m in self.RE_CJK.finditer(text_clean):
                term = m.group(0)
                if term not in all_stop:
                    freq[term] += 1
                    if len(examples[term]) < 3:
                        examples[term].append({'string_id': string_id, 'source_zh': text})
        
        # ç”Ÿæˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, count in freq.most_common(max_terms):
            if term in self.glossary_terms:
                continue
            
            candidates.append({
                'term_zh': term,
                'score': count,
                'status': 'proposed',
                'examples': examples[term]
            })
        
        return candidates


# ============================================================================
# LLM æ¨¡å¼
# ============================================================================

class LLMExtractor(BaseExtractor):
    """ä½¿ç”¨ LLM API çš„æœ¯è¯­æå–å™¨"""
    
    def __init__(self, glossary_terms: Set[str] = None, 
                 provider: str = None, model: str = None):
        super().__init__(glossary_terms)
        self.provider = provider
        self.model = model
        self.config = self._load_config()
    
    def _load_config(self) -> Dict:
        """åŠ è½½ LLM é…ç½®"""
        config_path = Path(__file__).parent.parent / 'workflow' / 'llm_config.yaml'
        
        if not config_path.exists():
            print(f"âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ° LLM é…ç½®æ–‡ä»¶ï¼š{config_path}")
            return {}
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _get_api_key(self) -> str:
        """è·å– API å¯†é’¥"""
        llm_config = self.config.get('llm', {})
        
        # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¯†é’¥
        if llm_config.get('api_key'):
            return llm_config['api_key']
        
        # æ ¹æ®æä¾›å•†ä»ç¯å¢ƒå˜é‡è·å–
        provider = self.provider or llm_config.get('provider', 'openai')
        env_vars = {
            'openai': 'OPENAI_API_KEY',
            'anthropic': 'ANTHROPIC_API_KEY',
            'gemini': 'GOOGLE_API_KEY',
        }
        
        env_var = env_vars.get(provider, 'OPENAI_API_KEY')
        api_key = os.environ.get(env_var)
        
        if not api_key:
            raise RuntimeError(
                f"âŒ æœªæ‰¾åˆ° API å¯†é’¥\n"
                f"   è¯·è®¾ç½®ç¯å¢ƒå˜é‡ {env_var} æˆ–åœ¨ workflow/llm_config.yaml ä¸­é…ç½® api_key"
            )
        
        return api_key
    
    @property
    def mode_name(self) -> str:
        return "llm"
    
    def extract(self, texts: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ LLM æå–æœ¯è¯­"""
        llm_config = self.config.get('llm', {})
        
        # è·å–é…ç½®
        provider = self.provider or llm_config.get('provider', 'openai')
        model = self.model or llm_config.get('model', 'gpt-4o-mini')
        
        # éªŒè¯ API å¯†é’¥
        try:
            api_key = self._get_api_key()
        except RuntimeError as e:
            print(str(e))
            print("\nğŸ’¡ æç¤ºï¼šLLM æ¨¡å¼éœ€è¦é…ç½® API å¯†é’¥")
            print("   1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport OPENAI_API_KEY='your-key'")
            print("   2. æˆ–ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼šworkflow/llm_config.yaml")
            sys.exit(1)
        
        print(f"âœ… ä½¿ç”¨ LLM æ¨¡å¼: {provider}/{model}")
        print(f"   API å¯†é’¥: {api_key[:8]}...")
        
        # TODO: å®ç° LLM API è°ƒç”¨
        print("\nâš ï¸  LLM æå–åŠŸèƒ½å°šæœªå®Œå…¨å®ç°")
        print("   å½“å‰ä»…éªŒè¯ API é…ç½®æ˜¯å¦æ­£ç¡®")
        print("   å®é™… API è°ƒç”¨å°†åœ¨åç»­ç‰ˆæœ¬å®ç°")
        
        # è¿”å›ç©ºåˆ—è¡¨ï¼ˆå ä½ï¼‰
        return []


# ============================================================================
# ä¸»é€»è¾‘
# ============================================================================

def load_glossary(glossary_path: str) -> Set[str]:
    """åŠ è½½æœ¯è¯­è¡¨"""
    if not glossary_path or not Path(glossary_path).exists():
        return set()
    
    with open(glossary_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
        terms = data.get('terms', {})
        return set(terms.keys())


def load_source_texts(input_csv: str) -> List[Dict]:
    """åŠ è½½æºæ–‡æœ¬"""
    texts = []
    with open(input_csv, 'r', encoding='utf-8-sig', newline='') as f:
        reader = csv.DictReader(f)
        
        if 'string_id' not in reader.fieldnames or 'source_zh' not in reader.fieldnames:
            raise ValueError("CSV å¿…é¡»åŒ…å« string_id å’Œ source_zh åˆ—")
        
        for row in reader:
            if row.get('source_zh'):
                texts.append({
                    'string_id': row['string_id'],
                    'text': row['source_zh']
                })
    
    return texts


def save_candidates(candidates: List[Dict], output_path: str, 
                   mode: str, texts_count: int, config: Dict = None) -> None:
    """ä¿å­˜å€™é€‰åˆ—è¡¨"""
    language_pair = {'source': 'zh-CN', 'target': 'ru-RU'}
    if config:
        language_pair = config.get('language_pair', language_pair)
    
    output = {
        'version': '2.0',
        'extraction_mode': mode,
        'generated_at': datetime.now().isoformat(),
        'language_pair': language_pair,
        'statistics': {
            'total_strings': texts_count,
            'unique_terms': len(candidates),
            'total_occurrences': sum(c['score'] for c in candidates) if candidates else 0
        },
        'candidates': candidates
    }
    
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.safe_dump(output, f, allow_unicode=True, sort_keys=False)


def main():
    parser = argparse.ArgumentParser(
        description='Extract Terms v2.0 - å¤šæ¨¡å¼æœ¯è¯­æå–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ¨¡å¼è¯´æ˜:
  jieba      ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯ï¼ˆé»˜è®¤ï¼Œéœ€å®‰è£… jiebaï¼‰
  heuristic  å¯å‘å¼æ­£åˆ™æå–ï¼ˆæ— ä¾èµ–ï¼Œå¿«é€Ÿï¼‰
  llm        ä½¿ç”¨ LLM API æå–ï¼ˆéœ€é…ç½® API å¯†é’¥ï¼‰

ç¤ºä¾‹:
  python extract_terms.py data/input.csv data/terms.yaml
  python extract_terms.py data/input.csv data/terms.yaml --mode heuristic
  python extract_terms.py data/input.csv data/terms.yaml --mode llm --model gpt-4o
        """
    )
    
    parser.add_argument('input_csv', help='è¾“å…¥ CSV æ–‡ä»¶')
    parser.add_argument('output_yaml', help='è¾“å‡ºæœ¯è¯­å€™é€‰ YAML')
    parser.add_argument('--mode', choices=['jieba', 'heuristic', 'llm'], 
                       default='jieba', help='æå–æ¨¡å¼ (é»˜è®¤: jieba)')
    parser.add_argument('--glossary', help='ç°æœ‰æœ¯è¯­è¡¨æ–‡ä»¶')
    parser.add_argument('--min-freq', type=int, default=2, help='æœ€å°è¯é¢‘ (é»˜è®¤: 2)')
    parser.add_argument('--model', help='LLM æ¨¡å‹ (ä»… llm æ¨¡å¼)')
    parser.add_argument('--provider', help='LLM æä¾›å•† (ä»… llm æ¨¡å¼)')
    
    args = parser.parse_args()
    
    print("ğŸš€ Extract Terms v2.0")
    print(f"   è¾“å…¥: {args.input_csv}")
    print(f"   è¾“å‡º: {args.output_yaml}")
    print(f"   æ¨¡å¼: {args.mode}")
    print()
    
    # åŠ è½½æœ¯è¯­è¡¨
    glossary_terms = load_glossary(args.glossary)
    if glossary_terms:
        print(f"âœ… åŠ è½½äº† {len(glossary_terms)} ä¸ªå·²çŸ¥æœ¯è¯­")
    
    # åŠ è½½æºæ–‡æœ¬
    texts = load_source_texts(args.input_csv)
    print(f"âœ… åŠ è½½äº† {len(texts)} æ¡æºæ–‡æœ¬")
    
    # é€‰æ‹©æå–å™¨
    mode = args.mode
    
    # jieba æ¨¡å¼è‡ªåŠ¨ fallback
    if mode == 'jieba' and not JIEBA_AVAILABLE:
        print("âš ï¸  jieba æœªå®‰è£…ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° heuristic æ¨¡å¼")
        mode = 'heuristic'
    
    # åˆ›å»ºæå–å™¨
    if mode == 'jieba':
        extractor = JiebaExtractor(glossary_terms)
        candidates = extractor.extract(texts, min_freq=args.min_freq)
    elif mode == 'heuristic':
        extractor = HeuristicExtractor(glossary_terms)
        candidates = extractor.extract(texts)
    elif mode == 'llm':
        extractor = LLMExtractor(glossary_terms, args.provider, args.model)
        candidates = extractor.extract(texts)
    
    print(f"\nâœ… æå–äº† {len(candidates)} ä¸ªæœ¯è¯­å€™é€‰")
    
    # åŠ è½½ LLM é…ç½®ï¼ˆç”¨äºè¯­è¨€å¯¹ï¼‰
    config = {}
    config_path = Path(__file__).parent.parent / 'workflow' / 'llm_config.yaml'
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    
    # ä¿å­˜ç»“æœ
    save_candidates(candidates, args.output_yaml, extractor.mode_name, len(texts), config)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output_yaml}")
    
    # æ‰“å° Top 10
    if candidates:
        print(f"\nğŸ“Š é«˜é¢‘æœ¯è¯­ TOP 10:")
        for i, c in enumerate(candidates[:10], 1):
            print(f"   {i}. {c['term_zh']} (score: {c['score']})")
    
    print("\nâœ… æœ¯è¯­æå–å®Œæˆ!")


if __name__ == "__main__":
    main()
