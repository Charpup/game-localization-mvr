#!/usr/bin/env python3
"""
Extract Terms Script
ä»æºæ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­å€™é€‰

Usage:
    python extract_terms.py <input_csv> <output_candidates_yaml> <glossary_yaml> [options]
"""

import csv
import re
import sys
import yaml
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime
from collections import Counter, defaultdict

# å°è¯•å¯¼å…¥ jieba
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False


class TermExtractor:
    """æœ¯è¯­æå–å™¨"""
    
    def __init__(self, input_csv: str, glossary_yaml: str = None):
        self.input_csv = Path(input_csv)
        self.glossary_path = Path(glossary_yaml) if glossary_yaml else None
        
        # æ£€æŸ¥ jieba æ˜¯å¦å¯ç”¨
        if not JIEBA_AVAILABLE:
            raise RuntimeError(
                "é”™è¯¯ï¼šjieba åˆ†è¯åº“æœªå®‰è£…ã€‚\n"
                "è¯·è¿è¡Œï¼špip install jieba\n"
                "jieba æ˜¯å¿…éœ€çš„ä¾èµ–ï¼Œç”¨äºä¸­æ–‡åˆ†è¯ä»¥ç¡®ä¿æœ¯è¯­æå–çš„å‡†ç¡®æ€§ã€‚"
            )
        
        self.source_texts: List[Dict] = []
        self.glossary_terms: Set[str] = set()
        self.term_frequencies: Counter = Counter()
        self.term_positions: Dict[str, List[str]] = defaultdict(list)
        
        # åœç”¨è¯åˆ—è¡¨ï¼ˆå¸¸è§çš„ã€ä¸åº”ä½œä¸ºæœ¯è¯­çš„è¯ï¼‰
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯åˆ—è¡¨"""
        # åŸºç¡€åœç”¨è¯
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'äº›',
            'ä¸ª', 'ä¸º', 'ä¸', 'æˆ–', 'åŠ', 'ä¹‹', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å¦‚æœ',
            'å¯ä»¥', 'å·²ç»', 'è¿˜', 'ä»', 'å¯¹', 'æŠŠ', 'è¢«', 'è®©', 'ç»™', 'ç”¨'
        }
        
        # å¯ä»¥ä»æ–‡ä»¶åŠ è½½æ›´å¤šåœç”¨è¯
        stopwords_file = Path(__file__).parent.parent / 'workflow' / 'stopwords.txt'
        if stopwords_file.exists():
            with open(stopwords_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        stopwords.add(word)
        
        return stopwords
    
    def load_source_texts(self) -> bool:
        """åŠ è½½æºæ–‡æœ¬"""
        try:
            with open(self.input_csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                if 'string_id' not in reader.fieldnames or 'source_zh' not in reader.fieldnames:
                    print("âŒ é”™è¯¯ï¼šCSV æ–‡ä»¶å¿…é¡»åŒ…å« string_id å’Œ source_zh åˆ—")
                    return False
                
                for row in reader:
                    if row.get('source_zh'):
                        self.source_texts.append({
                            'string_id': row['string_id'],
                            'text': row['source_zh']
                        })
            
            print(f"âœ… åŠ è½½äº† {len(self.source_texts)} æ¡æºæ–‡æœ¬")
            return True
            
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼š{self.input_csv}")
            return False
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
            return False
    
    def load_glossary(self) -> None:
        """åŠ è½½ç°æœ‰æœ¯è¯­è¡¨"""
        if not self.glossary_path or not self.glossary_path.exists():
            print("â„¹ï¸  æœªæ‰¾åˆ°ç°æœ‰æœ¯è¯­è¡¨ï¼Œå°†æå–æ‰€æœ‰å€™é€‰æœ¯è¯­")
            return
        
        try:
            with open(self.glossary_path, 'r', encoding='utf-8') as f:
                glossary = yaml.safe_load(f)
                terms = glossary.get('terms', {})
                self.glossary_terms = set(terms.keys())
            
            print(f"âœ… åŠ è½½äº† {len(self.glossary_terms)} ä¸ªå·²çŸ¥æœ¯è¯­")
            
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Šï¼šåŠ è½½æœ¯è¯­è¡¨æ—¶å‡ºé”™ï¼š{str(e)}")
    
    def extract_candidates(self, min_freq: int = 2, min_len: int = 2, max_len: int = 8) -> List[Dict]:
        """æå–æœ¯è¯­å€™é€‰"""
        print("\nğŸ” å¼€å§‹æå–æœ¯è¯­...")
        
        # éå†æ‰€æœ‰æºæ–‡æœ¬
        for item in self.source_texts:
            string_id = item['string_id']
            text = item['text']
            
            # ç§»é™¤ tokenï¼ˆâŸ¦PH_XâŸ§ ç­‰ï¼‰
            text_clean = re.sub(r'âŸ¦[^âŸ§]+âŸ§', '', text)
            
            # ä½¿ç”¨ jieba åˆ†è¯
            words = jieba.cut(text_clean)
            
            for word in words:
                # è¿‡æ»¤æ¡ä»¶
                word = word.strip()
                if not word:
                    continue
                if word in self.stopwords:
                    continue
                if len(word) < min_len or len(word) > max_len:
                    continue
                # æ’é™¤çº¯æ•°å­—å’Œçº¯è‹±æ–‡
                if re.match(r'^[0-9]+$', word) or re.match(r'^[a-zA-Z]+$', word):
                    continue
                # æ’é™¤å•ä¸ªæ ‡ç‚¹ç¬¦å·
                if re.match(r'^[^\w]+$', word):
                    continue
                
                # ç»Ÿè®¡
                self.term_frequencies[word] += 1
                self.term_positions[word].append(string_id)
        
        # ç”Ÿæˆå€™é€‰åˆ—è¡¨
        candidates = []
        for term, freq in self.term_frequencies.most_common():
            if freq < min_freq:
                break
            
            # è·³è¿‡å·²åœ¨æœ¯è¯­è¡¨ä¸­çš„è¯
            if term in self.glossary_terms:
                continue
            
            candidates.append({
                'term': term,
                'frequency': freq,
                'string_ids': list(set(self.term_positions[term])),  # å»é‡
                'suggested_translation': '',  # å¯ä»¥æ¥å…¥ç¿»è¯‘ API
                'category': 'å¾…åˆ†ç±»',
                'note': ''
            })
        
        print(f"âœ… æå–äº† {len(candidates)} ä¸ªæœ¯è¯­å€™é€‰ï¼ˆå»é™¤å·²çŸ¥æœ¯è¯­åï¼‰")
        print(f"   æ€»è¯æ±‡æ•°ï¼š{len(self.term_frequencies)}")
        print(f"   é«˜é¢‘è¯æ±‡ï¼ˆâ‰¥{min_freq}æ¬¡ï¼‰ï¼š{sum(1 for f in self.term_frequencies.values() if f >= min_freq)}")
        
        return candidates
    
    def save_candidates(self, candidates: List[Dict], output_path: str) -> bool:
        """ä¿å­˜å€™é€‰åˆ—è¡¨åˆ° YAML"""
        try:
            output = {
                'version': '1.0',
                'generated_at': datetime.now().isoformat(),
                'statistics': {
                    'total_strings': len(self.source_texts),
                    'unique_terms': len(candidates),
                    'total_occurrences': sum(c['frequency'] for c in candidates)
                },
                'candidates': candidates,
                'extraction_rules': {
                    'min_frequency': 2,
                    'min_length': 2,
                    'max_length': 8,
                    'segmentation': 'jieba'
                }
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                yaml.dump(output, f, allow_unicode=True, sort_keys=False, indent=2)
            
            print(f"âœ… å€™é€‰åˆ—è¡¨å·²ä¿å­˜åˆ°ï¼š{output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼šä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
            return False
    
    def print_summary(self, candidates: List[Dict], top_n: int = 10) -> None:
        """æ‰“å°æå–æ‘˜è¦"""
        print(f"\nğŸ“Š æœ¯è¯­æå–æ‘˜è¦ï¼š")
        print(f"   å…±å¤„ç†ï¼š{len(self.source_texts)} æ¡æ–‡æœ¬")
        print(f"   æå–å€™é€‰ï¼š{len(candidates)} ä¸ªæœ¯è¯­")
        print(f"   å·²çŸ¥æœ¯è¯­ï¼š{len(self.glossary_terms)} ä¸ªï¼ˆå·²è¿‡æ»¤ï¼‰")
        print()
        
        if candidates:
            print(f"   é«˜é¢‘æœ¯è¯­ TOP {min(top_n, len(candidates))}ï¼š")
            for i, cand in enumerate(candidates[:top_n], 1):
                print(f"      {i}. {cand['term']} (å‡ºç° {cand['frequency']} æ¬¡)")
    
    def run(self, output_path: str, min_freq: int = 2) -> bool:
        """æ‰§è¡Œæœ¯è¯­æå–"""
        print("ğŸš€ å¼€å§‹æœ¯è¯­æå–æµç¨‹...")
        print()
        
        # åŠ è½½æºæ–‡æœ¬
        if not self.load_source_texts():
            return False
        
        # åŠ è½½ç°æœ‰æœ¯è¯­è¡¨
        self.load_glossary()
        
        # æå–å€™é€‰
        candidates = self.extract_candidates(min_freq=min_freq)
        
        # ä¿å­˜ç»“æœ
        if not self.save_candidates(candidates, output_path):
            return False
        
        # æ‰“å°æ‘˜è¦
        self.print_summary(candidates)
        
        print()
        print("âœ… æœ¯è¯­æå–å®Œæˆï¼")
        return True


def main():
    """ä¸»å…¥å£"""
    if len(sys.argv) < 3:
        print("Usage: python extract_terms.py <input_csv> <output_candidates_yaml> [glossary_yaml] [min_freq]")
        print()
        print("å‚æ•°è¯´æ˜ï¼š")
        print("  input_csv              è¾“å…¥çš„ CSV æ–‡ä»¶ï¼ˆå¿…éœ€åŒ…å« string_id å’Œ source_zh åˆ—ï¼‰")
        print("  output_candidates_yaml è¾“å‡ºçš„æœ¯è¯­å€™é€‰ YAML æ–‡ä»¶")
        print("  glossary_yaml          ç°æœ‰æœ¯è¯­è¡¨ YAML æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
        print("  min_freq               æœ€å°è¯é¢‘ï¼ˆé»˜è®¤ 2ï¼‰")
        print()
        print("ç¤ºä¾‹ï¼š")
        print("  python extract_terms.py data/input.csv data/term_candidates.yaml")
        print("  python extract_terms.py data/input.csv data/term_candidates.yaml data/glossary.yaml 3")
        sys.exit(1)
    
    input_csv = sys.argv[1]
    output_yaml = sys.argv[2]
    glossary_yaml = sys.argv[3] if len(sys.argv) > 3 else None
    min_freq = int(sys.argv[4]) if len(sys.argv) > 4 else 2
    
    extractor = TermExtractor(input_csv, glossary_yaml)
    success = extractor.run(output_yaml, min_freq=min_freq)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
