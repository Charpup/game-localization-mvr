#!/usr/bin/env python3
"""
Metrics Aggregator (v2.0) - æ±‡æ€» LLM è°ƒç”¨ç»Ÿè®¡
ä» reports/*_progress.jsonl å’Œ data/llm_trace.jsonl è¯»å–æ•°æ®
ç”ŸæˆåŒ…å« Token æ¶ˆè€—å’Œè´¹ç”¨ä¼°ç®—çš„ç»Ÿè®¡æŠ¥å‘Š

Features:
- Token ç»Ÿè®¡ (prompt_tokens, completion_tokens)
- è´¹ç”¨è®¡ç®— (ç»“åˆ config/pricing.yaml)
- API ä½™é¢æŸ¥è¯¢ (å¯é€‰)
- Markdown + JSON åŒæ ¼å¼è¾“å‡º
"""

import os
import sys
import json
import glob
import yaml
import requests
from datetime import datetime
from collections import defaultdict
from typing import Optional, Dict, Any

# ç¡®ä¿è¾“å‡ºä¸ç¼“å†²
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(line_buffering=True)

def load_pricing_config(pricing_path: str = "config/pricing.yaml") -> Dict[str, Any]:
    """åŠ è½½å®šä»·é…ç½®"""
    if not os.path.exists(pricing_path):
        # å°è¯•ç›¸å¯¹è·¯å¾„
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        pricing_path = os.path.join(script_dir, "config", "pricing.yaml")

    if os.path.exists(pricing_path):
        with open(pricing_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f) or {}

    # é»˜è®¤å®šä»· (per 1M tokens, USD)
    return {
        "models": {
            "claude-haiku-4-5-20251001": {"input_per_1M": 0.25, "output_per_1M": 1.25},
            "claude-sonnet-4-5-20250929": {"input_per_1M": 3.0, "output_per_1M": 15.0},
            "gpt-4.1-mini": {"input_per_1M": 0.15, "output_per_1M": 0.60},
            "gpt-4.1": {"input_per_1M": 2.0, "output_per_1M": 8.0},
            "text-embedding-3-small": {"input_per_1M": 0.02, "output_per_1M": 0.0},
            "_default": {"input_per_1M": 0.50, "output_per_1M": 2.0}
        }
    }

def query_api_balance() -> Optional[Dict[str, Any]]:
    """
    æŸ¥è¯¢ API ä½™é¢ (å¦‚æœæ”¯æŒ)

    Returns:
        dict: {"balance": float, "currency": str, "updated_at": str} æˆ– None
    """
    base_url = os.getenv("LLM_BASE_URL", "").strip().rstrip("/")
    api_key = os.getenv("LLM_API_KEY", "").strip()

    if not base_url or not api_key:
        return None

    # å°è¯•å¸¸è§çš„ä½™é¢æŸ¥è¯¢ç«¯ç‚¹
    balance_endpoints = [
        "/dashboard/billing/credit_grants",  # OpenAI é£æ ¼
        "/v1/dashboard/billing/credit_grants",
        "/user/balance",  # æŸäº›ä»£ç†ç½‘å…³
        "/v1/user/balance",
    ]

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    for endpoint in balance_endpoints:
        try:
            resp = requests.get(
                f"{base_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            if resp.status_code == 200:
                data = resp.json()
                # è§£æä¸åŒæ ¼å¼çš„å“åº”
                if "total_available" in data:
                    return {
                        "balance": float(data["total_available"]),
                        "currency": "USD",
                        "updated_at": datetime.now().isoformat(),
                        "source": endpoint
                    }
                elif "balance" in data:
                    return {
                        "balance": float(data["balance"]),
                        "currency": data.get("currency", "USD"),
                        "updated_at": datetime.now().isoformat(),
                        "source": endpoint
                    }
                elif "data" in data and isinstance(data["data"], dict):
                    # åµŒå¥—æ ¼å¼
                    inner = data["data"]
                    if "balance" in inner:
                        return {
                            "balance": float(inner["balance"]),
                            "currency": inner.get("currency", "USD"),
                            "updated_at": datetime.now().isoformat(),
                            "source": endpoint
                        }
        except Exception:
            continue

    return None

def load_progress_logs(reports_dir: str = "reports") -> list:
    """åŠ è½½æ‰€æœ‰ JSONL è¿›åº¦æ—¥å¿—"""
    all_events = []

    pattern = os.path.join(reports_dir, "*_progress.jsonl")
    for filepath in glob.glob(pattern):
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        event = json.loads(line)
                        event['_source_file'] = os.path.basename(filepath)
                        all_events.append(event)
                    except json.JSONDecodeError:
                        continue

    return all_events

def load_trace_logs(trace_path: str = "data/llm_trace.jsonl") -> list:
    """åŠ è½½ LLM trace æ—¥å¿— (åŒ…å«è¯¦ç»† token ä¿¡æ¯)"""
    events = []

    if not os.path.exists(trace_path):
        return events

    with open(trace_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    event = json.loads(line)
                    events.append(event)
                except json.JSONDecodeError:
                    continue

    return events

def aggregate_metrics(events: list, trace_events: list = None, pricing: dict = None) -> dict:
    """æ±‡æ€»æŒ‡æ ‡ï¼ŒåŒ…å« Token å’Œè´¹ç”¨è®¡ç®—"""

    if pricing is None:
        pricing = load_pricing_config()

    metrics = {
        'summary': {
            'total_steps': 0,
            'total_batches': 0,
            'total_rows': 0,
            'total_success': 0,
            'total_failed': 0,
            'total_latency_ms': 0,
            'total_prompt_tokens': 0,
            'total_completion_tokens': 0,
            'total_tokens': 0,
            'estimated_cost_usd': 0.0
        },
        'by_step': defaultdict(lambda: {
            'batches': 0,
            'rows': 0,
            'success': 0,
            'failed': 0,
            'latency_ms': 0,
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'estimated_cost_usd': 0.0,
            'models': set()
        }),
        'by_model': defaultdict(lambda: {
            'batches': 0,
            'rows': 0,
            'latency_ms': 0,
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'estimated_cost_usd': 0.0
        }),
        'api_balance': None
    }

    steps_seen = set()
    model_pricing = pricing.get("models", {})
    default_pricing = model_pricing.get("_default", {"input_per_1M": 0.5, "output_per_1M": 2.0})

    # ä» trace æ—¥å¿—æ„å»º token æŸ¥æ‰¾è¡¨ (æŒ‰ request_id)
    token_lookup = {}
    if trace_events:
        for event in trace_events:
            req_id = event.get("request_id")
            usage = event.get("usage")
            if req_id and usage:
                token_lookup[req_id] = usage

    for event in events:
        step = event.get('step', 'unknown')
        event_type = event.get('event', '')

        # è·³è¿‡ unknown æ­¥éª¤ (å†å²æ®‹ç•™)
        if step == 'unknown':
            continue

        if event_type == 'step_start':
            steps_seen.add(step)
            model = event.get('model') or event.get('model_name') or 'unspecified'
            if model and model not in ('unknown', 'unspecified'):
                metrics['by_step'][step]['models'].add(model)

        elif event_type == 'batch_complete':
            rows = event.get('rows_in_batch') or event.get('batch_size') or 0
            latency = event.get('latency_ms', 0)
            status = event.get('status', 'SUCCESS')
            model = event.get('model') or event.get('model_name') or 'unspecified'

            # è·å– token ä½¿ç”¨é‡
            usage = event.get('usage', {})
            if not usage:
                # å°è¯•ä» trace æŸ¥æ‰¾
                req_id = event.get('request_id')
                if req_id and req_id in token_lookup:
                    usage = token_lookup[req_id]

            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)

            # è®¡ç®—è´¹ç”¨
            mp = model_pricing.get(model, default_pricing)
            batch_cost = (
                prompt_tokens * mp.get('input_per_1M', 0.5) / 1_000_000 +
                completion_tokens * mp.get('output_per_1M', 2.0) / 1_000_000
            )

            # æ›´æ–° summary
            metrics['summary']['total_batches'] += 1
            metrics['summary']['total_rows'] += rows
            metrics['summary']['total_latency_ms'] += latency
            metrics['summary']['total_prompt_tokens'] += prompt_tokens
            metrics['summary']['total_completion_tokens'] += completion_tokens
            metrics['summary']['estimated_cost_usd'] += batch_cost

            if status in ('SUCCESS', 'ok', 'success'):
                metrics['summary']['total_success'] += rows
            else:
                metrics['summary']['total_failed'] += rows

            # æ›´æ–° by_step
            metrics['by_step'][step]['batches'] += 1
            metrics['by_step'][step]['rows'] += rows
            metrics['by_step'][step]['latency_ms'] += latency
            metrics['by_step'][step]['prompt_tokens'] += prompt_tokens
            metrics['by_step'][step]['completion_tokens'] += completion_tokens
            metrics['by_step'][step]['estimated_cost_usd'] += batch_cost

            if status in ('SUCCESS', 'ok', 'success'):
                metrics['by_step'][step]['success'] += rows
            else:
                metrics['by_step'][step]['failed'] += rows

            # æ›´æ–° by_model
            if model and model not in ('unknown', 'unspecified'):
                metrics['by_model'][model]['batches'] += 1
                metrics['by_model'][model]['rows'] += rows
                metrics['by_model'][model]['latency_ms'] += latency
                metrics['by_model'][model]['prompt_tokens'] += prompt_tokens
                metrics['by_model'][model]['completion_tokens'] += completion_tokens
                metrics['by_model'][model]['estimated_cost_usd'] += batch_cost

    # è®¡ç®—æ€» token
    metrics['summary']['total_tokens'] = (
        metrics['summary']['total_prompt_tokens'] +
        metrics['summary']['total_completion_tokens']
    )

    # å››èˆäº”å…¥è´¹ç”¨
    metrics['summary']['estimated_cost_usd'] = round(
        metrics['summary']['estimated_cost_usd'], 6
    )

    for step_data in metrics['by_step'].values():
        step_data['estimated_cost_usd'] = round(step_data['estimated_cost_usd'], 6)
        step_data['models'] = list(step_data['models'])

    for model_data in metrics['by_model'].values():
        model_data['estimated_cost_usd'] = round(model_data['estimated_cost_usd'], 6)

    metrics['summary']['total_steps'] = len(steps_seen)

    return metrics

def generate_report(metrics: dict, output_path: str = "reports/metrics_report.md"):
    """ç”Ÿæˆå¢å¼ºç‰ˆ Markdown æŠ¥å‘Š"""

    lines = [
        "# LLM è°ƒç”¨ç»Ÿè®¡æŠ¥å‘Š (v2.0)",
        f"\nç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
    ]

    # API ä½™é¢ (å¦‚æœæœ‰)
    if metrics.get('api_balance'):
        balance = metrics['api_balance']
        lines.extend([
            "\n## API ä½™é¢",
            f"\n| é¡¹ç›® | å€¼ |",
            f"|------|-----|",
            f"| å½“å‰ä½™é¢ | {balance['balance']:.4f} {balance['currency']} |",
            f"| æŸ¥è¯¢æ—¶é—´ | {balance['updated_at']} |",
        ])

    # æ€»ä½“ç»Ÿè®¡
    s = metrics['summary']
    lines.extend([
        "\n## æ€»ä½“ç»Ÿè®¡\n",
        f"| æŒ‡æ ‡ | å€¼ |",
        f"|------|-----|",
        f"| æ­¥éª¤æ•° | {s['total_steps']} |",
        f"| æ‰¹æ¬¡æ•° | {s['total_batches']} |",
        f"| æ€»è¡Œæ•° | {s['total_rows']} |",
        f"| æˆåŠŸ | {s['total_success']} |",
        f"| å¤±è´¥ | {s['total_failed']} |",
        f"| æ€»å»¶è¿Ÿ | {s['total_latency_ms']:,}ms ({s['total_latency_ms']/1000:.1f}s) |",
        f"| Prompt Tokens | {s['total_prompt_tokens']:,} |",
        f"| Completion Tokens | {s['total_completion_tokens']:,} |",
        f"| **æ€» Tokens** | **{s['total_tokens']:,}** |",
        f"| **ä¼°ç®—è´¹ç”¨** | **${s['estimated_cost_usd']:.4f} USD** |",
    ])

    # æŒ‰æ­¥éª¤ç»Ÿè®¡
    lines.extend([
        "\n## æŒ‰æ­¥éª¤ç»Ÿè®¡\n",
        "| æ­¥éª¤ | æ‰¹æ¬¡ | è¡Œæ•° | Tokens | è´¹ç”¨(USD) | å»¶è¿Ÿ(s) | æ¨¡å‹ |",
        "|------|------|------|--------|-----------|---------|------|"
    ])

    for step, data in sorted(metrics['by_step'].items()):
        total_tokens = data['prompt_tokens'] + data['completion_tokens']
        models = ', '.join(data['models'][:2]) if data['models'] else 'N/A'
        if len(data['models']) > 2:
            models += f" (+{len(data['models'])-2})"

        lines.append(
            f"| {step} | {data['batches']} | {data['rows']} | "
            f"{total_tokens:,} | ${data['estimated_cost_usd']:.4f} | "
            f"{data['latency_ms']/1000:.1f} | {models} |"
        )

    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    if metrics['by_model']:
        lines.extend([
            "\n## æŒ‰æ¨¡å‹ç»Ÿè®¡\n",
            "| æ¨¡å‹ | æ‰¹æ¬¡ | è¡Œæ•° | Prompt | Completion | è´¹ç”¨(USD) |",
            "|------|------|------|--------|------------|-----------|"
        ])

        for model, data in sorted(metrics['by_model'].items()):
            lines.append(
                f"| {model} | {data['batches']} | {data['rows']} | "
                f"{data['prompt_tokens']:,} | {data['completion_tokens']:,} | "
                f"${data['estimated_cost_usd']:.4f} |"
            )

    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"âœ… Metrics æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    return output_path

def main():
    import argparse
    parser = argparse.ArgumentParser(description="æ±‡æ€» LLM è°ƒç”¨ç»Ÿè®¡ (v2.0)")
    parser.add_argument("--reports-dir", default="reports", help="JSONL æ—¥å¿—ç›®å½•")
    parser.add_argument("--trace-path", default="data/llm_trace.jsonl", help="LLM trace æ—¥å¿—è·¯å¾„")
    parser.add_argument("--output", default="reports/metrics_report", help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„ (ä¸å«æ‰©å±•å)")
    parser.add_argument("--query-balance", action="store_true", help="æŸ¥è¯¢ API ä½™é¢")
    parser.add_argument("--json", action="store_true", help="åŒæ—¶è¾“å‡º JSON æ ¼å¼")

    args = parser.parse_args()

    # åŠ è½½ pricing
    pricing = load_pricing_config()
    print(f"å·²åŠ è½½å®šä»·é…ç½®: {len(pricing.get('models', {}))} ä¸ªæ¨¡å‹")

    # åŠ è½½ progress æ—¥å¿—
    events = load_progress_logs(args.reports_dir)
    print(f"åŠ è½½äº† {len(events)} æ¡ progress äº‹ä»¶")

    # åŠ è½½ trace æ—¥å¿—
    trace_events = load_trace_logs(args.trace_path)
    print(f"åŠ è½½äº† {len(trace_events)} æ¡ trace äº‹ä»¶")

    if not events:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• progress äº‹ä»¶")
        return

    # æ±‡æ€»æŒ‡æ ‡
    metrics = aggregate_metrics(events, trace_events, pricing)

    # æŸ¥è¯¢ API ä½™é¢ (å¯é€‰)
    if args.query_balance:
        print("æ­£åœ¨æŸ¥è¯¢ API ä½™é¢...")
        balance = query_api_balance()
        if balance:
            metrics['api_balance'] = balance
            print(f"âœ… ä½™é¢: {balance['balance']:.4f} {balance['currency']}")
        else:
            print("âš ï¸ æ— æ³•æŸ¥è¯¢ä½™é¢ (ç«¯ç‚¹ä¸æ”¯æŒæˆ–è®¤è¯å¤±è´¥)")

    # ç”ŸæˆæŠ¥å‘Š
    md_path = args.output if args.output.endswith('.md') else f"{args.output}.md"
    generate_report(metrics, md_path)

    # JSON è¾“å‡º
    if args.json:
        json_path = args.output.replace('.md', '') + '.json'
        # è½¬æ¢ defaultdict ä¸ºæ™®é€š dict
        output_metrics = json.loads(json.dumps(metrics, default=list))
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_metrics, f, indent=2, ensure_ascii=False)
        print(f"âœ… JSON æŠ¥å‘Š: {json_path}")

    # æ‰“å°æ‘˜è¦
    s = metrics['summary']
    print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æ€» Tokens: {s['total_tokens']:,}")
    print(f"   ä¼°ç®—è´¹ç”¨: ${s['estimated_cost_usd']:.4f} USD")

if __name__ == "__main__":
    main()
