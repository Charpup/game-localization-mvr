#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
verify_multi_models_metrics.py
éªŒè¯ 5 ä¸ªæŒ‡å®šæ¨¡å‹çš„ LLM è°ƒç”¨è¿é€šæ€§ï¼Œå¹¶è§¦å‘ Metrics è·¯å¾„è¿›è¡Œè´¹ç”¨å¯¹è´¦ã€‚
"""
import os
import sys
import time
from pathlib import Path
from datetime import datetime

# ç¡®ä¿è„šæœ¬è·¯å¾„åœ¨ Python è·¯å¾„ä¸­
sys.path.append(str(Path(__file__).parent.parent / "scripts"))

try:
    from runtime_adapter import LLMClient
    from cost_monitor import CostMonitor
    import json
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®è·¯å¾„
API_KEY_FILE = r"C:\Users\bob_c\.gemini\antigravity\auto_Localization\data\attachment\api_key.txt"
ACCESS_TOKEN_FILE = r"C:\Users\bob_c\.gemini\antigravity\auto_Localization\data\attachment\api access token.txt"
BASE_URL = "https://api.apiyi.com/v1"
TRACE_PATH = r"C:\Users\bob_c\.gemini\antigravity\auto_Localization\data\llm_trace.jsonl"

# å¾…æµ‹è¯•æ¨¡å‹åˆ—è¡¨
MODELS = [
    "gpt-5.2",
    "claude-haiku-4-5-20251001-thinking",
    "DeepSeek-V3.2-Exp-thinking",
    "gemini-2.5-flash",
    "glm-4.5-flash"
]

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡åŠ API Key æ–‡ä»¶è·¯å¾„"""
    print("ğŸ› ï¸ æ­£åœ¨è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
    os.environ["LLM_BASE_URL"] = BASE_URL
    os.environ["LLM_API_KEY_FILE"] = API_KEY_FILE
    os.environ["LLM_TRACE_PATH"] = TRACE_PATH
    os.environ["LLM_RUN_ID"] = f"verify_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(API_KEY_FILE).exists():
        print(f"âš ï¸ è­¦å‘Š: API Key æ–‡ä»¶ä¸å­˜åœ¨: {API_KEY_FILE}")
    if not Path(ACCESS_TOKEN_FILE).exists():
        print(f"âš ï¸ è­¦å‘Š: Access Token æ–‡ä»¶ä¸å­˜åœ¨: {ACCESS_TOKEN_FILE}")
    else:
        print(f"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆã€‚è¿è¡Œ ID: {os.environ['LLM_RUN_ID']}")

def run_multi_model_test():
    """æ‰§è¡Œå¤šæ¨¡å‹è°ƒç”¨æµ‹è¯•"""
    setup_environment()
    
    # å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå†åˆå§‹åŒ–
    monitor = CostMonitor(BASE_URL, os.environ["LLM_RUN_ID"])
    llm = LLMClient()
    
    print(f"\nğŸš€ å¼€å§‹å¯¹ {len(MODELS)} ä¸ªæ¨¡å‹è¿›è¡Œé€ä¸€è°ƒç”¨æµ‹è¯•...")
    
    for model_name in MODELS:
        print(f"\n--- æµ‹è¯•æ¨¡å‹: {model_name} ---")
        try:
            start_ts = time.time()
            # æ‰§è¡Œæç®€è°ƒç”¨
            response = llm.chat(
                system="You are a helpful assistant.",
                user="Hello! Please reply with exactly one word: Success.",
                metadata={"step": "verify_multi_model", "model_override": model_name}
            )
            elapsed = time.time() - start_ts
            
            print(f"âœ… è°ƒç”¨æˆåŠŸ | è€—æ—¶: {elapsed:.2f}s | å“åº”: {response.text.strip()}")
            
            # æ‰‹åŠ¨åˆ·å…¥ monitor è®¡æ•°
            monitor.on_llm_call()
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥ ({model_name}): {str(e)}")

    print("\n" + "="*60)
    print("ç­‰å¾…æ—¥å¿—è½åœ°...")
    time.sleep(2)
    
    print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆè´¹ç”¨å¯¹è´¦å¿«ç…§...")
    snapshot_path = monitor.snapshot(reason="verify_complete")
    
    if snapshot_path:
        print(f"âœ… å¿«ç…§å·²ä¿å­˜: {snapshot_path}")
        try:
            with open(snapshot_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                apiyi = data.get("apiyi", {})
                recon = data.get("reconciliation", {})
                local = data.get("local", {})
                
                print("\nğŸ’° å¯¹è´¦æ‘˜è¦:")
                print(f"   æœ¬åœ°æ€»è¯·æ±‚è®°å½•: {local.get('total_calls', 0)}")
                print(f"   APIyi æ€»ä½™é¢æŠ¥å‘Š: ${apiyi.get('cumulative_used_usd', 'n/a')}")
                print(f"   æœ¬æ¬¡ä¼šè¯ APIyi æ‰£è´¹: ${apiyi.get('total_cost_usd_reported', 0)}")
                print(f"   æœ¬åœ°é¢„ä¼°æ€»æ¶ˆè€—: ${local.get('total_cost_usd_est', 0)}")
                print(f"   è´¹ç”¨åå·® (Delta): ${recon.get('delta_usd', 0)} ({recon.get('delta_ratio', 0)*100:.2f}%)")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å¿«ç…§å¤±è´¥: {e}")

if __name__ == "__main__":
    run_multi_model_test()
