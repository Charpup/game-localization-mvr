#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
test_data_compatibility.py
Regression tests for data format compatibility.

Ensures v1.2.0 can process:
- v1.1.x generated data files
- Checkpoint format compatibility
- Glossary format compatibility

Usage:
    pytest tests/regression/test_data_compatibility.py -v
    pytest tests/regression/test_data_compatibility.py -m "v1_1_data" -v
"""

import os
import sys
import json
import csv
import yaml
import tempfile
import pytest
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# Ensure imports work
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "scripts"))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "skill" / "scripts"))

from scripts.normalize_guard import PlaceholderFreezer, NormalizeGuard
from scripts.rehydrate_export import RehydrateExporter


# =============================================================================
# pytest Configuration
# =============================================================================

def pytest_configure(config):
    """Register custom markers for data compatibility tests."""
    config.addinivalue_line("markers", "v1_1_data: v1.1.x generated data compatibility")
    config.addinivalue_line("markers", "checkpoint: Checkpoint format compatibility")
    config.addinivalue_line("markers", "glossary: Glossary format compatibility")
    config.addinivalue_line("markers", "regression: Regression test suite")


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def schema_path() -> str:
    """Path to placeholder schema."""
    return str(Path(__file__).parent.parent.parent / "skill" / "workflow" / "placeholder_schema.yaml")


# =============================================================================
# v1.1.x Generated Data Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_data
class TestV11GeneratedDataCompatibility:
    """Test v1.1.x generated data can be processed by v1.2.0.
    
    These tests ensure backward compatibility with data files
    generated by v1.1.x versions of the pipeline.
    """
    
    def test_v1_1_normalized_csv_format(self, temp_dir, schema_path):
        """Test v1.1.x normalized CSV can be processed."""
        # Create v1.1.x style normalized CSV
        normalized_csv = temp_dir / "v1_1_normalized.csv"
        with open(normalized_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "is_long_text"])
            writer.writerow(["1", "⟦PH_1⟧ test", "False"])
            writer.writerow(["2", "<b>bold</b>", "False"])
        
        # Should be readable
        with open(normalized_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 2
            assert "string_id" in rows[0]
            assert "source_zh" in rows[0]
    
    def test_v1_1_translated_csv_format(self, temp_dir):
        """Test v1.1.x translated CSV format."""
        translated_csv = temp_dir / "v1_1_translated.csv"
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_text", "confidence"])
            writer.writerow(["1", "攻击", "Атака", "0.95"])
            writer.writerow(["2", "防御", "Защита", "0.92"])
        
        with open(translated_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 2
            assert rows[0]["target_text"] == "Атака"
    
    def test_v1_1_qa_report_format(self, temp_dir):
        """Test v1.1.x QA report JSON format."""
        qa_report = temp_dir / "v1_1_qa_report.json"
        
        # v1.1.x style QA report
        v1_1_report = {
            "metadata": {
                "generated_at": "2026-01-15T10:00:00",
                "version": "1.1.0"
            },
            "summary": {
                "total_rows": 100,
                "errors_found": 5,
                "error_rate": 0.05
            },
            "errors": [
                {
                    "row_id": "1",
                    "type": "placeholder_mismatch",
                    "severity": "critical",
                    "message": "Missing token ⟦PH_1⟧"
                }
            ]
        }
        
        with open(qa_report, "w", encoding="utf-8") as f:
            json.dump(v1_1_report, f, ensure_ascii=False, indent=2)
        
        # Should be readable
        with open(qa_report, "r", encoding="utf-8") as f:
            loaded = json.load(f)
        
        assert loaded["summary"]["total_rows"] == 100
        assert len(loaded["errors"]) == 1
    
    def test_v1_1_llm_trace_format(self, temp_dir):
        """Test v1.1.x LLM trace JSONL format."""
        trace_file = temp_dir / "v1_1_llm_trace.jsonl"
        
        # v1.1.x style trace entries
        trace_entries = [
            {
                "timestamp": "2026-01-15T10:00:01",
                "phase": "translate",
                "model": "gpt-4",
                "input_tokens": 100,
                "output_tokens": 50,
                "cost": 0.003
            },
            {
                "timestamp": "2026-01-15T10:00:02",
                "phase": "qa",
                "model": "gpt-3.5",
                "input_tokens": 80,
                "output_tokens": 30,
                "cost": 0.001
            }
        ]
        
        with open(trace_file, "w", encoding="utf-8") as f:
            for entry in trace_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        
        # Should be readable line by line
        with open(trace_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            assert len(lines) == 2
            loaded = json.loads(lines[0])
            assert loaded["phase"] == "translate"
    
    def test_v1_1_metrics_report_format(self, temp_dir):
        """Test v1.1.x metrics report markdown format."""
        metrics_file = temp_dir / "v1_1_metrics.md"
        
        # v1.1.x style metrics report
        report = """# Metrics Report

## Summary
- Total Rows: 1000
- Total Cost: $3.50
- Avg Cost/Row: $0.0035

## Phase Breakdown
| Phase | Calls | Tokens | Cost |
|-------|-------|--------|------|
| translate | 1000 | 150000 | $3.00 |
| qa | 1000 | 50000 | $0.50 |

## Model Distribution
- gpt-4: 100%
"""
        
        with open(metrics_file, "w", encoding="utf-8") as f:
            f.write(report)
        
        # Should be readable
        with open(metrics_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        assert "Total Rows: 1000" in content
        assert "translate" in content


# =============================================================================
# Checkpoint Format Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.checkpoint
class TestCheckpointFormatCompatibility:
    """Test checkpoint format compatibility.
    
    Checkpoints are used to resume batch processing
    and must maintain backward compatibility.
    """
    
    def test_v1_1_checkpoint_format(self, temp_dir):
        """Test v1.1.x checkpoint JSON format."""
        checkpoint_file = temp_dir / "v1_1_checkpoint.json"
        
        # v1.1.x style checkpoint
        checkpoint = {
            "version": "1.1.0",
            "batch_id": "batch_001",
            "created_at": "2026-01-15T10:00:00",
            "completed_rows": 500,
            "total_rows": 1000,
            "status": "in_progress",
            "row_status": {
                "0": "completed",
                "1": "completed",
                "2": "failed"
            },
            "failed_rows": ["2"],
            "metadata": {
                "input_file": "input.csv",
                "output_file": "output.csv"
            }
        }
        
        with open(checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        
        # Should be readable
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            loaded = json.load(f)
        
        assert loaded["completed_rows"] == 500
        assert loaded["total_rows"] == 1000
        assert "row_status" in loaded
    
    def test_v1_2_checkpoint_format(self, temp_dir):
        """Test v1.2.x checkpoint JSON format (current)."""
        checkpoint_file = temp_dir / "v1_2_checkpoint.json"
        
        # v1.2.x style checkpoint (may have additional fields)
        checkpoint = {
            "version": "1.2.0",
            "batch_id": "batch_002",
            "created_at": "2026-02-14T10:00:00",
            "updated_at": "2026-02-14T10:30:00",
            "completed_rows": 750,
            "total_rows": 1000,
            "status": "in_progress",
            "row_status": {
                "0": {"status": "completed", "timestamp": "2026-02-14T10:05:00"},
                "1": {"status": "completed", "timestamp": "2026-02-14T10:10:00"},
                "2": {"status": "failed", "error": "API timeout"}
            },
            "failed_rows": ["2"],
            "metadata": {
                "input_file": "input.csv",
                "output_file": "output.csv",
                "pipeline_version": "1.2.0"
            },
            "performance": {
                "avg_time_per_row": 2.5,
                "estimated_completion": "2026-02-14T11:00:00"
            }
        }
        
        with open(checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            loaded = json.load(f)
        
        assert loaded["version"] == "1.2.0"
        assert "performance" in loaded
    
    def test_checkpoint_resume_compatibility(self, temp_dir):
        """Test that v1.1 checkpoints can inform v1.2 processing."""
        checkpoint_file = temp_dir / "checkpoint.json"
        
        # Create a v1.1 style checkpoint
        checkpoint = {
            "completed_rows": 100,
            "total_rows": 200,
            "failed_rows": ["5", "10"],
            "row_status": {str(i): "completed" for i in range(100)}
        }
        checkpoint["row_status"]["5"] = "failed"
        checkpoint["row_status"]["10"] = "failed"
        
        with open(checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f)
        
        # Load and validate structure
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            loaded = json.load(f)
        
        # Both v1.1 and v1.2 should have these keys
        assert "completed_rows" in loaded
        assert "total_rows" in loaded
        assert "failed_rows" in loaded
        
        # Can calculate resume position
        resume_from = loaded["completed_rows"]
        assert resume_from == 100
    
    def test_checkpoint_migration_v1_to_v2(self, temp_dir):
        """Test checkpoint data can be migrated from v1.1 to v1.2."""
        v1_checkpoint_file = temp_dir / "v1_checkpoint.json"
        
        # v1.1 format with simple row_status
        v1_checkpoint = {
            "version": "1.1.0",
            "completed_rows": 50,
            "total_rows": 100,
            "row_status": {
                "0": "completed",
                "1": "completed",
                "2": "failed"
            }
        }
        
        with open(v1_checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(v1_checkpoint, f)
        
        # Simulate migration to v1.2 format
        with open(v1_checkpoint_file, "r", encoding="utf-8") as f:
            v1_data = json.load(f)
        
        # Migrate to v1.2
        v2_data = {
            "version": "1.2.0",
            "completed_rows": v1_data["completed_rows"],
            "total_rows": v1_data["total_rows"],
            "row_status": {}
        }
        
        # Convert simple status to detailed status
        for row_id, status in v1_data["row_status"].items():
            v2_data["row_status"][row_id] = {
                "status": status,
                "timestamp": datetime.now().isoformat()
            }
        
        # Verify migration
        assert v2_data["version"] == "1.2.0"
        assert len(v2_data["row_status"]) == len(v1_data["row_status"])


# =============================================================================
# Glossary Format Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.glossary
class TestGlossaryFormatCompatibility:
    """Test glossary format compatibility.
    
    Glossaries are critical for consistent translation
    and must maintain backward compatibility.
    """
    
    def test_v1_1_glossary_approved_format(self, temp_dir):
        """Test v1.1.x approved glossary YAML format."""
        glossary_file = temp_dir / "v1_1_approved.yaml"
        
        # v1.1.x style approved glossary
        glossary = {
            "meta": {
                "type": "approved",
                "language_pair": "zhCN_ruRU",
                "version": "1.1.0"
            },
            "entries": [
                {
                    "term_zh": "攻击",
                    "term_ru": "Атака",
                    "status": "approved",
                    "confidence": 1.0
                },
                {
                    "term_zh": "防御",
                    "term_ru": "Защита",
                    "status": "approved",
                    "confidence": 0.95
                }
            ]
        }
        
        with open(glossary_file, "w", encoding="utf-8") as f:
            yaml.dump(glossary, f, allow_unicode=True)
        
        # Should be readable
        with open(glossary_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["meta"]["type"] == "approved"
        assert len(loaded["entries"]) == 2
        assert loaded["entries"][0]["term_zh"] == "攻击"
    
    def test_v1_1_glossary_compiled_format(self, temp_dir):
        """Test v1.1.x compiled glossary YAML format."""
        glossary_file = temp_dir / "v1_1_compiled.yaml"
        
        # v1.1.x style compiled glossary
        glossary = {
            "meta": {
                "type": "compiled",
                "entry_count": 2,
                "note": "Runtime read-only"
            },
            "entries": [
                {"term_zh": "攻击", "term_ru": "Атака", "scope": "general"},
                {"term_zh": "防御", "term_ru": "Защита", "scope": "general"}
            ]
        }
        
        with open(glossary_file, "w", encoding="utf-8") as f:
            yaml.dump(glossary, f, allow_unicode=True)
        
        with open(glossary_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["meta"]["type"] == "compiled"
        assert loaded["meta"]["entry_count"] == 2
    
    def test_v1_2_glossary_format(self, temp_dir):
        """Test v1.2.x glossary YAML format (current)."""
        glossary_file = temp_dir / "v1_2_glossary.yaml"
        
        # v1.2.x style glossary (may have additional fields)
        glossary = {
            "meta": {
                "type": "compiled",
                "entry_count": 2,
                "language_pair": "zhCN_ruRU",
                "version": "1.2.0",
                "generated_at": "2026-02-14T10:00:00"
            },
            "entries": [
                {
                    "term_zh": "攻击",
                    "term_ru": "Атака",
                    "scope": "general",
                    "tags": ["combat", "action"],
                    "domain": "gameplay"
                }
            ]
        }
        
        with open(glossary_file, "w", encoding="utf-8") as f:
            yaml.dump(glossary, f, allow_unicode=True)
        
        with open(glossary_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert "tags" in loaded["entries"][0]
        assert "domain" in loaded["entries"][0]
    
    def test_glossary_term_candidates_format(self, temp_dir):
        """Test term candidates YAML format."""
        candidates_file = temp_dir / "term_candidates.yaml"
        
        # Term candidates format
        candidates = {
            "meta": {
                "type": "candidates",
                "generated_at": "2026-02-14T10:00:00",
                "source_file": "input.csv"
            },
            "terms": [
                {
                    "term": "技能",
                    "frequency": 150,
                    "contexts": ["主动技能", "被动技能"],
                    "suggested_translation": "",
                    "confidence": 0.0
                },
                {
                    "term": "暴击",
                    "frequency": 89,
                    "contexts": ["暴击率", "暴击伤害"],
                    "suggested_translation": "",
                    "confidence": 0.0
                }
            ]
        }
        
        with open(candidates_file, "w", encoding="utf-8") as f:
            yaml.dump(candidates, f, allow_unicode=True)
        
        with open(candidates_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["meta"]["type"] == "candidates"
        assert len(loaded["terms"]) == 2
        assert loaded["terms"][0]["frequency"] == 150
    
    def test_glossary_global_format(self, temp_dir):
        """Test global glossary format."""
        global_file = temp_dir / "global.yaml"
        
        # Global glossary format
        global_glossary = {
            "meta": {
                "scope": "global",
                "language_pair": "*",
                "version": 1
            },
            "entries": [
                {
                    "term_zh": "确定",
                    "term_ru": "OK",
                    "status": "approved",
                    "tags": ["ui", "button"],
                    "confidence": 1.0
                }
            ]
        }
        
        with open(global_file, "w", encoding="utf-8") as f:
            yaml.dump(global_glossary, f, allow_unicode=True)
        
        with open(global_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["meta"]["scope"] == "global"
        assert loaded["meta"]["language_pair"] == "*"
    
    def test_glossary_merge_compatibility(self, temp_dir):
        """Test that v1.1 and v1.2 glossaries can be merged."""
        # Create v1.1 glossary
        v1_glossary = {
            "meta": {"type": "approved"},
            "entries": [
                {"term_zh": "攻击", "term_ru": "Атака", "status": "approved"}
            ]
        }
        
        # Create v1.2 glossary with additional fields
        v2_glossary = {
            "meta": {"type": "approved", "version": "1.2.0"},
            "entries": [
                {"term_zh": "防御", "term_ru": "Защита", "status": "approved", "tags": ["combat"]}
            ]
        }
        
        # Simulate merge (take entries from both)
        merged = {
            "meta": {"type": "compiled", "version": "1.2.0"},
            "entries": v1_glossary["entries"] + v2_glossary["entries"]
        }
        
        assert len(merged["entries"]) == 2
        assert merged["entries"][0]["term_zh"] == "攻击"
        assert merged["entries"][1]["term_zh"] == "防御"


# =============================================================================
# Integration Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_data
@pytest.mark.checkpoint
@pytest.mark.glossary
class TestDataIntegrationCompatibility:
    """Integration tests for data format compatibility."""
    
    def test_full_workflow_with_v1_1_data(self, temp_dir, schema_path):
        """Test full pipeline can process v1.1 style data."""
        # Step 1: Create v1.1 style input
        input_csv = temp_dir / "input.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh"])
            writer.writerow(["1", "攻击 {value} 点"])
        
        # Step 2: Normalize
        normalized_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        guard = NormalizeGuard(str(input_csv), str(normalized_csv), str(map_json), schema_path)
        guard.run()
        
        # Step 3: Create v1.1 style glossary
        glossary_file = temp_dir / "glossary.yaml"
        glossary = {
            "meta": {"type": "compiled", "entry_count": 1},
            "entries": [{"term_zh": "攻击", "term_ru": "Атака", "scope": "general"}]
        }
        with open(glossary_file, "w", encoding="utf-8") as f:
            yaml.dump(glossary, f, allow_unicode=True)
        
        # Step 4: Create v1.1 style checkpoint
        checkpoint_file = temp_dir / "checkpoint.json"
        checkpoint = {
            "version": "1.1.0",
            "completed_rows": 0,
            "total_rows": 1,
            "row_status": {"0": "pending"}
        }
        with open(checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f)
        
        # All files should exist and be valid
        assert normalized_csv.exists()
        assert map_json.exists()
        
        # Verify map can be loaded by RehydrateExporter
        with open(map_json, "r", encoding="utf-8") as f:
            map_data = json.load(f)
        
        # Should be loadable
        assert len(map_data) > 0 or "mappings" in map_data
    
    def test_v1_1_repair_tasks_format(self, temp_dir):
        """Test v1.1.x repair tasks JSONL format."""
        tasks_file = temp_dir / "repair_tasks.jsonl"
        
        # v1.1.x style repair tasks
        tasks = [
            {
                "row_id": "1",
                "issue_type": "placeholder_mismatch",
                "severity": "critical",
                "original_text": "test",
                "suggested_fix": "fix"
            },
            {
                "row_id": "2",
                "issue_type": "forbidden_pattern",
                "severity": "major",
                "pattern": "XXX",
                "original_text": "test"
            }
        ]
        
        with open(tasks_file, "w", encoding="utf-8") as f:
            for task in tasks:
                f.write(json.dumps(task, ensure_ascii=False) + "\n")
        
        # Should be readable
        with open(tasks_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            assert len(lines) == 2
            loaded = json.loads(lines[0])
            assert loaded["row_id"] == "1"
            assert loaded["severity"] == "critical"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
