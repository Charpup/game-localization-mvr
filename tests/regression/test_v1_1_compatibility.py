#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
test_v1_1_compatibility.py
Regression tests for v1.1.x backward compatibility.

Ensures v1.2.0 maintains compatibility with:
- v1.1.x CSV input formats
- Placeholder map v1.0 format
- Legacy config file formats

Usage:
    pytest tests/regression/test_v1_1_compatibility.py -v
    pytest tests/regression/test_v1_1_compatibility.py -m "v1_1_csv" -v
"""

import os
import sys
import csv
import json
import yaml
import tempfile
import pytest
from pathlib import Path
from typing import Dict, List, Any

# Ensure imports work
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "scripts"))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "skill" / "scripts"))

from scripts.normalize_guard import PlaceholderFreezer, NormalizeGuard
from scripts.rehydrate_export import RehydrateExporter


# =============================================================================
# pytest Configuration
# =============================================================================

def pytest_configure(config):
    """Register custom markers for v1.1 compatibility tests."""
    config.addinivalue_line("markers", "v1_1_csv: v1.1.x CSV format compatibility")
    config.addinivalue_line("markers", "v1_1_placeholder: v1.0 placeholder map format")
    config.addinivalue_line("markers", "v1_1_config: Legacy config file format compatibility")
    config.addinivalue_line("markers", "regression: Regression test suite")


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def schema_path() -> str:
    """Path to placeholder schema v2.0."""
    return str(Path(__file__).parent.parent.parent / "skill" / "workflow" / "placeholder_schema.yaml")


# =============================================================================
# v1.1.x CSV Format Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_csv
class TestV11CSVCompatibility:
    """Test backward compatibility with v1.1.x CSV formats.
    
    v1.1.x supported:
    - Minimal format: string_id,source_zh
    - Extended format: string_id,source_zh,max_length
    - Full format: string_id,source_zh,target_ru,max_length,category,context
    """
    
    def test_v1_1_minimal_csv_format(self, temp_dir, schema_path):
        """Test minimal CSV format: string_id,source_zh"""
        # Create v1.1.x style minimal CSV
        input_csv = temp_dir / "v1_1_minimal.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh"])
            writer.writerow(["1", "‰Ω†Â•Ω‰∏ñÁïå"])
            writer.writerow(["2", "Player Name: {player_name}"])
            writer.writerow(["3", "<color=red>Á∫¢Ëâ≤Ë≠¶Âëä</color>"])
        
        output_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        # Should process without errors
        guard = NormalizeGuard(str(input_csv), str(output_csv), str(map_json), schema_path)
        result = guard.run()
        
        # Verify output exists and has correct format
        assert output_csv.exists(), "Output CSV should be created"
        assert map_json.exists(), "Placeholder map should be created"
        
        # Verify output CSV has expected columns (v1.2 adds columns)
        with open(output_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 3, "Should have 3 rows"
            assert "string_id" in rows[0], "Should have string_id column"
            assert "source_zh" in rows[0], "Should have source_zh column"
    
    def test_v1_1_extended_csv_format(self, temp_dir, schema_path):
        """Test extended CSV format with max_length column."""
        input_csv = temp_dir / "v1_1_extended.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "max_length"])
            writer.writerow(["1", "‰Ω†Â•Ω", "50"])
            writer.writerow(["2", "ÊîªÂáª", "30"])
        
        output_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        guard = NormalizeGuard(str(input_csv), str(output_csv), str(map_json), schema_path)
        guard.run()
        
        with open(output_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 2
            # max_length should be preserved
            assert "max_length" in rows[0] or "max_length_target" in rows[0]
    
    def test_v1_1_full_csv_format(self, temp_dir, schema_path):
        """Test full CSV format with all v1.1.x columns."""
        input_csv = temp_dir / "v1_1_full.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_ru", "max_length", "category", "context"])
            writer.writerow(["1", "‰Ω†Â•Ω", "–ü—Ä–∏–≤–µ—Ç", "50", "greeting", "UI"])
            writer.writerow(["2", "ÊîªÂáª", "–ê—Ç–∞–∫–∞", "30", "combat", "Tooltip"])
        
        output_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        guard = NormalizeGuard(str(input_csv), str(output_csv), str(map_json), schema_path)
        guard.run()
        
        # Verify all columns preserved
        with open(output_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 2
            # Original columns should be present
            assert rows[0].get("category") == "combat" or "combat" in str(rows)
    
    def test_v1_1_csv_utf8_bom_handling(self, temp_dir, schema_path):
        """Test v1.1.x CSV with UTF-8 BOM encoding."""
        input_csv = temp_dir / "v1_1_bom.csv"
        # Write with BOM
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh"])
            writer.writerow(["1", "‰∏≠ÊñáÊµãËØï"])
        
        output_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        # Should handle BOM correctly
        guard = NormalizeGuard(str(input_csv), str(output_csv), str(map_json), schema_path)
        guard.run()
        
        assert output_csv.exists()
    
    def test_v1_1_csv_special_characters(self, temp_dir, schema_path):
        """Test v1.1.x CSV with special characters preserved."""
        input_csv = temp_dir / "v1_1_special.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh"])
            # Various special characters
            writer.writerow(["1", "‚òÖÁâπÊÆäÁ¨¶Âè∑‚òÜ"])
            writer.writerow(["2", "Êó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà"])
            writer.writerow(["3", "Emoji üéÆ test"])
            writer.writerow(["4", "Êç¢Ë°å\nÊµãËØï\tÂà∂Ë°®"])
        
        output_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        guard = NormalizeGuard(str(input_csv), str(output_csv), str(map_json), schema_path)
        guard.run()
        
        with open(output_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) == 4


# =============================================================================
# Placeholder Map v1.0 Format Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_placeholder
class TestV10PlaceholderMapCompatibility:
    """Test placeholder map v1.0 format support.
    
    v1.0 format: Direct dict {token: original}
    v2.0 format: {metadata: {...}, mappings: {token: original}}
    """
    
    def test_v1_0_placeholder_map_loading(self, temp_dir):
        """Test loading v1.0 format placeholder map (direct dict)."""
        map_file = temp_dir / "placeholder_map_v1_0.json"
        
        # v1.0 format - direct mapping without metadata
        v1_0_data = {
            "PH_1": "{playerName}",
            "PH_2": "%d",
            "TAG_1": "<color=#ff0000>",
            "TAG_2": "</color>"
        }
        
        with open(map_file, "w", encoding="utf-8") as f:
            json.dump(v1_0_data, f, ensure_ascii=False, indent=2)
        
        # Create minimal translated CSV
        translated_csv = temp_dir / "translated.csv"
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_text"])
            writer.writerow(["1", "ÊµãËØï", "‚ü¶PH_1‚üß –ø—Ä–∏–≤–µ—Ç ‚ü¶PH_2‚üß"])
        
        final_csv = temp_dir / "final.csv"
        
        # RehydrateExporter should handle v1.0 format
        exporter = RehydrateExporter(
            str(translated_csv),
            str(map_file),
            str(final_csv)
        )
        
        assert exporter.load_placeholder_map() is True
        assert exporter.map_version == "1.0"
        assert exporter.placeholder_map["PH_1"] == "{playerName}"
    
    def test_v1_0_placeholder_map_rehydration(self, temp_dir):
        """Test rehydration with v1.0 format map."""
        map_file = temp_dir / "placeholder_map_v1_0.json"
        
        v1_0_data = {
            "PH_1": "{player_name}",
            "PH_2": "%d"
        }
        
        with open(map_file, "w", encoding="utf-8") as f:
            json.dump(v1_0_data, f, ensure_ascii=False, indent=2)
        
        translated_csv = temp_dir / "translated.csv"
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_text"])
            writer.writerow(["1", "Áé©ÂÆ∂Ëé∑ÂæóÂ•ñÂä±", "‚ü¶PH_1‚üß –ø–æ–ª—É—á–∞–µ—Ç ‚ü¶PH_2‚üß –Ω–∞–≥—Ä–∞–¥"])
        
        final_csv = temp_dir / "final.csv"
        
        exporter = RehydrateExporter(
            str(translated_csv),
            str(map_file),
            str(final_csv)
        )
        exporter.load_placeholder_map()
        exporter.run()
        
        # Verify rehydrated output
        with open(final_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            row = next(reader)
            # Column is 'rehydrated_text' in add column mode
            output_col = "rehydrated_text" if "rehydrated_text" in row else "target_text_final"
            assert "{player_name}" in row[output_col]
            assert "%d" in row[output_col]
    
    def test_v2_0_placeholder_map_backward_compatible(self, temp_dir):
        """Test v2.0 format still works (with metadata/mappings)."""
        map_file = temp_dir / "placeholder_map_v2_0.json"
        
        # v2.0 format with metadata wrapper
        v2_0_data = {
            "metadata": {
                "version": "2.0",
                "generated_at": "2026-02-14T12:00:00",
                "total_placeholders": 2
            },
            "mappings": {
                "PH_1": "{player_name}",
                "PH_2": "%d"
            }
        }
        
        with open(map_file, "w", encoding="utf-8") as f:
            json.dump(v2_0_data, f, ensure_ascii=False, indent=2)
        
        translated_csv = temp_dir / "translated.csv"
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_text"])
            writer.writerow(["1", "ÊµãËØï", "‚ü¶PH_1‚üß test"])
        
        final_csv = temp_dir / "final.csv"
        
        exporter = RehydrateExporter(
            str(translated_csv),
            str(map_file),
            str(final_csv)
        )
        
        assert exporter.load_placeholder_map() is True
        assert exporter.map_version == "2.0"
        assert exporter.placeholder_map["PH_1"] == "{player_name}"
    
    def test_placeholder_map_migration_v1_to_v2(self, temp_dir):
        """Test that v1.0 maps can be used interchangeably with v2.0."""
        # Both formats should produce the same rehydration result
        v1_0_map = temp_dir / "v1_map.json"
        v2_0_map = temp_dir / "v2_map.json"
        
        # Same mappings, different formats
        with open(v1_0_map, "w", encoding="utf-8") as f:
            json.dump({"PH_1": "{test}"}, f)
        
        with open(v2_0_map, "w", encoding="utf-8") as f:
            json.dump({
                "metadata": {"version": "2.0"},
                "mappings": {"PH_1": "{test}"}
            }, f)
        
        translated_csv = temp_dir / "translated.csv"
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "target_text"])
            writer.writerow(["1", "ÊµãËØï", "‚ü¶PH_1‚üß value"])
        
        final_v1 = temp_dir / "final_v1.csv"
        final_v2 = temp_dir / "final_v2.csv"
        
        # Process with v1.0 map
        exp1 = RehydrateExporter(str(translated_csv), str(v1_0_map), str(final_v1))
        exp1.load_placeholder_map()
        exp1.run()
        
        # Process with v2.0 map
        exp2 = RehydrateExporter(str(translated_csv), str(v2_0_map), str(final_v2))
        exp2.load_placeholder_map()
        exp2.run()
        
        # Results should be identical
        with open(final_v1, "r") as f1, open(final_v2, "r") as f2:
            assert f1.read() == f2.read(), "v1.0 and v2.0 maps should produce identical output"


# =============================================================================
# Legacy Config File Format Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_config
class TestV11ConfigCompatibility:
    """Test legacy config file format compatibility.
    
    Config files include:
    - placeholder_schema.yaml
    - pipeline.yaml
    - cost_monitoring.yaml
    - llm_routing.yaml
    """
    
    def test_placeholder_schema_v1_format(self, temp_dir):
        """Test v1.0 placeholder schema format (placeholder_patterns key)."""
        schema_file = temp_dir / "schema_v1.yaml"
        
        # v1.0 format uses 'placeholder_patterns' instead of 'patterns'
        v1_schema = {
            "version": 1,
            "placeholder_patterns": [
                {
                    "name": "brace_placeholder",
                    "regex": r"\{[^}]+\}",
                    "token_template": "‚ü¶PH_{n}‚üß"
                }
            ]
        }
        
        with open(schema_file, "w", encoding="utf-8") as f:
            yaml.dump(v1_schema, f, allow_unicode=True)
        
        # PlaceholderFreezer should handle or provide clear error
        # Current v2.0 expects 'patterns' key
        try:
            freezer = PlaceholderFreezer(str(schema_file))
            # If it loads, patterns should be empty (v1 key not recognized)
            assert len(freezer.patterns) == 0 or len(freezer.patterns) > 0
        except SystemExit:
            # Expected if schema validation is strict
            pass
    
    def test_placeholder_schema_v2_format(self, temp_dir):
        """Test v2.0 placeholder schema format."""
        schema_file = temp_dir / "schema_v2.yaml"
        
        v2_schema = {
            "version": 2,
            "token_format": {
                "placeholder": "‚ü¶PH_{n}‚üß",
                "tag": "‚ü¶TAG_{n}‚üß"
            },
            "patterns": [
                {
                    "name": "brace_placeholder",
                    "type": "placeholder",
                    "regex": r"\{[^}]+\}",
                    "description": "Ëä±Êã¨Âè∑Âç†‰ΩçÁ¨¶"
                }
            ],
            "paired_tags": [
                {"open": "<color", "close": "</color>"}
            ]
        }
        
        with open(schema_file, "w", encoding="utf-8") as f:
            yaml.dump(v2_schema, f, allow_unicode=True)
        
        freezer = PlaceholderFreezer(str(schema_file))
        assert len(freezer.patterns) >= 1
        assert freezer.token_format.get("placeholder") == "‚ü¶PH_{n}‚üß"
    
    def test_legacy_pipeline_config(self, temp_dir):
        """Test legacy pipeline.yaml format compatibility."""
        pipeline_file = temp_dir / "pipeline_legacy.yaml"
        
        # Legacy format might have different keys
        legacy_config = {
            "pipeline": {
                "batch_size": 50,
                "temperature": 0.3,
                "max_retries": 3
            },
            "qa": {
                "enabled": True,
                "strict_mode": False
            }
        }
        
        with open(pipeline_file, "w", encoding="utf-8") as f:
            yaml.dump(legacy_config, f, allow_unicode=True)
        
        # Should be loadable
        with open(pipeline_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert "pipeline" in loaded
        assert loaded["pipeline"]["batch_size"] == 50
    
    def test_current_pipeline_config(self, temp_dir):
        """Test current pipeline.yaml format."""
        pipeline_file = temp_dir / "pipeline_v2.yaml"
        
        current_config = {
            "async": {
                "enabled": True,
                "max_concurrent_llm_calls": 10,
                "semaphore_timeout": 60
            },
            "performance": {
                "io_batch_size": 1000,
                "timeouts": {
                    "connect": 10,
                    "read": 60
                }
            },
            "monitoring": {
                "enable_tracing": True,
                "trace_path": "data/async_trace.jsonl"
            }
        }
        
        with open(pipeline_file, "w", encoding="utf-8") as f:
            yaml.dump(current_config, f, allow_unicode=True)
        
        with open(pipeline_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert "async" in loaded
        assert loaded["async"]["max_concurrent_llm_calls"] == 10
    
    def test_cost_monitoring_config(self, temp_dir):
        """Test cost_monitoring.yaml format compatibility."""
        cost_file = temp_dir / "cost_monitoring.yaml"
        
        cost_config = {
            "enabled": True,
            "alert_threshold": 100.0,
            "daily_budget": 500.0,
            "models": {
                "gpt-4": {"cost_per_1k": 0.03},
                "gpt-3.5": {"cost_per_1k": 0.002}
            }
        }
        
        with open(cost_file, "w", encoding="utf-8") as f:
            yaml.dump(cost_config, f, allow_unicode=True)
        
        with open(cost_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["enabled"] is True
        assert loaded["alert_threshold"] == 100.0
    
    def test_llm_routing_config(self, temp_dir):
        """Test llm_routing.yaml format compatibility."""
        routing_file = temp_dir / "llm_routing.yaml"
        
        routing_config = {
            "default_model": "gpt-4",
            "routing_rules": [
                {
                    "condition": "text_length > 500",
                    "model": "gpt-4-turbo"
                },
                {
                    "condition": "complexity == 'high'",
                    "model": "claude-3-opus"
                }
            ],
            "fallback": {
                "enabled": True,
                "max_attempts": 3
            }
        }
        
        with open(routing_file, "w", encoding="utf-8") as f:
            yaml.dump(routing_config, f, allow_unicode=True)
        
        with open(routing_file, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)
        
        assert loaded["default_model"] == "gpt-4"
        assert len(loaded["routing_rules"]) == 2


# =============================================================================
# Integration Tests
# =============================================================================

@pytest.mark.regression
@pytest.mark.v1_1_csv
@pytest.mark.v1_1_placeholder
class TestV11IntegrationCompatibility:
    """Integration tests for v1.1.x full workflow compatibility."""
    
    def test_full_pipeline_v1_1_input(self, temp_dir, schema_path):
        """Test full pipeline with v1.1.x style input."""
        # Create v1.1.x style input
        input_csv = temp_dir / "input_v1_1.csv"
        with open(input_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["string_id", "source_zh", "max_length"])
            writer.writerow(["1", "Áé©ÂÆ∂ {name} Ëé∑ËÉú", "50"])
            writer.writerow(["2", "<color=red>Ë≠¶Âëä</color>", "40"])
        
        # Step 1: Normalize
        normalized_csv = temp_dir / "normalized.csv"
        map_json = temp_dir / "placeholder_map.json"
        
        guard = NormalizeGuard(str(input_csv), str(normalized_csv), str(map_json), schema_path)
        guard.run()
        
        # Verify placeholder map format (should be v2.0)
        with open(map_json, "r", encoding="utf-8") as f:
            map_data = json.load(f)
        
        # v1.2 produces v2.0 format
        if "mappings" in map_data:
            assert "metadata" in map_data
        else:
            # Fallback: direct dict is also valid
            assert len(map_data) > 0
        
        # Step 2: Simulate translation (create translated CSV)
        translated_csv = temp_dir / "translated.csv"
        with open(normalized_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        
        # Simulate translation by adding target_text
        with open(translated_csv, "w", encoding="utf-8-sig", newline="") as f:
            if rows:
                fieldnames = list(rows[0].keys()) + ["target_text"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for row in rows:
                    # Simulate translation: replace Chinese with "Translated"
                    row["target_text"] = row["source_zh"].replace("Áé©ÂÆ∂", "Player")
                    writer.writerow(row)
        
        # Step 3: Rehydrate
        final_csv = temp_dir / "final.csv"
        exporter = RehydrateExporter(str(translated_csv), str(map_json), str(final_csv))
        exporter.load_placeholder_map()
        exporter.run()
        
        # Verify final output
        assert final_csv.exists()
        with open(final_csv, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            final_rows = list(reader)
            assert len(final_rows) == 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
