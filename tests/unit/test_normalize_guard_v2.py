#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
test_normalize_guard_v2.py
å®Œæ•´å•å…ƒæµ‹è¯• for normalize_guard.py
ç›®æ ‡ï¼š90%+ æµ‹è¯•è¦†ç›–ç‡
"""

import os
import sys
import json
import csv
import pytest
from io import StringIO
from pathlib import Path
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock, mock_open

# ç¡®ä¿èƒ½å¯¼å…¥ scripts ç›®å½•ä¸‹çš„æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.scripts.normalize_guard import (
    PlaceholderFreezer,
    NormalizeGuard,
    detect_unbalanced_basic,
    TAG_PATTERN,
    LONG_TEXT_THRESHOLD
)


# ============================================================================
# Fixtures
# ============================================================================

@pytest.fixture
def mock_schema_content():
    """æ¨¡æ‹Ÿ schema YAML å†…å®¹ - ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²é¿å… YAML è½¬ä¹‰é—®é¢˜"""
    return '''
version: 2
token_format:
  placeholder: "âŸ¦PH_{n}âŸ§"
  tag: "âŸ¦TAG_{n}âŸ§"
patterns:
  - name: brace_placeholder
    type: placeholder
    regex: '\\{[^{}]+\\}'
  - name: printf_placeholder
    type: placeholder
    regex: '%(?:\\d+\\$)?[\\-\\+0# ]*(?:\\d+)?(?:\\.\\d+)?[a-zA-Z]'
  - name: angle_tag
    type: tag
    regex: '</?\\w+(?:\\s*=?\\s*[^>]*)?>'
  - name: escapes
    type: placeholder
    regex: '\\\\[ntr]'
'''


@pytest.fixture
def temp_schema_file(tmp_path, mock_schema_content):
    """åˆ›å»ºä¸´æ—¶ schema æ–‡ä»¶"""
    schema_path = tmp_path / "test_schema.yaml"
    schema_path.write_text(mock_schema_content, encoding='utf-8')
    return str(schema_path)


@pytest.fixture
def freezer(temp_schema_file):
    """åˆ›å»º PlaceholderFreezer å®ä¾‹"""
    return PlaceholderFreezer(temp_schema_file)


@pytest.fixture
def sample_csv_content():
    """ç¤ºä¾‹ CSV å†…å®¹"""
    return """string_id,source_zh,context
TEST_001,æå‡è‡ªèº«æ”»å‡»åŠ›{0}ç‚¹,æŠ€èƒ½æè¿°
TEST_002,<color=red>è­¦å‘Š</color>æ–‡æœ¬,UIæ–‡æœ¬
TEST_003,ä¼¤å®³å€¼ï¼š%dç‚¹,æˆ˜æ–—æ•°å€¼
"""


# ============================================================================
# Test PlaceholderFreezer - Initialization
# ============================================================================

class TestPlaceholderFreezerInit:
    """æµ‹è¯• PlaceholderFreezer åˆå§‹åŒ–"""
    
    def test_init_success(self, temp_schema_file):
        """æµ‹è¯•æˆåŠŸåˆå§‹åŒ–"""
        freezer = PlaceholderFreezer(temp_schema_file)
        assert freezer.ph_counter == 0
        assert freezer.tag_counter == 0
        assert freezer.placeholder_map == {}
        assert freezer.reverse_map == {}
        assert len(freezer.patterns) == 4
        assert 'placeholder' in freezer.token_format
        assert 'tag' in freezer.token_format
    
    def test_init_file_not_found(self):
        """æµ‹è¯• schema æ–‡ä»¶ä¸å­˜åœ¨"""
        with pytest.raises(SystemExit) as exc_info:
            PlaceholderFreezer("/nonexistent/schema.yaml")
        assert exc_info.value.code == 1
    
    def test_init_empty_patterns(self, tmp_path):
        """æµ‹è¯•ç©ºçš„ patterns"""
        schema_path = tmp_path / "empty_schema.yaml"
        schema_path.write_text("version: 2\npatterns: []", encoding='utf-8')
        
        with patch('builtins.print') as mock_print:
            freezer = PlaceholderFreezer(str(schema_path))
            mock_print.assert_any_call("âš ï¸  Warning: No patterns found in schema")
    
    def test_init_invalid_yaml(self, tmp_path):
        """æµ‹è¯•æ— æ•ˆçš„ YAML"""
        schema_path = tmp_path / "invalid_schema.yaml"
        schema_path.write_text("invalid: [", encoding='utf-8')
        
        with pytest.raises(SystemExit) as exc_info:
            PlaceholderFreezer(str(schema_path))
        assert exc_info.value.code == 1


# ============================================================================
# Test PlaceholderFreezer - Tag Protection
# ============================================================================

class TestTagProtection:
    """æµ‹è¯•æ ‡ç­¾ä¿æŠ¤åŠŸèƒ½"""
    
    def test_protect_tags_simple(self, freezer):
        """æµ‹è¯•ç®€å•æ ‡ç­¾ä¿æŠ¤"""
        text = "<color=red>çº¢è‰²æ–‡æœ¬</color>"
        frozen, protected = freezer.protect_tags(text)
        
        assert len(protected) == 2
        assert "<color=red>" in protected
        assert "</color>" in protected
        assert "__TAG_0__" in frozen
        assert "__TAG_1__" in frozen
        assert "çº¢è‰²æ–‡æœ¬" in frozen
    
    def test_protect_tags_multiple(self, freezer):
        """æµ‹è¯•å¤šä¸ªæ ‡ç­¾ä¿æŠ¤"""
        text = "<b><color=#FF00FF>åŠ ç²—å½©è‰²</color></b>"
        frozen, protected = freezer.protect_tags(text)
        
        assert len(protected) == 4
        assert "__TAG_0__" in frozen
        assert "__TAG_3__" in frozen
    
    def test_protect_tags_no_tags(self, freezer):
        """æµ‹è¯•æ— æ ‡ç­¾æ–‡æœ¬"""
        text = "æ™®é€šæ–‡æœ¬æ²¡æœ‰æ ‡ç­¾"
        frozen, protected = freezer.protect_tags(text)
        
        assert len(protected) == 0
        assert frozen == text
    
    def test_protect_tags_nested(self, freezer):
        """æµ‹è¯•åµŒå¥—æ ‡ç­¾"""
        text = "<b>ç²—ä½“<i>æ–œä½“</i></b>"
        frozen, protected = freezer.protect_tags(text)
        
        assert len(protected) == 4
        assert all("__TAG_" in frozen for _ in range(4))
    
    def test_restore_tags(self, freezer):
        """æµ‹è¯•æ ‡ç­¾è¿˜åŸ"""
        text = "<color=red>çº¢è‰²</color>"
        frozen, protected = freezer.protect_tags(text)
        restored = freezer.restore_tags(frozen, protected)
        
        assert restored == text
    
    def test_restore_tags_empty_list(self, freezer):
        """æµ‹è¯•ç©ºæ ‡ç­¾åˆ—è¡¨è¿˜åŸ"""
        text = "æ™®é€šæ–‡æœ¬"
        restored = freezer.restore_tags(text, [])
        assert restored == text


# ============================================================================
# Test PlaceholderFreezer - freeze_text
# ============================================================================

class TestFreezeText:
    """æµ‹è¯• freeze_text åŠŸèƒ½"""
    
    def test_freeze_brace_placeholder(self, freezer):
        """æµ‹è¯•èŠ±æ‹¬å·å ä½ç¬¦å†»ç»“"""
        text = "æ”»å‡»åŠ›{0}ç‚¹"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "{0}" not in frozen
        assert "âŸ¦PH_1âŸ§" in frozen
        assert local_map["PH_1"] == "{0}"
    
    def test_freeze_printf_placeholder(self, freezer):
        """æµ‹è¯• printf é£æ ¼å ä½ç¬¦å†»ç»“"""
        text = "ä¼¤å®³å€¼ï¼š%dç‚¹"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "%d" not in frozen
        assert "âŸ¦PH_1âŸ§" in frozen
        assert local_map["PH_1"] == "%d"
    
    def test_freeze_angle_tag(self, freezer):
        """æµ‹è¯•å°–æ‹¬å·æ ‡ç­¾å†»ç»“"""
        text = "<color=red>çº¢è‰²æ–‡æœ¬</color>"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "<color=red>" not in frozen
        assert "</color>" not in frozen
        assert "âŸ¦TAG_1âŸ§" in frozen
        assert local_map["TAG_1"] == "<color=red>"
    
    def test_freeze_escape_sequence(self, freezer):
        """æµ‹è¯•è½¬ä¹‰åºåˆ—å†»ç»“"""
        text = "ç¬¬ä¸€è¡Œ\\nç¬¬äºŒè¡Œ"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "\\n" not in frozen
        assert "âŸ¦PH_1âŸ§" in frozen
        assert local_map["PH_1"] == "\\n"
    
    def test_freeze_empty_string(self, freezer):
        """æµ‹è¯•ç©ºå­—ç¬¦ä¸²"""
        frozen, local_map = freezer.freeze_text("", source_lang='en')
        
        assert frozen == ""
        assert local_map == {}
    
    def test_freeze_token_reuse(self, freezer):
        """æµ‹è¯• token é‡ç”¨æœºåˆ¶"""
        text = "{0}å’Œ{0}"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        # ç›¸åŒå ä½ç¬¦åº”è¯¥é‡ç”¨åŒä¸€ä¸ª token
        assert frozen.count("âŸ¦PH_1âŸ§") == 2
        assert "PH_2" not in local_map
    
    def test_freeze_multiple_different_placeholders(self, freezer):
        """æµ‹è¯•å¤šä¸ªä¸åŒå ä½ç¬¦"""
        text = "{0}æ”»å‡»{1}é˜²å¾¡"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "âŸ¦PH_1âŸ§" in frozen
        assert "âŸ¦PH_2âŸ§" in frozen
        assert local_map["PH_1"] == "{0}"
        assert local_map["PH_2"] == "{1}"
    
    def test_freeze_chinese_segmentation(self, freezer):
        """æµ‹è¯•ä¸­æ–‡åˆ†è¯"""
        text = "æå‡è‡ªèº«æ”»å‡»åŠ›"
        frozen, local_map = freezer.freeze_text(text, source_lang='zh-CN')
        
        # ä¸­æ–‡åº”è¯¥è¢«åˆ†è¯å¹¶æ·»åŠ ç©ºæ ¼
        assert ' ' in frozen
        assert 'æå‡' in frozen
        assert 'è‡ªèº«' in frozen
    
    def test_freeze_chinese_with_placeholder(self, freezer):
        """æµ‹è¯•ä¸­æ–‡åˆ†è¯ä¸å ä½ç¬¦ç»“åˆ"""
        text = "æå‡{0}ç‚¹æ”»å‡»åŠ›"
        frozen, local_map = freezer.freeze_text(text, source_lang='zh-CN')
        
        # ä¸­æ–‡åˆ†è¯ä¼šåœ¨èŠ±æ‹¬å·å‘¨å›´æ·»åŠ ç©ºæ ¼ï¼Œæ‰€ä»¥å®é™…åŒ¹é…çš„æ˜¯ "{ 0 }"
        assert '{0}' not in frozen or '{ 0 }' not in frozen
        assert 'PH_1' in frozen  # token name
        # ä¸­æ–‡éƒ¨åˆ†åº”è¯¥æœ‰åˆ†è¯
        assert ' ' in frozen
    
    def test_freeze_tags_in_chinese(self, freezer):
        """æµ‹è¯•ä¸­æ–‡æ–‡æœ¬ä¸­çš„æ ‡ç­¾ä¿æŠ¤"""
        text = "<color=red>çº¢è‰²</color>è­¦å‘Š"
        frozen, local_map = freezer.freeze_text(text, source_lang='zh-CN')
        
        # ä¸­æ–‡åˆ†è¯åï¼Œæ ‡ç­¾è¢«ä¿æŠ¤ä¸º __TAG_X__ æ ¼å¼
        assert "<color=red>" not in frozen
        assert "</color>" not in frozen
        assert "TAG_1" in frozen or "TAG" in frozen
        assert "çº¢è‰²" in frozen
    
    def test_freeze_non_chinese_no_segmentation(self, freezer):
        """æµ‹è¯•éä¸­æ–‡è¯­è¨€ä¸åˆ†è¯"""
        text = "English text"
        frozen, local_map = freezer.freeze_text(text, source_lang='en-US')
        
        # è‹±æ–‡ä¸åº”è¯¥è¢«åˆ†è¯
        assert frozen == "English text"
    
    def test_freeze_complex_mixed_content(self, freezer):
        """æµ‹è¯•å¤æ‚æ··åˆå†…å®¹"""
        text = "<b>{playerName}</b>é€ æˆ%dç‚¹ä¼¤å®³\\n"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert "âŸ¦TAG_1âŸ§" in frozen  # <b>
        assert "âŸ¦PH_1âŸ§" in frozen  # {playerName}
        assert "âŸ¦TAG_2âŸ§" in frozen  # </b>
        assert "âŸ¦PH_2âŸ§" in frozen  # %d
        assert "âŸ¦PH_3âŸ§" in frozen  # \\n

# ============================================================================
# Test PlaceholderFreezer - Counter Management
# ============================================================================

class TestCounterManagement:
    """æµ‹è¯•è®¡æ•°å™¨ç®¡ç†"""
    
    def test_reset_counters(self, freezer):
        """æµ‹è¯•é‡ç½®è®¡æ•°å™¨"""
        freezer.freeze_text("{0}", source_lang='en')
        assert freezer.ph_counter == 1
        
        freezer.reset_counters()
        assert freezer.ph_counter == 0
        assert freezer.tag_counter == 0
        assert freezer.placeholder_map == {}
        assert freezer.reverse_map == {}
    
    def test_counters_increment_correctly(self, freezer):
        """æµ‹è¯•è®¡æ•°å™¨æ­£ç¡®é€’å¢"""
        freezer.freeze_text("{0}", source_lang='en')
        assert freezer.ph_counter == 1
        
        freezer.freeze_text("<b>", source_lang='en')
        assert freezer.tag_counter == 1
        
        freezer.freeze_text("{1}", source_lang='en')
        assert freezer.ph_counter == 2


# ============================================================================
# Test detect_unbalanced_basic
# ============================================================================

class TestDetectUnbalancedBasic:
    """æµ‹è¯•åŸºæœ¬å¹³è¡¡æ£€æŸ¥å‡½æ•°"""
    
    def test_balanced_text(self):
        """æµ‹è¯•å¹³è¡¡çš„æ–‡æœ¬"""
        text = "æ­£å¸¸{æ–‡æœ¬}å†…å®¹[æµ‹è¯•]"
        issues = detect_unbalanced_basic(text)
        assert len(issues) == 0
    
    def test_unbalanced_braces(self):
        """æµ‹è¯•ä¸å¹³è¡¡çš„èŠ±æ‹¬å·"""
        text = "ç¼ºå°‘å³æ‹¬å·{æ–‡æœ¬"
        issues = detect_unbalanced_basic(text)
        assert 'brace_unbalanced' in issues
    
    def test_unbalanced_angles(self):
        """æµ‹è¯•ä¸å¹³è¡¡çš„å°–æ‹¬å·"""
        text = "ç¼ºå°‘å³å°–æ‹¬å·<æ–‡æœ¬"
        issues = detect_unbalanced_basic(text)
        assert 'angle_unbalanced' in issues
    
    def test_unbalanced_square(self):
        """æµ‹è¯•ä¸å¹³è¡¡çš„æ–¹æ‹¬å·"""
        text = "ç¼ºå°‘å³æ–¹æ‹¬å·[æ–‡æœ¬"
        issues = detect_unbalanced_basic(text)
        assert 'square_unbalanced' in issues
    
    def test_multiple_unbalanced(self):
        """æµ‹è¯•å¤šç§ä¸å¹³è¡¡"""
        text = "{<["
        issues = detect_unbalanced_basic(text)
        assert len(issues) == 3
        assert 'brace_unbalanced' in issues
        assert 'angle_unbalanced' in issues
        assert 'square_unbalanced' in issues
    
    def test_empty_string(self):
        """æµ‹è¯•ç©ºå­—ç¬¦ä¸²"""
        issues = detect_unbalanced_basic("")
        assert len(issues) == 0
    
    def test_nested_balanced(self):
        """æµ‹è¯•åµŒå¥—ä½†å¹³è¡¡çš„æ–‡æœ¬"""
        text = "å¤–{ä¸­[å†…]ä¸­}å¤–"
        issues = detect_unbalanced_basic(text)
        assert len(issues) == 0


# ============================================================================
# Test NormalizeGuard - Initialization
# ============================================================================

class TestNormalizeGuardInit:
    """æµ‹è¯• NormalizeGuard åˆå§‹åŒ–"""
    
    def test_init_success(self, temp_schema_file):
        """æµ‹è¯•æˆåŠŸåˆå§‹åŒ–"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file,
            source_lang="zh-CN"
        )
        
        assert guard.input_path == Path("input.csv")
        assert guard.source_lang == "zh-CN"
        assert guard.errors == []
        assert guard.warnings == []
        assert guard.sanity_errors == []
    
    def test_init_default_source_lang(self, temp_schema_file):
        """æµ‹è¯•é»˜è®¤æºè¯­è¨€"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file
        )
        
        assert guard.source_lang == "zh-CN"


# ============================================================================
# Test NormalizeGuard - Header Validation
# ============================================================================

class TestValidateInputHeaders:
    """æµ‹è¯•è¾“å…¥å¤´éªŒè¯"""
    
    def test_valid_headers(self, temp_schema_file):
        """æµ‹è¯•æœ‰æ•ˆçš„å¤´"""
        guard = NormalizeGuard(
            input_path="test.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file
        )
        
        result = guard.validate_input_headers(['string_id', 'source_zh', 'context'])
        assert result is True
        assert len(guard.errors) == 0
    
    def test_missing_string_id(self, temp_schema_file):
        """æµ‹è¯•ç¼ºå°‘ string_id"""
        guard = NormalizeGuard(
            input_path="test.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file
        )
        
        result = guard.validate_input_headers(['source_zh'])
        assert result is False
        assert any("Missing required columns" in e for e in guard.errors)
    
    def test_missing_source_zh(self, temp_schema_file):
        """æµ‹è¯•ç¼ºå°‘ source_zh"""
        guard = NormalizeGuard(
            input_path="test.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file
        )
        
        result = guard.validate_input_headers(['string_id'])
        assert result is False
        assert any("Missing required columns" in e for e in guard.errors)


# ============================================================================
# Test NormalizeGuard - CSV Processing
# ============================================================================

class TestProcessCSV:
    """æµ‹è¯• CSV å¤„ç†"""
    
    def test_process_csv_success(self, tmp_path, temp_schema_file, freezer):
        """æµ‹è¯•æˆåŠŸå¤„ç† CSV"""
        # åˆ›å»ºæµ‹è¯• CSV
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,æ–‡æœ¬{0}\nTEST_002,<b>æ ‡ç­¾</b>", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is True
        assert len(rows) == 2
        assert rows[0]['string_id'] == 'TEST_001'
        assert 'tokenized_zh' in rows[0]
        assert 'is_long_text' in rows[0]
    
    def test_process_csv_empty_string_id(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ç©º string_id"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\n,æ–‡æœ¬\nTEST_002,æ–‡æœ¬2", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        # é”™è¯¯ä¼šè¿”å› False
        assert success is False
        assert any("Empty string_id" in e for e in guard.errors)
    
    def test_process_csv_duplicate_id(self, tmp_path, temp_schema_file):
        """æµ‹è¯•é‡å¤çš„ string_id"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,æ–‡æœ¬1\nTEST_001,æ–‡æœ¬2", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        # é”™è¯¯ä¼šè¿”å› False
        assert success is False
        assert any("Duplicate string_id" in e for e in guard.errors)
    
    def test_process_csv_unbalanced_text(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ä¸å¹³è¡¡çš„æ–‡æœ¬æ£€æµ‹"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,{æœªé—­åˆ", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is True
        assert len(guard.sanity_errors) == 1
        assert guard.sanity_errors[0]['string_id'] == 'TEST_001'
        assert 'brace_unbalanced' in guard.sanity_errors[0]['issues']
    
    def test_process_csv_long_text_detection(self, tmp_path, temp_schema_file):
        """æµ‹è¯•é•¿æ–‡æœ¬æ£€æµ‹"""
        long_text = "A" * (LONG_TEXT_THRESHOLD + 10)
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text(f"string_id,source_zh\nTEST_001,{long_text}", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is True
        assert rows[0]['is_long_text'] == 1
    
    def test_process_csv_not_long_text(self, tmp_path, temp_schema_file):
        """æµ‹è¯•éé•¿æ–‡æœ¬"""
        short_text = "A" * 10
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text(f"string_id,source_zh\nTEST_001,{short_text}", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is True
        assert rows[0]['is_long_text'] == 0
    
    def test_process_csv_file_not_found(self, temp_schema_file):
        """æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨"""
        guard = NormalizeGuard(
            input_path="/nonexistent/file.csv",
            output_draft_path="draft.csv",
            output_map_path="map.json",
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is False
        assert any("not found" in e for e in guard.errors)
    
    def test_process_csv_preserve_extra_columns(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ä¿ç•™é¢å¤–åˆ—"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh,context,extra\nTEST_001,æ–‡æœ¬,ä¸Šä¸‹æ–‡,é¢å¤–", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        success, rows = guard.process_csv()
        
        assert success is True
        assert 'context' in rows[0]
        assert 'extra' in rows[0]
        assert rows[0]['context'] == 'ä¸Šä¸‹æ–‡'


# ============================================================================
# Test NormalizeGuard - Write Output Files
# ============================================================================

class TestWriteOutputFiles:
    """æµ‹è¯•è¾“å‡ºæ–‡ä»¶å†™å…¥"""
    
    def test_write_draft_csv_success(self, tmp_path, temp_schema_file):
        """æµ‹è¯•æˆåŠŸå†™å…¥ draft CSV"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        rows = [
            {
                'string_id': 'TEST_001',
                'source_zh': 'æ–‡æœ¬',
                'tokenized_zh': 'æ–‡æœ¬',
                'is_long_text': 0,
                'context': 'ä¸Šä¸‹æ–‡'
            }
        ]
        
        result = guard.write_draft_csv(rows)
        assert result is True
        assert (tmp_path / "draft.csv").exists()
    
    def test_write_draft_csv_empty_rows(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ç©ºè¡Œå†™å…¥"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        result = guard.write_draft_csv([])
        assert result is True
        assert any("No rows to write" in w for w in guard.warnings)
    
    def test_write_placeholder_map_success(self, tmp_path, temp_schema_file):
        """æµ‹è¯•æˆåŠŸå†™å…¥ placeholder map"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        # å…ˆå†»ç»“ä¸€äº›å†…å®¹
        guard.freezer.freeze_text("{0}", source_lang='en')
        guard.freezer.freeze_text("<b>", source_lang='en')
        
        result = guard.write_placeholder_map()
        assert result is True
        assert (tmp_path / "map.json").exists()
        
        # éªŒè¯å†…å®¹
        with open(tmp_path / "map.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        assert 'metadata' in data
        assert 'mappings' in data
        assert data['metadata']['ph_count'] == 1
        assert data['metadata']['tag_count'] == 1
        assert data['mappings']['PH_1'] == "{0}"
        assert data['mappings']['TAG_1'] == "<b>"


# ============================================================================
# Test NormalizeGuard - QA Report
# ============================================================================

class TestQAReport:
    """æµ‹è¯• QA æŠ¥å‘Šç”Ÿæˆ"""
    
    def test_write_early_qa_report_with_errors(self, tmp_path, temp_schema_file):
        """æµ‹è¯•æœ‰é”™è¯¯æ—¶ç”Ÿæˆ QA æŠ¥å‘Š"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        guard.sanity_errors.append({
            'string_id': 'TEST_001',
            'issues': ['brace_unbalanced'],
            'source_zh': '{æœªé—­åˆ',
            'row': 2
        })
        
        with patch('builtins.print') as mock_print:
            guard.write_early_qa_report(10)
            
            # éªŒè¯æŠ¥å‘Šæ–‡ä»¶è¢«åˆ›å»º
            qa_path = tmp_path / "qa_hard_report.json"
            assert qa_path.exists()
            
            with open(qa_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            assert report['has_errors'] is True
            assert report['total_rows'] == 10
            assert len(report['errors']) == 1
    
    def test_write_early_qa_report_no_errors(self, tmp_path, temp_schema_file):
        """æµ‹è¯•æ— é”™è¯¯æ—¶ä¸ç”Ÿæˆ QA æŠ¥å‘Š"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        guard.write_early_qa_report(10)
        
        # ä¸åº”è¯¥åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶
        qa_path = tmp_path / "qa_hard_report.json"
        assert not qa_path.exists()


# ============================================================================
# Test NormalizeGuard - Full Workflow
# ============================================================================

class TestFullWorkflow:
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
    
    def test_run_full_success(self, tmp_path, temp_schema_file):
        """æµ‹è¯•å®Œæ•´æˆåŠŸè¿è¡Œ"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,æ–‡æœ¬{0}\nTEST_002,<b>åŠ ç²—</b>", encoding='utf-8')
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        with patch('builtins.print') as mock_print:
            result = guard.run()
            assert result is True
            
            # éªŒè¯æ–‡ä»¶è¢«åˆ›å»º
            assert (tmp_path / "draft.csv").exists()
            assert (tmp_path / "map.json").exists()
    
    def test_run_with_validation_failure(self, tmp_path, temp_schema_file):
        """æµ‹è¯•éªŒè¯å¤±è´¥"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("source_zh\næ–‡æœ¬", encoding='utf-8')  # ç¼ºå°‘ string_id
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        with patch('builtins.print') as mock_print:
            result = guard.run()
            assert result is False


# ============================================================================
# Test Edge Cases
# ============================================================================

class TestEdgeCases:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    
    def test_unicode_content(self, freezer):
        """æµ‹è¯• Unicode å†…å®¹"""
        text = "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ{0}ğŸ®æ¸¸æˆ"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert local_map["PH_1"] == "{0}"
        assert "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ" in frozen
        assert "ğŸ®" in frozen
    
    def test_special_characters(self, freezer):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        text = "ç‰¹æ®Šå­—ç¬¦ï¼š!@#$%^&*(){0}"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        assert local_map["PH_1"] == "{0}"
        assert "!@#$%^&*()" in frozen
    
    def test_very_long_text(self, freezer):
        """æµ‹è¯•è¶…é•¿æ–‡æœ¬"""
        text = "A" * 10000
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        assert len(frozen) == 10000
    
    def test_multiple_same_tags(self, freezer):
        """æµ‹è¯•å¤šä¸ªç›¸åŒæ ‡ç­¾"""
        text = "<b>ç¬¬ä¸€</b>æ™®é€š<b>ç¬¬äºŒ</b>"
        frozen, local_map = freezer.freeze_text(text, source_lang='en')
        
        # æ¯ä¸ª <b> å’Œ </b> æ˜¯ä¸åŒæ ‡ç­¾ï¼Œåº”è¯¥æœ‰ 4 ä¸ª token
        assert "TAG_1" in local_map
        assert "TAG_2" in local_map
        # ç¬¬ä¸€ä¸ª<b>å’Œç¬¬äºŒä¸ª<b>æ˜¯ç›¸åŒå†…å®¹ä½†å‡ºç°å¤šæ¬¡ï¼Œæ ¹æ®é‡ç”¨æœºåˆ¶ä¼šæ˜¯åŒä¸€ä¸ªtoken
        # ä½†å®é™…åº”è¯¥è‡³å°‘æœ‰ TAG_1 å’Œ TAG_2
    
    def test_tag_pattern_regex(self):
        """æµ‹è¯•æ ‡ç­¾æ­£åˆ™è¡¨è¾¾å¼"""
        # æµ‹è¯•å„ç§æ ‡ç­¾æ ¼å¼
        assert TAG_PATTERN.match("<b>")
        assert TAG_PATTERN.match("</b>")
        assert TAG_PATTERN.match("<color=red>")
        assert TAG_PATTERN.match("<size=14>")
        assert not TAG_PATTERN.match("ä¸æ˜¯æ ‡ç­¾")
        assert not TAG_PATTERN.match("<")


# ============================================================================
# Test Error Handling
# ============================================================================

class TestErrorHandling:
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    
    def test_process_csv_unicode_decode_error(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ç¼–ç é”™è¯¯å¤„ç†"""
        # åˆ›å»ºåŒ…å«é UTF-8 å†…å®¹çš„æ–‡ä»¶
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_bytes(b"\xff\xfe")  # BOM without content
        
        guard = NormalizeGuard(
            input_path=str(csv_path),
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        # åº”è¯¥èƒ½å¤„ç†ï¼Œå› ä¸º utf-8-sig å¯ä»¥å¤„ç† BOM
        success, rows = guard.process_csv()
        # ç©ºå†…å®¹å¯èƒ½è¿”å›ç©ºåˆ—è¡¨
        assert isinstance(rows, list)
    
    def test_freeze_with_invalid_regex_in_schema(self, tmp_path):
        """æµ‹è¯• schema ä¸­åŒ…å«æ— æ•ˆæ­£åˆ™"""
        schema_content = '''
version: 2
patterns:
  - name: invalid_regex
    type: placeholder
    regex: "[invalid("
'''
        schema_path = tmp_path / "bad_schema.yaml"
        schema_path.write_text(schema_content, encoding='utf-8')
        
        freezer = PlaceholderFreezer(str(schema_path))
        
        # åº”è¯¥æ‰“å°è­¦å‘Šä½†ç»§ç»­å·¥ä½œ
        with patch('builtins.print') as mock_print:
            frozen, local_map = freezer.freeze_text("æµ‹è¯•æ–‡æœ¬", source_lang='en')
            # æ£€æŸ¥æ˜¯å¦æœ‰æ‰“å°å…³äºæ— æ•ˆæ­£åˆ™çš„è­¦å‘Š
            found_warning = False
            for call in mock_print.call_args_list:
                if any("Invalid regex" in str(arg) for arg in call.args):
                    found_warning = True
                    break
            assert found_warning, "åº”è¯¥æ‰“å°å…³äºæ— æ•ˆæ­£åˆ™çš„è­¦å‘Š"


# ============================================================================
# Test Main Function
# ============================================================================

class TestMainFunction:
    """æµ‹è¯•ä¸»å‡½æ•°"""
    
    def test_main_success(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ä¸»å‡½æ•°æˆåŠŸæ‰§è¡Œ"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,æ–‡æœ¬", encoding='utf-8')
        
        draft_path = tmp_path / "draft.csv"
        map_path = tmp_path / "map.json"
        
        test_args = [
            'normalize_guard.py',
            str(csv_path),
            str(draft_path),
            str(map_path),
            temp_schema_file
        ]
        
        with patch.object(sys, 'argv', test_args):
            with patch('scripts.normalize_guard.sys.exit') as mock_exit:
                from src.scripts.normalize_guard import main
                main()
                mock_exit.assert_called_once_with(0)
    
    def test_main_with_source_lang(self, tmp_path, temp_schema_file):
        """æµ‹è¯•ä¸»å‡½æ•°å¸¦è¯­è¨€å‚æ•°"""
        csv_path = tmp_path / "test_input.csv"
        csv_path.write_text("string_id,source_zh\nTEST_001,æ–‡æœ¬", encoding='utf-8')
        
        draft_path = tmp_path / "draft.csv"
        map_path = tmp_path / "map.json"
        
        test_args = [
            'normalize_guard.py',
            str(csv_path),
            str(draft_path),
            str(map_path),
            temp_schema_file,
            '--source-lang', 'zh-TW'
        ]
        
        with patch.object(sys, 'argv', test_args):
            with patch('scripts.normalize_guard.sys.exit') as mock_exit:
                from src.scripts.normalize_guard import main
                main()
                mock_exit.assert_called_once_with(0)


# ============================================================================
# Test Print Methods
# ============================================================================

class TestPrintMethods:
    """æµ‹è¯•æ‰“å°æ–¹æ³•"""
    
    def test_print_errors(self, tmp_path, temp_schema_file):
        """æµ‹è¯•é”™è¯¯æ‰“å°"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        guard.warnings.append("è­¦å‘Š1")
        guard.errors.append("é”™è¯¯1")
        
        with patch('builtins.print') as mock_print:
            guard._print_errors()
            # éªŒè¯æ‰“å°äº†è­¦å‘Šå’Œé”™è¯¯
            assert mock_print.call_count >= 2
    
    def test_print_summary(self, tmp_path, temp_schema_file):
        """æµ‹è¯•æ€»ç»“æ‰“å°"""
        guard = NormalizeGuard(
            input_path="input.csv",
            output_draft_path=str(tmp_path / "draft.csv"),
            output_map_path=str(tmp_path / "map.json"),
            schema_path=temp_schema_file
        )
        
        guard.freezer.freeze_text("{0}", source_lang='en')
        
        rows = [{'string_id': 'TEST_001', 'source_zh': 'æ–‡æœ¬'}]
        
        with patch('builtins.print') as mock_print:
            guard._print_summary(rows)
            assert mock_print.call_count >= 5


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
